<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[机器学习入门之《统计学习方法》笔记整理——K近邻法]]></title>
    <url>%2F2018%2F02%2F12%2Fk_NN%2F</url>
    <content type="text"><![CDATA[k近邻算法&emsp;&emsp;k近邻算法，即是给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的K个实例，这K个实例的多数属于某个类，就把该输入实例分类到这个类中。 &emsp;&emsp;直接给出k近邻算法： 算法 (k近邻法)输入: 训练数据集T = \left \{ (x_1,y_1),(x_2,y_2),...,(x_n,y_n) \right \} , 其中x_i \in X = \mathbb{R}^n,y_i \in Y = \left \{ c_1,c_2,...,c_K \right \},i = 1,2,...,N ；实例特征向量x ； 输出: 实例x 所属的类y . (1) 根据给定的距离度量，在训练集T 中找到与x 最邻近的k 个点，蕴盖这k 个点的x 的邻域记作N_k(x) ； (2) 在N_k(x) 中根据分类决策规则（如多数表决）决定x 的类别y ： y=arg \max \limits_{c_j} \sum \limits_{x_i \in N_k(x)} I(y_i=c_i),i=1,2,...,N; j=1,2,...,K其中，I 为指示函数，即当y_i=c_i 时I 为1，否则I 为0。 &emsp;&emsp;k近邻法没有显式的学习过程。 k近邻模型&emsp;&emsp;k近邻算法使用的模型实际上对应于特征空间的划分，模型由三个基本要素——距离度量、k值的选择和分类决策规则决定。 距离度量&emsp;&emsp;特征空间中俩个实例的距离是俩个实例点相似程度的反映，k近邻中一般使用欧氏距离。 &emsp;&emsp;设特征空间X 是n 维实数向量空间\mathbb{R}^n ，x_i,x_j \in X,x_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(n)})^T,x_j=(x_j^{(1)},x_j^{(2)},...,x_j^{(n)})^T ，x_i,x_j 的L_p 距离为 L_p(x_i,x_j)=\left ( \sum \limits_{l=1}^{n} \left | x_i^{(l)}-x_j^{(l)} \right |^p\right )^{\frac{1}{p}}p \geq 1​$$ 。 &emsp;&emsp;当$$p=2$$ 时，称为欧氏距离(Euclidean distance)，即 $$L_2(x_i,x_j)=\left ( \sum \limits_{l=1}^{n} \left | x_i^{(l)}-x_j^{(l)} \right |^2\right )^{\frac{1}{2}}&emsp;&emsp;当p=1 时，称为曼哈顿距离(Manhattan distance)，即 L_1(x_i,x_j)=\sum \limits_{l=1}^{n} \left |x_i^{(l)}−x_j^{(l)} \right|&emsp;&emsp;当p=\infty 时，它是各个坐标距离的最大值，即 L_{\infty }(x_i,x_j)=\max \limits_{l} \left |x_i^{(l)}−x_j^{(l)} \right|&emsp;&emsp;不同的距离度量所确定的最近邻点是不同的。 k值选择&emsp;&emsp;k值得选择会对k近邻算法的结果产生重大影响！！！&emsp;&emsp;如果选择的k值较小，就相当于用较小的的邻域中的训练实例进行预测。此时预测的结果会对近邻的实例点非常敏感。&emsp;&emsp;如果选择较大的k值，就相当于在较大的邻域中训练实例进行预测。此时，与输入实例较远的训练实例也会对预测起作用，使预测发生错误。&emsp;&emsp;如果k等于训练样本个数，此时将输入实例简单的预测为训练样本中最多的类。这时模型过于简单，会完全忽略训练样本中的大量有用信息，是不可取的。&emsp;&emsp;在应用中，k值一般选取一个比较小的数值，通常采用交叉验证法来选取最优的k值。 分类决策规则&emsp;&emsp;k近邻算法中分类决策规则往往是多数表决，即由输入实例的k个邻近的训练实例中的多数类决定输入实例的类。 k近邻法的实现：kd树&emsp;&emsp;kd树是一种对k维空间中的样本点进行存储以便对其进行快速检索的树形结构，它是一种二叉树，表示对k维空间的一个划分。构造k树相当于不断的用垂直于坐标轴的超平面去划分k维空间，构成一些列的k维超矩形区域，kd树的每个节点对应于一个k维的超矩形区域。 构造kd树&emsp;&emsp;通俗来讲，对于一个样本空间的样本点，计算每一个维度的方差，按照方差最大的那个维度来排序，因为方差大代表的是数据分散的比较开，这样分割会有更高的分割效率。取中位数作为根节点，小于中位数的样本点作为左子树，大于的作为右子树。重复进行，直到得到一棵完整的二叉树。 算法 (构造平衡kd树)输入：k维空间数据集T=\left \{ x_1,x_2,...,x_N \right \} ，其中x_i = (x_i^{(1)},x_i^{(2)},...,x_i^{(k)})^T, i=1,2,...,N; 输出：kd树 (1) 开始：构造根节点，根节点对应于包含T 的k 维空间的超矩形区域。 &emsp;&emsp;选择中x^{(1)} 为坐标轴，以T 中x^{(1)} 坐标的中位数作为且分点，将根节点对应的超矩形区域切分为两个子区域，切分面为垂直于x^{(1)} 轴的平面。将落在切分面上的点作为根节点，左子节点为对应坐标x^{(1)} 小于切分点的区域，右子节点为对应坐标x^{(1)} 大于切分点的区域。 (2) 重复：对深度为j 的节点，选择中x^{(1)} 为切分的坐标轴，l=j( \mod k )+ 1 ，以该节点的区域中所有实例的x^{(l)} 坐标的中位数为切分点，将该节点对应的超矩形区域切分为两个子区域。 (3) 直到子区域内没有实例存在时停止。 例子 : 给定一个二维空间的数据集：T=\left \{ (2,3)^T,(4,5) ^T,(9,6)^T,(4,7)^T,(8,1)^T,(7,2)^T \right \} 构造一个平衡kd树。(Wikipedia) Python代码如下: 12345678910111213141516171819202122232425262728293031323334from collections import namedtuplefrom operator import itemgetterfrom pprint import pformatclass Node(namedtuple('Node', 'location left_child right_child')): def __repr__(self): return pformat(tuple(self))def kdtree(point_list, depth=0): try: k = len(point_list[0]) # assumes all points have the same dimension except IndexError as e: # if not point_list: return None # Select axis based on depth so that axis cycles through all valid values axis = depth % k # Sort point list and choose median as pivot element point_list.sort(key=itemgetter(axis)) median = len(point_list) // 2 # choose median # Create node and construct subtrees return Node( location=point_list[median], left_child=kdtree(point_list[:median], depth + 1), right_child=kdtree(point_list[median + 1:], depth + 1) )def main(): point_list = [(2,3), (5,4), (9,6), (4,7), (8,1), (7,2)] tree = kdtree(point_list) print(tree)if __name__ == '__main__': main() &emsp;&emsp;我们得到以下结果: 1234&gt; ((7, 2),&gt; ((5, 4), ((2, 3), None, None), ((4, 7), None, None)),&gt; ((9, 6), ((8, 1), None, None), None))&gt; &emsp;&emsp;得到如下所示的特征空间和kd树: 搜索kd树&emsp;&emsp;给定一个目标点，搜索其最近邻，首先找到包含目标点的叶节点，然后从该叶节点出发，依次退回到其父节点，不断查找是否存在比当前最近点更近的点，直到退回到根节点时终止，获得目标点的最近邻点。 算法 (用kd树的最近邻搜索)输入：已构造的kd树；目标点x ； 输出：x 的最近邻。 (1) 首先找到包含目标节点的叶子结点：从根节点出发，按照相应维度比较，递归向下访问kd树，如果目标点x的当前维度的坐标小于根节点，则移动到左子节点，否则移动到右子节点，直到子节点为叶子节点为止。 (2) 以此叶节点为“当前最近点” (3) 递归的向上回退，在每个节点进行以下操作： &emsp;&emsp;(a) 如果该节点保存的实例点距离比当前最近点更小，则该点作为新的“当前最近点” &emsp;&emsp;(b) 检查“当前最近点”的父节点的另一子节点对应的区域是否存在更近的点，如果存在，则移动到该点，接着，递归地进行最近邻搜索。如果不存在，则继续向上回退 (4) 当回到根节点时，搜索结束，获得最近邻点 kd树最近邻搜索实现，Python代码如下： 1234567891011121314151617181920212223242526272829303132333435def get_distance(a, b): return np.linalg.norm(a-b)def nn_search(test_point, node, best_point, best_dist, best_label): if node is not None: cur_dist = get_distance(test_point, node.node_feature) if cur_dist &lt; best_dist: best_dist = cur_dist best_point = node.node_feature best_label = node.node_label axis = node.axis search_left = False if test_point[axis] &lt; node.node_feature[axis]: search_left = True best_point, best_dist, best_label = nn_search(test_point, node.left_child, best_point, best_dist, best_label) else: best_point, best_dist, best_label = nn_search(test_point, node.right_child, best_point, best_dist, best_label) if np.abs(node.node_feature[axis] - test_point[axis]) &lt; best_dist: if search_left: best_point, best_dist, best_label = nn_search(test_point, node.right_child, best_point, best_dist, best_label) else: best_point, best_dist, best_label = nn_search(test_point, node.left_child, best_point, best_dist, best_label) return best_point, best_dist, best_labeldef nn(test_point, tree): best_point , best_dist, best_label = nn_search(test_point, tree, None, np.inf, None) return best_label 小结&emsp;&emsp;KNN是一种lazy-learning算法，它不需要训练，分类的时间复杂度为N（训练样本的个数），引入kd树来实现KNN时间复杂度为logN。kd树更适合于训练样本树远大于空间维度的情况，如果训练样本数接近于空间维度，那么它的效率会迅速下降，几乎接近于线性扫描。 &emsp;&emsp;KNN算法不仅可以用于分类，还可以用于回归。 参考文章 统计学习方法笔记（三）K近邻算法 K近邻算法原理及实现（Python） K-d tree - Wikipedia]]></content>
      <categories>
        <category>note</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>KNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习入门之《统计学习方法》笔记整理——感知机]]></title>
    <url>%2F2018%2F02%2F11%2Fperceptron%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;从头开始学习李航老师的《统计学习方法》，这本书写的很好，非常适合机器学习入门。 感知机模型&emsp;&emsp;什么是感知机？感知机是二类分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取+1和-1二值。感知机学习旨在求出可以将数据进行划分的分离超平面，所以感知机能够解决的问题首先要求特征空间是线性可分的，再者是二类分类，即将样本分为{+1, -1}两类。分离超平面方程为： w·x+b=0&emsp;&emsp;这样，我们就可以构建一个由输入空间到输出空间的函数： f(x)=sign(w·x+b)&emsp;&emsp;称为感知机。其中，w和b为感知机模型的参数，w \in R^n 叫作权值（weight），b \in R 叫作偏置,sign 是符号函数，即 sign(x)=\begin{cases} +1 & \text{ , } x\geq 0 \\ -1 & \text{ , } x< 0 \end{cases}&emsp;&emsp;感知机模型的假设空间是定义在特征空间中的线性分类模型，即函数集合\left \{ f \mid f(x)=w\cdot x+b \right \} 。 感知机学习策略&emsp;&emsp;给定一个数据集T = \left \{ (x_1,y_1),(x_2,y_2),...,(x_N,y_N) \right \} ，其中，x_i \in X= \mathbb{R}^n ，y_i \in Y = \left \{ +1,-1 \right \} ，i = 1,2,...,N 。我们假定数据集中所有y_i=+1 的实例i，有w\cdot x +b>0 ，对所有y_i=-1 的实例i，有$w\cdot x +b0&emsp;&emsp;因此，误分类点x_i 到超平面S 的距离可以写作： -\frac{1}{\left \| w \right \|} y_i (w\cdot x_i+b)&emsp;&emsp;假设超平面S 的误分类点的集合为M ,那么所有误分类点到超平面S 的总距离为 -\frac{1}{\left \| w \right \|} \sum \limits_{x_i\in M} y_i (w\cdot x_i+b)&emsp;&emsp;这里\left \| w \right \| 的值是固定的，不必考虑，于是我们就可以得到感知机sign(w\cdot x+b) 的损失函数为： L(w,b)=-\sum \limits_{x_i\in M} y_i (w\cdot x_i+b)&emsp;&emsp;这个损失函数就是感知机学习的经验风险函数。 感知机学习算法&emsp;&emsp;通过上面的损失函数，我们很容易得到目标函数 \min \limits_{w,b}L(w,b)=-\sum \limits_{x_i\in M} y_i (w\cdot x_i+b)&emsp;&emsp;感知机学习算法是误分类驱动的，具体采用随机梯度下降法( stochastic gradient descent )。 原始形式&emsp;&emsp;所谓原始形式，就是我们用梯度下降的方法，对参数w 和b 进行不断的迭代更新。任意选取一个超平面w_0,b_0 然后使用梯度下降法不断地极小化目标函数。随机梯度下降的效率要高于批量梯度下降 ( 参考Andrew Ng的CS229讲义, Part 1 LMS algorithm部分,翻译) 。 &emsp;&emsp;假设误分类点集合M 是固定的，那么损失函数L(w,b) 的梯度为 \nabla_w L(w,b) = -\sum \limits_{x_i \in M} y_ix_i\nabla_b L(w,b) = -\sum \limits_{x_i \in M} y_i&emsp;&emsp;接下来，随机选取一个误分类点(x_i,y_i) ，对w,b 进行更新： w \leftarrow w+\eta y_ix_ib \leftarrow b+\eta y_i&emsp;&emsp;其中\eta(0]]></content>
      <categories>
        <category>note</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Perceptron</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GPSR协议的NS2仿真全过程（环境+实验）]]></title>
    <url>%2F2018%2F02%2F07%2FGPSR_NS2%2F</url>
    <content type="text"><![CDATA[前些日子帮老师做了个NS2仿真的小项目，现在项目做完了，写篇博客把流程记录下来。做项目时，NS2和GPSR相关的东西找了好久，总会遇到问题，希望我这篇博客能给广大同学们带来点帮助吧。 NS2环境搭建软硬件环境概述 Windows10(x64) VMware Workstation Pro 12.5 LinuxMint 18.1 ns-allinone-2.35 环境搭建过程首先，在官网下载ns-allinone-2.35.tar.gz压缩包(http://sourceforge.net/projects/nsnam/files/allinone/ns-allinone-2.35/ns-allinone-2.35.tar.gz/download)，再下载GPSR源码，我选择的是CSDN上的KeLiu版(http://download.csdn.net/download/joanna_yan/8474651)。 NS2安装按Ctrl+Alt+T，打开终端 依次输入 sudo apt-get update sudo apt-get upgrade sudo apt-get install build-essential sudo apt-get install tcl8.5 tcl8.5-dev tk8.5 tk8.5-dev sudo apt-get install libxmu-dev libxmu-headers tar xvfz ns-allinone-2.35.tar.gz cd ns-allinone-2.35 sudo ./install 在安装的时候会报个错，这是由于源码gcc版本比较老修改只要在linkstate/ls.h文件137行修改成 然后重新安装如上图所示，这样就安装成功了。 然后我们需要配置环境变量，否则无法启动。 sudo gedit /home/(用户名)/.bashrc 在最后加上下面语句，用户名换成自己的即可 12345export PATH="$PATH:/home/(用户名)/ns-allinone-2.35/bin:/home/(用户名)/ns-allinone-2.35/tcl8.5.10/unix:/home/(用户名)/ns-allinone-2.35/tk8.5.10/unix"export LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/home/(用户名)/ns-allinone-2.35/otcl-1.14:/home/(用户名)/ns-allinone-2.35/lib"export TCL_LIBRARY="$TCL_LIBRARY:/home/(用户名)/ns-allinone-2.35/tcl8.5.10/library" 修改完毕，保存，关闭当前终端，再打开一个新的终端，输入ns，回车，如果显示一个%，就证明ns2安装成功了。 NAM安装终端输入nam，如果能够出现nam的窗口则nam可以正常使用，如果提示nam没有安装或者是不能识别的命令，cd /home/ns-allinone-2.35/nam.1.15，ls看看是否有nam文件，如果有的话cp nam ../bin，把nam命令复制到bin中。如果没有的话，sudo ./configure，再sudo make，现在得到了nam，再把nam命令复制到bin中。 接着在终端输入nam检验是否可以运行。 开始实验添加GPSR协议我们搭建好了NS2仿真平台，现在就可以把我们准备好的协议源码解压，放到ns-2.35目录下，然后对ns2的代码进行修改，使我们的协议可以正常运行。 123456789101112131415161718192021222324252627282930311. 进入$HOME/ns-allinone-2.30/ns-2.30/common,修改packet.henum packet_t&#123; //增加 PT_GPSR &#125;class p_info &#123; //增加 name_[PT_GPSR]= “gpsr” &#125;2. 进入$HOME/ns-allinone-2.30/ns-2.30/trace,修改 cmu-trace.cc void CMUTrace::format(Packet* p, const char *why) &#123; //增加 case PT_GPSR; break;&#125;3. 进入$HOME/ns-allinone-2.30/ns-2.30/queue，修改priqueue.ccvoid PriQueue::recv(Packet *p, Handler *h) &#123; //增加 case PT_GPSR:&#125; 12345674. 进入$HOME/ns-allinone-2.30/ns-2.30/tcl/lib,修改ns-packet.tclforeach prot&#123; #增加GPSR &#125; 123456789101112135. 进入$HOME/ns-allinone-2.30/ns-2.30/ ，修改MakefileOBJ_STL = #最后按照格式加入（ gpsr前为TAB键而不是空格） gpsr/gpsr_neighbor.o\ gpsr/gpsr_sinklist.o\ gpsr/gpsr.o#如果需要加入调试信息，则在CCOPT = -Wall 加上 -g, 如下： CCOPT = -g -Wall 6.重新编译，执行如下命令 cd $HOME/ns-allinone-2.30/ns-2.30/common touch packet.cc cd .. sudo make clean sudo make 修改协议源码在仿真过程中，发现KeLiu版GPSR协议在移动场景下存在一些问题，我们进行如下修改： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751761771781791801811821831841851861871881891901911921931941951961971981992002012022032042052062072082092102112122132142152162172182192202212222232242252262272282292302312322332342352362372382392402412422432442452462472482492502512522532542552562572582592602612622632642652662672682692702712722732742752762772782792802812822832842852862872882892902912922932941、gpsr.h文件：90行左右：class GPSRUpdateSinkLocTimer : publicTimerHandler &#123;public: GPSRUpdateSinkLocTimer(GPSRAgent *a) : TimerHandler() &#123;a_=a;&#125; &lt;---这7行protected: virtual void expire(Event *e); GPSRAgent *a_;&#125;; class GPSRQueryTimer : public TimerHandler&#123;public: GPSRQueryTimer(GPSRAgent *a) : TimerHandler() &#123;a_=a;&#125;protected: virtual void expire(Event *e); GPSRAgent *a_;&#125;; 106行左右： friend class GPSRHelloTimer; friend class GPSRQueryTimer; friend class GPSRUpdateSinkLocTimer; MobileNode *node_; //the attached mobile node PortClassifier *port_dmux_; //for the higher layer app de-multiplexing 125行左右：GPSRHelloTimer hello_timer_; GPSRQueryTimer query_timer_; GPSRUpdateSinkLocTimer update_sink_loc_timer_; &lt;---这行 intplanar_type_; //1=GG planarize, 0=RNG planarize double hello_period_; double query_period_; double start_update_time_; &lt;--- double update_sink_loc_period_; &lt;---这2行 void turnon(); //setto be alive void turnoff(); //setto be dead void startSink(); voidstartSink(double);165行左右：void hellotout(); //called bytimer::expire(Event*) void querytout(); void updatesinkloctout(); &lt;---这行public: GPSRAgent();2、gpsr.cc文件：70行左右：voidGPSRQueryTimer::expire(Event *e)&#123; a_-&gt;querytout();&#125;voidGPSRUpdateSinkLocTimer::expire(Event *e)&#123; a_-&gt;updatesinkloctout();&#125;voidGPSRAgent::hellotout()&#123; getLoc(); nblist-&gt;myinfo(my_id,my_x,my_y);//sink_list-&gt;update_sink_loc(my_id,my_x,my_y);//printf("%f\n",node_-&gt;speed()); hellomsg(); hello_timer.resched(hello_period);&#125;void GPSRAgent::updatesinkloctout()&#123; getLoc(); sink_list-&gt;update_sink_loc(my_id,my_x,my_y); //printf("__\n"); update_sink_loc_timer.resched(update_sink_loc_period);&#125;voidGPSRAgent::startSink()&#123; if(sink_list-&gt;new_sink(my_id, my_x, my_y, my_id, 0, query_counter)) querytout();&#125;119行左右：GPSRAgent::GPSRAgent() : Agent(PT_GPSR), hello_timer(this), query_timer(this),update_sink_loc_timer_(this), my_id(-1), my_x(0.0), my_y_(0.0), recv_counter(0), query_counter(0), query_period_(INFINITE_DELAY)&#123; bind("planar_type", &amp;planar_type); bind("hello_period", &amp;hello_period); bind("update_sink_loc_period", &amp;update_sink_loc_period); sink_list_ = new Sinks(); nblist_ = new GPSRNeighbors(); for(int i=0; i&lt;5; i++) randSend_.reset_next_substream();&#125;voidGPSRAgent::turnon()&#123; getLoc(); nblist-&gt;myinfo(my_id, my_x, my_y); hello_timer.resched(randSend.uniform(0.0, 0.5)); update_sink_loc_timer.resched(start_update_time);&#125;voidGPSRAgent::turnoff()&#123; hello_timer_.resched(INFINITE_DELAY); query_timer_.resched(INFINITE_DELAY); update_sink_loc_timer_.resched(INFINITE_DELAY);&#125;3、gpsr_sinklist.h文件中：55行：class Sinks &#123; struct sink_entry *sinklist_; public: Sinks(); bool new_sink(nsaddr_t, double, double, nsaddr_t, int, int); bool update_sink_loc(nsaddr_t, double, double); bool remove_sink(nsaddr_t); void getLocbyID(nsaddr_t, double&amp;, double&amp;, int&amp;); void dump();&#125;;4、gpsr_sinklist.cc中：74行： temp-&gt;next_ = sinklist_; sinklist_ = temp; return true;&#125;boolSinks::update_sink_loc(nsaddr_t id,doublex,double y)&#123; struct sink_entry *temp = sinklist_; while(temp)&#123; if(temp-&gt;id_ == id)&#123; temp-&gt;x_ = x; temp-&gt;y_ = y; return true;&#125; temp = temp-&gt;next_; &#125; return false;&#125;boolSinks::remove_sink(nsaddr_t id)&#123; struct sink_entry *temp; struct sink_entry *p, *q; 修改完成后重新编译NS2。 但是NAM还是有问题，这里是自带的TCL文件的问题，做如下修改就可以了： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110210310410510610710810911011111211311411511611711811961行左右：set opt(tr) trace.tr ;# trace fileset opt(nam) gpsr.nam &lt;---这里set opt(rp) gpsr ;# routing protocol script(dsr or dsdv)set opt(lm) "off" ;# log movement117行左右：（修改比较多，行数不准）# ======================================================================# Agent/GPSR settingAgent/GPSR set planar_type_ 1 ;#1=GG planarize, 0=RNG planarizeAgent/GPSR set hello_period_ 1.5 ;#Hello message periodAgent/GPSR set update_sink_loc_period_ 0.5Agent/GPSR set start_update_time_ 0.001#======================================================================159行左右：set tracefd [open $opt(tr) w]ns_ trace-all tracefd set namfile [open $opt(nam) w]ns_ namtrace-all-wireless namfile opt(x)opt(y)topo load_flatgrid opt(x) $opt(y)prop topography topo 197行左右：source ./gpsr.tclfor &#123;set i 0&#125; &#123;i &lt; opt(nn) &#125; &#123;incr i&#125;&#123; gpsr-create-mobile-node $i ns_ initial_node_pos node_($i) 20 node_(i) namattach $namfile&#125;## Source the Connection and Movementscripts# 下面是CBR和场景文件的问题：给个简单的3个节点的例子，在trace文件中可以看到数据包的转发和接收cbr文件：# GPSR routing agent settingsfor &#123;set i 0&#125; &#123;i &lt; opt(nn)&#125; &#123;incr i&#125; &#123; ns_ at 0.00002 "ragent_($i) turnon" ns_ at 2.0 "ragent_($i) neighborlist"# ns_ at 30.0 "ragent_($i) turnoff"&#125;ns_ at 11.2 "ragent_(2) startSink10.0" #&lt;---这里只要让目标节点startSink就可以，例子是0向2发set null_(1) [new Agent/Null]ns_ attach-agent node(2) $null(1) set udp_(1) [new Agent/UDP]ns_ attach-agent node(0) $udp(1) set cbr_(1) [new Application/Traffic/CBR]$cbr(1) set packetSize 32$cbr(1) set interval 2.0$cbr(1) set random 1# $cbr(1) set maxpkts 100cbr_(1) attach-agent udp_(1)ns_ connect udp(1) $null(1)ns_ at 66.0 "cbr_(1) start"ns_ at 150.0 "cbr_(1) stop" 修改底层协议为802.11pGPSR是路由协议，也就是工作在网络层的，底层的协议默认应该是IEEE802.11。应该用IEEE802.11p，这个才是针在ns-allinone-2.35/ns-2.35/tcl/ex/802.11目录下找到了IEEE802-11p.tcl文件，里面的设置都是符合IEEE802.11p协议的参数对车载自组网的协议。 所以在wireless-gpsr.tcl中把其他的MAC层和物理层的设置都注释掉，换上IEEE802.11p的设置： 12345678910111213141516171819202122232425262728293031323334353694行左右：#Phy/WirelessPhy set CPThresh_ 10.0#Phy/WirelessPhy set CSThresh_ 1.559e-11#Phy/WirelessPhy set RXThresh_ 3.652e-10#Phy/WirelessPhy set Rb_ 2*1e6#Phy/WirelessPhy set freq_ 914e+6 #Phy/WirelessPhy set L_ 1.0 # The transimssion radio range #Phy/WirelessPhy set Pt_ 6.9872e-4 ;# ?m#Phy/WirelessPhy set Pt_ 8.5872e-4 ;# 40m#Phy/WirelessPhy set Pt_ 1.33826e-3 ;# 50m#Phy/WirelessPhy set Pt_ 7.214e-3 ;# 100m#Phy/WirelessPhy set Pt_ 0.2818 ;# 250m #802.11pputs "Loading IEEE802.11pconfiguration..."source ../tcl/ex/802.11/IEEE802-11p.tclputs "Load complete..." 修改无线传输范围ns-2.35/indep-utils/propagation/下有个threshold工具，可以通过距离、功率等等条件算出这些参数。 我们要先对threshold.cc文件进行修改 #include “iostream.h” 修改为#include “iostream”，同时加上using namespace std; 再添加头文件：#include “cstring” 在当前目录终端输入命令，进行编译： cd ns/indep-utils/propagation/ g++ -lm threshold.cc -o threshold 编译成功生成threshold文件这时我们利用threshold工具就可以得到我们需要的RXThresh的值了。 ./threshold -m TwoRayGround -r 1 550 这就是我们需要的RXThresh数值，将我们新得到的数值替换之前的数值，即可改变我们无线传输的范围了。 参数设置好后，我们就可以开始仿真实验了。（中途我还修改了协议其他地方的代码，涉及到项目内容，在此不过多赘述，所以后面的数据跟GPSR协议本身跑出来的数据不大一样） setdest生成随机场景setdest可以随机产生无线网络仿真所需要的移动场景。 setdest程序放在 ns-2.35/indep-utils/cmu-scen-gen/setdest/目录下 usage：setdest [-nnodes][-p pause][-M maxrate][-t time][-x x][-y y] 我们需要50个节点，最大速度10m/s，持续时间20s，场景长250，宽200 ./setdest -n 50-p 0.0 -M 10.0 -t 20 -x 250 -y 200 &gt;scen_50_0_10_20_25_20 cbrgen生成数据流cbrgen工具在ns-2.35/indep-utils/cmu-scen-gen/目录下 usage：cbrgen.tcl [-type cbr][-nn nodes][-seed seed][-mcconnections][-rate rate] 我们此时有50个节点，需要一组数据流所以，输入： ns cbrgen.tcl -type cbr -nn 50 -seed 1 -mc 1-rate 1.0 &gt; cbr_n50_m1_r1 重新编写tcl脚本现在，我们把仿真需要的场景，数据流都准备好了，我们把生成的场景文件移动到gpsr协议的文件夹中，再次修改tcl脚本 添加： 123source scen_50_0_10_20_25_20source cbr_n50_m1_r1 修改完的脚本是这个样子的(这里我另存为gpsr-wireless.tcl文件)： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204set opt(chan) Channel/WirelessChannelset opt(prop) Propagation/TwoRayGroundset opt(netif) Phy/WirelessPhyset opt(mac) Mac/802_11set opt(ifq) Queue/DropTail/PriQueueset opt(ll) LL set opt(ant) Antenna/OmniAntennaset opt(x) 250set opt(y) 200set opt(ifqlen) 50set opt(nn) 50set opt(seed) 0.0set opt(stop) 20.0set opt(tr) trace.trset opt(nam) out.namset opt(rp) gpsrset opt(lm) "off" LL set mindelay_ 50usLL set delay_ 25usLL set bandwidth_ 0 ;# not usedAgent/Null set sport_ 0Agent/Null set dport_ 0Agent/CBR set sport_ 0Agent/CBR set dport_ 0Agent/UDP set sport_ 0Agent/UDP set dport_ 0Agent/UDP set packetSize_ 1460Queue/DropTail/PriQueue setPrefer_Routing_Protocols 1Antenna/OmniAntenna set X_ 200Antenna/OmniAntenna set Y_ 200Antenna/OmniAntenna set Z_ 1.5Antenna/OmniAntenna set Gt_ 1.0Antenna/OmniAntenna set Gr_ 1.0#802.11pputs "Loading IEEE802.11pconfiguration..."source ../tcl/ex/802.11/IEEE802-11p.tclputs "Load complete..."# Agent/GPSR settingAgent/GPSR set planar_type_ 1 ;#1=GG planarize, 0=RNG planarizeAgent/GPSR set hello_period_ 5.0 ;#Hello message periodAgent/GPSR set update_sink_loc_period_ 0.5Agent/GPSR set start_update_time_ 0.001source ../tcl/lib/ns-bsnode.tclsource ../tcl/mobility/com.tclset ns_ [new Simulator]set chan [new $opt(chan)]set prop [new $opt(prop)]set topo [new Topography] set tracefd [open $opt(tr) w]ns_ trace-all tracefd set namfile [open $opt(nam) w]ns_ namtrace-all-wireless namfile opt(x)opt(y)topo load_flatgrid opt(x) $opt(y)prop topography toposet god_ [create-god $opt(nn)]$ns_ node-config -adhocRouting gpsr \ -llType $opt(ll) \ -macType $opt(mac) \ -ifqType $opt(ifq) \ -ifqLen $opt(ifqlen) \ -antType $opt(ant) \ -propType $opt(prop) \ -phyType $opt(netif) \ -channelType $opt(chan) \ -topoInstance $topo \ -agentTrace ON \ -routerTrace ON \ -macTrace OFF \ -movementTrace OFF source ./gpsr.tclfor &#123;set i 0&#125; &#123;i &lt; opt(nn)&#125; &#123;incr i&#125; &#123; gpsr-create-mobile-node$i; node_(i)namattach $namfile;&#125;source scen_50_0_10_20_25_20source cbr_n50_m1_r1for &#123;set i 0&#125; &#123;i &lt; opt(nn) &#125; &#123;incr i&#125;&#123; node_(i) namattach $namfile&#125;for &#123;set i 0&#125; &#123;i &lt; opt(nn)&#125; &#123;incr i&#125; &#123; ns_ initial_node_pos node_($i) 20&#125;for &#123;set i 0&#125; &#123;i &lt; opt(nn)&#125; &#123;incr i&#125; &#123; ns_at opt(stop).0 "node_(i) reset"&#125;ns_ at opt(stop) "stop"ns_ at opt(stop).01 "puts\"NSEXITING...\" ; $ns_ halt"proc stop &#123;&#125; &#123; globalns_ tracefd namfile $ns_flush-trace close$tracefd close$namfile exit0&#125;puts tracefd "M 0.0 nn opt(nn) xopt(x) y opt(y) rp $opt(rp)"puts tracefd "M 0.0 prop opt(prop)ant $opt(ant)"puts "Starting Simulation..."$ns_ run 我们在此目录中打开终端，输入 ns gpsr-wireless.tcl 如图所示运行nam查看仿真状况 如图我们可以看到节点移动情况 trace文件分析下面是整个仿真过程最重要的部分，trace文件分析 投递率我使用的是grep工具 grep “r.*AGT” trace.tr &gt; g_r grep “s.*AGT” trace.tr &gt; g_s wc g_? 即可得到数据包的发送和接收情况：然后，我们通过awk脚本来获取协议中的时延情况： 脚本代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879#BEGIN表明这是程序开头执行的一段语句，且只执行一次。BEGIN &#123;#程序初始化，设定一变量以记录目前处理的封包的最大ID号码。在awk环境下变量的使用不需要声明，直接赋值。highest_uid = 0;&#125;#下面大括号里面的内容会针对要进行处理的记录（也就是我们的trace文件）的每一行都重复执行一次&#123;event = 1; #1表示一行的第一栏，是事件的动作。每一栏默认是以空格分隔的。下同。time = $2; #事件发生的时间node_nb = $3; #发生事件的节点号（但是两边夹着“”，下面一句代码将“”处理掉）node_nb=substr(node_nb,2,1); #第三栏的内容是形如0的节点号码，我只要得出中间的节点号码0，所以要对字符串0进行处理。trace_type = $4; #trace文件跟踪事件的层次（指在路由层或mac层等等） flag = $5; #uid = $6; #包的uid号码（普通包头的uid）pkt_type = $7; #包的类型（是信令或是数据）pkt_size = $8; #包的大小（byte）#下面的代码记录目前最高的CBR流的packet ID，本来的延迟分析是针对所有的包的（包括信令），这里作了简化，只针对CBR封包，以后大家做延时分析可以做相应的改动即可。if ( event=="s" &amp;&amp;node_nb==0 &amp;&amp; pkt_type=="cbr" &amp;&amp; uid &gt; highest_uid)&#123;#if判断句的前三个判断条件就不说了，第四个是说每个包的记录次数不超过1highest_uid = uid;&#125;#记录封包的传送时间if ( event=="s" &amp;&amp;node_nb==0 &amp;&amp; pkt_type=="cbr" &amp;&amp; uid==highest_uid )start_time[uid] = time; # start_time[]表明这是一个数组#记录封包的接收时间if ( event=="r" &amp;&amp;node_nb ==2 &amp;&amp; pkt_type=="cbr" &amp;&amp; uid==highest_uid )end_time[uid] = time;&#125;#END表明这是程序结束前执行的语句，也只执行一次END &#123;#当每行资料都读取完毕后，开始计算有效封包的端到端延迟时间。for ( packet_id = 0; packet_id &lt;=highest_uid; packet_id++ )&#123;start = start_time[packet_id];end = end_time[packet_id];packet_duration = end - start;#只把接收时间大于传送时间的记录打印出来if ( start &lt; end ) printf("%d%f\n", packet_id, packet_duration);&#125;&#125; 我们在终端输入命令： gawk -f delay.awk trace.tr &gt; delay.csv 因为我本人熟悉Python语言，所以绘图我就用Python和matplotlib库来做了： 1234567891011121314151617181920212223242526272829import pandas as pdimport numpy as npimport matplotlib.pyplot as pltcol = ['x','y']data = pd.read_csv("delay.csv",names=col)x = data['x']y = data['y']plt.xlabel('Time(s)')plt.ylabel('Transmission Speed(KB/s)')plt.title('GPSR Analysis')plt.xlim(50,150)plt.ylim(0.0015,0.0055)plt.plot(x0,y0,color='blue',linewidth=1.5,linestyle="-",label='GPSR')plt.plot(x,y,color='red',linewidth=1.5,linestyle="-",label='NNGPSR')plt.show() 得到时延的图 至此，本次ns2实验就结束了。 这篇博客写的比较匆忙，如有错误，可评论区加以说明。 参考文章 NS2笔记八gpsr移植 关于802.11p和场景文件 NS2中cbrgen和setdest的使用 GPSR源码修改 ubuntu14.04LTS下搭建NS2实验环境]]></content>
      <categories>
        <category>note</category>
      </categories>
      <tags>
        <tag>GPSR</tag>
        <tag>NS2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GPSR协议概述]]></title>
    <url>%2F2018%2F02%2F06%2FGPSR_introduce%2F</url>
    <content type="text"><![CDATA[1. GPSR协议简介&emsp;&emsp;GPSR通过应用邻居节点和终点的地理位置，允许每个节点对全局路由分配做出决策。当一个节点以贪婪算法转发一个包时，它有比自己更接近终点的邻居节点，这个节点就选择距离终点最近的邻居节点来转发该包。当没有这种邻居节点时，数据包进入周围模式，将包向前传送给网络平面字图的临近节点，直到传到距离终点较近的节点，将包转发的方式为贪婪算法模式。 2. GPSR协议流程 3. 协议源文件 gpsr_packet.h : 定义不同类型的包 gpsr_neighbor.h : 定义该gpsr实现所使用的每个节点的邻居列表 gpsr_neighbor.cc : 邻居列表类的实现 gpsr.h : 该实现的GPSR路由代理函数的定义 gpsr.cc : GPSR路由代理的实现 gpsr_sinklist.h: 用于多个接收器的场景的定义 gpsr_sinklist.cc: 实现gpsr_sinklist.h 4. 宏定义123456789101112131415161718192021222324252627282930313233343536373839#define DEFAULT_GPSR_TIMEOUT 200.0 //生存时间#define INIFINITE_DISTANCE 1000000000.0 //无穷大 #define SINK_TRACE_FILE "sink_trace.tr" //sink_trace文件#define NB_TRACE_FILE "gpsrnb_trace.tr" //nb_trace文件#define GPSR_CURRENT Scheduler::instance().clock() //计时器#define INFINITE_DELAY 5000000000000.0 //无穷大#define GPSRTYPE_HELLO 0x01 //hello msg#define GPSRTYPE_QUERY 0x02 //query msg from the sink#define GPSRTYPE_DATA 0x04 //the CBR data msg#define GPSR_MODE_GF 0x01 //greedy forwarding mode#define GPSR_MODE_PERI 0x02 //perimeter routing mode#define HDR_GPSR(p) ((structhdr_gpsr*)hdr_gpsr::access(p)) //gpsr报头#define HDR_GPSR_HELLO(p) ((struct hdr_gpsr_hello*)hdr_gpsr::access(p)) //hello报头#define HDR_GPSR_QUERY(p) ((struct hdr_gpsr_query*)hdr_gpsr::access(p)) //query报头#define HDR_GPSR_DATA(p) ((struct hdr_gpsr_data*)hdr_gpsr::access(p)) //data报头 #define PI 3.141593 //PI#define MAX(a, b)(a&gt;=b?a:b) //最大#define MIN(a, b)(a&gt;=b?b:a) //最小 5. 结构体1234567891011121314151617struct hdr_gpsr //gpsr报头struct hdr_gpsr_hello //hello报头struct hdr_gpsr_query //query报头struct hdr_gpsr_data //data报头union hdr_all_gpsr //总报头 struct gpsr_neighbor //邻居 struct sink_entry //数据接收器 6. 类123456789class Sinks //sink表维护一个数据接收器列表，它用于多个数据接收器，这不是GPSR设计的一部分class GPSRNeighbors //网络中每个节点的邻居列表class GPSRAgent //GPSR路由代理，定义路由代理、路由的方法(行为)class GPSRHelloTimer: public TimerHandler //定时发送‘hello’信息class GPSRQueryTimer: public TimerHandler //数据接收器使用的查询计时器来触发数据查询。它不是GPSR路由设计的一部分。 7. 相关函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114void GPSRHelloTimer::expire(Event*e) //hello计时器计时方法void GPSRQueryTimer::expire(Event*e) //查询计时器计时方法void GPSRUpdateSinkLocTimer::expire(Event *e) //数据接收器更新计时器计时方法void GPSRAgent::hellotout() //侦查函数void GPSRAgent::updatesinkloctout() //数据接收器侦查void GPSRAgent::startSink() //开始接受数据void GPSRAgent::startSink(doublegp) //开始接受数据void GPSRAgent::querytout() //查询侦查void GPSRAgent::getLoc() //获取位置void GPSRAgent::GetLocation(double *x, double *y) //获取位置GPSRAgent::GPSRAgent(): Agent(PT_GPSR), hello_timer(this), query_timer(this), update_sink_loc_timer_(this), my_id(-1), my_x(0.0), my_y_(0.0), recv_counter(0), query_counter(0), query_period_(INFINITE_DELAY) //协议初始化void GPSRAgent::turnon() //开启协议void GPSRAgent::turnoff() //关闭协议void GPSRAgent::hellomsg() //发送hello包void GPSRAgent::query(nsaddr_t id) //开始查询void GPSRAgent::recvHello(Packet *p) //接受hello包void GPSRAgent::recvQuery(Packet*p) //接受查询信息void GPSRAgent::sinkRecv(Packet *p) //数据接收器接受包信息void GPSRAgent::forwardData(Packet*p) //数据信息判断并转发void GPSRAgent::recv(Packet *p, Handler *h) //接受数据包void GPSRAgent::trace(char *fmt, ...) //trace函数int GPSRAgent::command(int argc, const charconst argv) //接受参数命令 GPSRNeighbors::GPSRNeighbors() //邻居表初始化GPSRNeighbors::~GPSRNeighbors() //析构函数double GPSRNeighbors::getdis(double ax, double ay, double bx, double by) //获取位置int GPSRNeighbors::nbsize() //邻居节点数量void GPSRNeighbors::myinfo(nsaddr_t mid, double mx, double my) //获取信息struct gpsr_neighbor* GPSRNeighbors::getnb(nsaddr_t nid) //获取邻居表void GPSRNeighbors::newNB(nsaddr_t nid, double nx, double ny) //添加新的邻居节点void GPSRNeighbors::delnb(nsaddr_t nid) //删除邻居节点void GPSRNeighbors::delnb(struct gpsr_neighbor *nb) //删除邻居节点void GPSRNeighbors::delalltimeout() //删除所有超时节点信息nsaddr_t GPSRNeighbors::gf_nexthop(double dx, double dy) //贪婪模式下一跳struct gpsr_neighbor* GPSRNeighbors::gg_planarize() struct gpsr_neighbor* GPSRNeighbors::rng_planarize() //进行周长路由计算double GPSRNeighbors::angle(double x1, double y1, double x2, double y2) //计算角度int GPSRNeighbors::intersect(nsaddr_t theother, double sx, double sy, double dx, double dy) //检查两条线是否局部相交int GPSRNeighbors::num_of_neighbors(struct gpsr_neighbor *nblist) //给定邻居表的节点数量void GPSRNeighbors::free_neighbors(struct gpsr_neighbor *nblist) //释放邻居节点nsaddr_t GPSRNeighbors::peri_nexthop(inttype_, nsaddr_t last, double sx, double sy,double dx, double dy) //周边模式下一跳void GPSRNeighbors::dump() //转储邻居表 Sinks::Sinks() //数据接收器初始化bool Sinks::new_sink(nsaddr_tid, double x, double y, nsaddr_t lasthop, int hops, int seqno) //创建新的数据接收器bool Sinks::remove_sink(nsaddr_t id) //删除数据接收器void Sinks::getLocbyID(nsaddr_t id, double &amp;x, double &amp;y, int &amp;hops) //通过ID获取节点位置void Sinks::dump() //转储]]></content>
      <categories>
        <category>note</category>
      </categories>
      <tags>
        <tag>GPSR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李开复《人工智能》读后感]]></title>
    <url>%2F2018%2F02%2F06%2FArtificial_Intelligence_Recommend%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;放假之前，从学校图书馆找资料的时候无意间看到了这本书，觉得可以看看，无聊的时候当个消遣，就借了回来。 &emsp;&emsp;《人工智能》这本书其实就是告诉我们：个人应该做些什么，才能避免被AI取代？企业应该如何升级，才能在新的商业变局到来前抓住先机？本书不太适合作为人工智能入门书籍，因为里面专业知识很少，可以作为科普书籍，能够了解人工智能的一些术语。 &emsp;&emsp;人工智能已经来了，而且它就在我们身边，几乎无处不在。 &emsp;&emsp;我们真的知道什么是人工智能吗？我们真的准备好与人工智能共同发展了吗？我们该如何在心理上将人和机器摆在正确的位置？我们该如何规划人工智能时代的未来生活？ &emsp;&emsp;其实，人工智能已经到处都是，什么都做：可以陪人聊天，可以写标准新闻，能画画，能翻译，能开车，能认出人的样子，能在互联网上搜答案，能在仓库搬货，能送快递到家。 人工智能是什么？有以下五种定义： &emsp;1) 让人觉得不可思议的计算机程序（某方面特别聪明的计算机程序），比如AlphaGo、AlphaGo Zero等等。 &emsp;2) 与人类思考方式相似的计算机程序。但这事儿太难，人的意识（复杂的技术和哲学问题），谁知道自己是怎么思考的，还要交给机器，my god… &emsp;3) 与人类行为方式相似的计算机程序，也就是说机器并不知道怎么想的，行为方式倒是很像人，比如可以和人聊天的ELIZA。 &emsp;4) 学会自己学习的计算机程序，刚开始笨笨的，慢慢地就越来越聪明，最后逆天的那种。AlphaGo也是因为学了很多很多棋谱才变得这么厉害的，还有ImageNet比赛中的算法也是学了海量的图片才达到比人眼更高的图像识别率的。 &emsp;5) 根据对环境的感知，做出合理的行动，并获得最大收益的计算机程序。 这个定义可以算是综合了以上几个定义得出的结论。 &emsp;&emsp;这一次人工智能复兴的最大特点是，AI在语音识别、机器视觉、数据挖掘等多个领域走进了业界的真实应用场景，与商业模式紧密结合，开始在产业界发挥出真正的价值。 &emsp;&emsp;历史上有过3次AI热潮，第一次因为图灵测试，第二次因为语言识别，但是都很快又到了低谷，这一次，深度学习携手大数据引领了第三次热潮，目前正处于技术曲线的攀升期，前景广阔。 &emsp;&emsp;AlphaGo带来的警示是：如果计算机可以在两年内实现大多数人预测要花20年或更长时间才能完成的进步，那么，还有哪些突破会以远超常人预期的速度来临？这些突破会不会超出我们对人工智能的想象，颠覆人类预想中的未来？我们已为这些即将到来的技术突破做好准备了吗？ &emsp;&emsp;“五秒钟准则”：一项人从事的工作，如果可以在5秒钟内完成思考并做出决策，那么这项工作很可能会被人工智能取代。但是人工智能也会带来新的工作。 人工智能分三个层级： &emsp;1) 弱人工智能：在某方面很聪明，但只在这方面聪明，别的事啥也不会。比如AlphaGo，下围棋世界第一，别的方面就是个弱智，连棋子都得别人帮它拿。 &emsp;2) 强人工智能：人能做什么，它就能做什么。跟美剧《西部世界》里的机器人差不多，但它有没有意识，不好说。 &emsp;3) 超人工智能：比最聪明的人类还要聪明100000000倍。。都不止，它的NB，超乎你想象。我们不知道它是谁，不知道它在哪里，不知道它什么时候出现，也不知道它会干什么。 人工智能目前还很“稚嫩”的几个地方： 跨领域推理，人类强大的跨领域联想、类比能力，可以举一反三，触类旁通。不过迁移学习也正在发展，可以将计算机在一个领域学到的经验转换到另一个领域 抽象能力，如皮克斯工作室《头脑特工队》的抽象空间 知其然，也知其所以然，了解事物运行的本质规律 常识 自我意识 审美 情感 &emsp;&emsp;人工智能不仅是一次技术层次的革命，未来它必将与重大的社会经济变革、教育变革、思想变革、文化变革等同步。人工智能可能成为下一次工业革命的核心驱动力，人工智能更有可能成为人类社会全新的一次大发现、大变革、大融合、大发展的开端。 人工智能主要的商业应用场景： 自动驾驶：这个不用多说，Google，Tesla，百度。。都在研究 智慧金融：量化交易与智能投顾、风控、安防与客户身份认证、智能客服、精准营销 智慧生活：机器翻译、智能家居、智能超市 智慧医疗：辅助诊断疾病、对疑难病症的医疗科学研究 艺术创作：机器音乐、机器绘画、机器文学创作 &emsp;&emsp;大多数情况下，人工智能并不是一种全新的业务流程或全新的商业模式，而是对现有业务流程、商业模式的根本性改造。AI重在提升效率，而非发明新流程、新业务。未来10年，不仅仅是高科技领域，任何一个企业，如果不尽早为自己的业务引入“AI+”的先进思维方式，就很容易处于落后的追随者地位。 &emsp;&emsp;大时代，大格局。AI将成为国家科技战略的核心方向，业界的巨大人力物力，软硬件技术的成熟都为人工智能创业奠定了基础。 AI创业的五大基石： 清晰的领域界限 闭环的、自动标注的数据 千万级的数据量 超大规模的计算能力 顶尖的AI科学家 AI产业发展的六大挑战： 前沿科研与工业界尚未紧密衔接 人才缺口巨大，人才结构失衡 数据孤岛化和碎片化问题明显 可复用和标准化的技术框架、平台、工具、服务尚未成熟 一些领域存在超前发展、盲目投资等问题 创业难度相对较高，早期创业团队需要更多支持 创新工场在AI领域的投资分布： 人工智能研究院主要工作任务包括： 对接科研成果与商业实践，帮助海内外顶级人工智能人才创业 培育和孵化高水准的人工智能技术团队 积累和建设人工智能数据集，促进大数据的有序聚合和合理利用 开展广泛合作，促进人工智能产业的可持续发展 &emsp;&emsp;人工智能时代，程式化的、重复性的、仅靠记忆与练习就可以掌握的技能将是最没有价值的技能，几乎一定可以由机器来完成；反之，那些最能体现人的综合素质的技能，例如，人对于复杂系统的综合分析、决策能力，对于艺术和文化的审美能力和创造性思维，由生活经验及文化熏陶产生的直觉、常识，基于人自身的情感（爱、恨、热情、冷漠等）与他人互动的能力……这些是人工智能时代最有价值，最值得培养、学习的技能。 &emsp;&emsp;说道学习，楼教主可是必须提及的，楼教主（楼天城）的编程功力深厚，以及“一个人挑落一个队”的传奇故事，让开复老师在书中都赞扬了一番。（附上楼教主图片一张） AI时代该怎么学？ 主动挑战极限 从实践中学习 关注启发式教育，培养创造力和独立解决问题的能力 互动式的在线学习将愈来愈重要 主动向机器学习 既学习人-人协作，也学习人-机协作 学习要追随兴趣 AI时代该学什么？ &emsp;&emsp;人工智能时代，程式化的、重复性的、仅靠记忆与练习就可以掌握的技能将是最没有价值的技能，几乎一定可以由机器来完成；反之，那些最能体现人的综合素质的技能，例如，人对于复杂系统的综合分析、决策能力，对于艺术和文化的审美能力和创造性思维，由生活经验及文化熏陶产生的直觉、常识，基于人自身的情感（爱、恨、热情、冷漠等）与他人互动的能力……这些是人工智能时代最有价值，最值得培养、学习的技能。而且，这些技能中，大多数都是因人而异的，需要“定制化”教育或培养，不可能从传统的“批量”教育中获取。 &emsp;&emsp;人工智能技术正在彻彻底底改变人类对机器行为的认知，重建人类与机器之间的相互协作关系。史无前例的自动驾驶正在重构我们脑中的出行地图和人类生活图景，今天的人工智能技术也正在机器翻译、机器写作、机器绘画等人文和艺术领域进行大胆的尝试…… &emsp;&emsp;人工智能因为对生产效率的大幅改进、对人类劳动的部分替代、对生活方式的根本变革，而必然触及社会、经济、政治、文学、艺术等人类生活的方方面面。 &emsp;&emsp;我们无需担忧和恐惧这个时代的到来，我们所要做的，应当是尽早认清AI与人类的关系，了解变革的规律，尽早制定更能适应新时代需求的科研战略、经济发展布局、社会保障体系、教育制度等，以便更好地迎接这个时代的到来。 &emsp;&emsp;这是复兴的时代，这是发展的时代，这是人工智能的时代。 &emsp;&emsp;其实，我们不一定要做时代的弄潮儿，但是，随着时代、科技的发展，我们必须要赶上时代的步伐，不能被时代所抛弃。AI只是一种新的工具，不会取代人类，只会丰富人类生活。未来是一个人类和机器共存，协作完成各类工作的全新时代]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>人工智能</tag>
        <tag>推荐</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows10+Anaconda+TensorFlow(CPU and GPU)环境快速搭建]]></title>
    <url>%2F2018%2F02%2F02%2FAnaconda_TensorFlow%2F</url>
    <content type="text"><![CDATA[今天分享一下本人在笔记本上配置TensorFlow环境的过程。 说明电脑配置： Acer笔记本 CPU Inter Core i5-6200U GPU NVIDIA GeForce 940M(忽略掉我的渣渣GPU) Windows10 所需的环境： Anaconda3(64bit) CUDA-8.0 CuDNN-5.1 Python-3.6 TensorFlow 或者 TensorFlow-gpu 首先安装Anaconda3​ 我们从官网下载(https://www.anaconda.com/download/#windows)，也可以使用我上传百度网盘的版本，链接：https://pan.baidu.com/s/1dGEC57z 密码：2om4使用Linux的小伙伴可以同样下载Linux版本的Anaconda，之后我会再做补充的。 ​ 下载好后，我们进入安装界面： ​ 这里，我们把两个都选上，第一个是加入环境变量，因为我之前安装过一次所以这里提示不要重复添加，第二个是默认的Python3.6，让后Install。 ​ 在完成Anaconda的安装后，我们打开Anaconda的命令行(最好用管理员身份运行，否则可能会有权限的问题)： ​ 我们可以看到一个和Windows命令行很像的一个窗口： 安装CUDA和CuDNN​ 这里为安装GPU版本的TensorFlow做准备，CPU版本可跳过此部分。 ​ CUDA是NVIDIA推出的运算平台，CuDNN是专门针对Deep Learning框架设计的一套GPU计算加速方案。虽然在之后用conda命令安装tensorflow-gpu时会自动安装cudatoolkit和cudnn，但是我总觉得自己安装一遍比较放心。 ​ 我所用的CUDA和CuDNN分享到百度网盘了，链接：https://pan.baidu.com/s/1dGEC57z 密码：2om4 ​ 先安装CUDA ​ 打开首先先解压： ​ 这里我们选择自定义，因为我们只安装CUDA ​ 只选择CUDA其他组件不安装，否则会安装失败 ​ 这里可能会提示你安装Visual Studio，忽略掉就好了 ​ 然后就开始安装了，等待安装结束就好了。 ​ 解压cudnn的压缩包里面有三个文件夹 ​ 把这三个文件夹复制到你cuda的安装目录下，我的地址是C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v8.0 ​ 这样CUDA和CuDNN就安装好了。 创建TensorFlow环境​ 我们在刚刚打开的命令行里输入命令(conda的命令大家可以在这篇博客中找到http://blog.csdn.net/fyuanfena/article/details/52080270)： conda create -n tensorflow_gpu python=3.6 ​ 中间会让我们确认一下，输入个y回车就好了。安装好后会给我们提示用activate，和deactivate进行环境的切换。 ​ 我们先切换到创建好的环境中： activate tensorflow_gpu ​ 现在，基本环境已经配置好了，我们要安装一些重要的Python科学运算库，Anaconda已经为我们准备好的一系列常用的Python苦，例如numpy，pandas，matplotlib等等，所以我们只需要安装一次anaconda库就可以把这些库全部安装好。 conda install anaconda ​ 可以看到，真的有好多常用库。 安装TensorFlow​ 之后就是我们最重要的一步，安装TensorFlow： CPU版本 conda install tensorflow GPU版本 conda install tensorflow-gpu ​ 这样我们的TensorFlow环境已经配置好了。 测试​ 最后，我们进入jupyter notebook(Anaconda自带的Python IDE，自我感觉挺好用的)输入一段官方文档录入的代码测试一下： ​ 直接输入jupyter notebook，回车 1234import tensorflow as tfhello = tf.constant('Hello,TensorFlow!')sess = tf.Session()print(sess.run(hello)) ​ 恭喜，你的TensorFlow已经可以用了，接下来快搭建你自己的神经网络吧~！ 参考文章 Anaconda常用命令大全 NVIDIA CuDNN 安装说明]]></content>
      <categories>
        <category>DeepLearning</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>环境搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow简单神经网络解决Kaggle比赛Titanic问题]]></title>
    <url>%2F2018%2F02%2F01%2FTensorflow_Kaggle_Titanic%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;又到了假期，忙碌了一个学期，终于可以休息一下了。 &emsp;&emsp;一直想再Kaggle上参加一次比赛，在学校要上课，还跟老师做个项目，现在有时间了，就马上用Kaggle的入门比赛试试手。 &emsp;&emsp;一场比赛，总的来说收获不小，平时学习的时候总是眼高手低，结果中间出现令人吐血的失误 &gt;_&lt; Kaggle比赛介绍 &emsp;&emsp;简而言之，Kaggle 是玩数据、ML 的开发者们展示功力、扬名立万的江湖，网址：https://www.kaggle.com/ &emsp;&emsp;Kaggle虽然高手云集，但是对于萌新们来说也是非常友好的，这次的Titanic问题就是适合萌新Getting Started的入门题。 Kaggle 是当今最大的数据科学家、机器学习开发者社区，其行业地位独一无二。 (此话引用自谷歌收购 Kaggle 为什么会震动三界（AI、机器学习、数据科学界）？) Titanic问题概述Titanic: Machine Learning from Disaster 比赛说明&emsp;&emsp;RMS泰坦尼克号的沉没是历史上最臭名昭着的沉船之一。 1912年4月15日，在首航期间，泰坦尼克号撞上一座冰山后沉没，2224名乘客和机组人员中有1502人遇难。这一耸人听闻的悲剧震撼了国际社会，导致了更好的船舶安全条例。 &emsp;&emsp;沉船导致生命损失的原因之一是乘客和船员没有足够的救生艇。虽然幸存下来的运气有一些因素，但一些人比其他人更有可能生存，比如妇女，儿童和上层阶级。 &emsp;&emsp;在这个挑战中，我们要求你完成对什么样的人可能生存的分析。特别是，我们要求你运用机器学习的工具来预测哪些乘客幸存下来的悲剧。 目标&emsp;&emsp;这是你的工作，以预测是否有乘客幸存下来的泰坦尼克号或不。&emsp;&emsp;对于测试集中的每个PassengerId，您必须预测Survived变量的0或1值。 度量值&emsp;&emsp;您的分数是您正确预测的乘客的百分比。这被称为“准确性”。 提交文件格式&emsp;&emsp;你应该提交一个csv文件，正好有418个条目和一个标题行。如果您有额外的列（超出PassengerId和Survived）或行，您的提交将会显示错误。 该文件应该有2列： &emsp;PassengerId（按任意顺序排序）&emsp;生存（包含你的二元预测：1存活，0死亡） 数据总览&emsp;&emsp;首先，我们先把一些库和训练数据导入 1234567import osimport numpy as npimport pandas as pdimport tensorflow as tftrain_data = pd.read_csv('train.csv')print(train_data.info()) &emsp;&emsp;简单的看一下训练数据的信息，其中Embarked有两个缺失值，Age缺失值较多，Cabin有效值太少了跟本没什么用。 12345678910111213141516171819&gt; &lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;&gt; RangeIndex: 891 entries, 0 to 890&gt; Data columns (total 12 columns):&gt; PassengerId 891 non-null int64&gt; Survived 891 non-null int64&gt; Pclass 891 non-null int64&gt; Name 891 non-null object&gt; Sex 891 non-null object&gt; Age 714 non-null float64&gt; SibSp 891 non-null int64&gt; Parch 891 non-null int64&gt; Ticket 891 non-null object&gt; Fare 891 non-null float64&gt; Cabin 204 non-null object&gt; Embarked 889 non-null object&gt; dtypes: float64(2), int64(5), object(5)&gt; memory usage: 83.6+ KB&gt; None&gt; 数据清洗&emsp;&emsp;在我们开始搭建神经网络进行训练之前，数据清洗是必要的。这一步可以简单一些，不过如果想要得到更好的效果，清洗之前的数据分析还是不可少的。这里的数据分析，我就不再赘述了，给大家推荐一篇博客，上面有很详细的分析过程——Kaggle_Titanic生存预测 &emsp;&emsp;我们用随机森林算法，对’Age’的缺失值进行预测，当然这里也可以用其他回归算法，来进行预测 12345678910from sklearn.ensemble import RandomForestRegressorage = train_data[['Age','Survived','Fare','Parch','SibSp','Pclass']]age_notnull = age.loc[(train_data.Age.notnull())]age_isnull = age.loc[(train_data.Age.isnull())]X = age_notnull.values[:,1:]Y = age_notnull.values[:,0]rfr = RandomForestRegressor(n_estimators=1000,n_jobs=-1)rfr.fit(X,Y)predictAges = rfr.predict(age_isnull.values[:,1:])train_data.loc[(train_data.Age.isnull()),'Age'] = predictAges &emsp;&emsp;如果对上一步觉得太麻烦，或不喜欢的话，可以更简单一点，直接把缺失值都给0 1train_data = train_data.fillna(0) #缺失字段填0 &emsp;&emsp;然后，对于性别’Sex’，我们将其二值化’male’为0，’female’为1 12train_data.loc[train_data['Sex']=='male','Sex'] = 0train_data.loc[train_data['Sex']=='female','Sex'] = 1 &emsp;&emsp;我们把’Embarked’也填补下缺失值，因为缺失值较少，所以我们直接给它填补上它的众数’S’，把’S’，’C’，’Q’定性转换为0,1,2，这样便于机器进行学习 1234train_data['Embarked'] = train_data['Embarked'].fillna('S')train_data.loc[train_data['Embarked'] == 'S','Embarked'] = 0train_data.loc[train_data['Embarked'] == 'C','Embarked'] = 1train_data.loc[train_data['Embarked'] == 'Q','Embarked'] = 2 &emsp;&emsp;最后，把’Cabin’这个与生存关系不重要而且有效数据极少的标签丢掉，再加上一个’Deceased’，代表的是是否遇难，这一步很重要，很重要，很重要！我在做的时候没加这个，后面网络的y的标签我也只设了1，训练出的模型跟没训练一样，所有的都是0。发现的时候，死的心都有了╥﹏╥…（希望不会有初学者和我犯一样的错误 ToT ） 12train_data.drop(['Cabin'],axis=1,inplace=True)train_data['Deceased'] = train_data['Survived'].apply(lambda s: 1 - s) &emsp;&emsp;然后，我们再查看一下数据信息 1train_data.info() &emsp;&emsp;这次信息就整齐多了 123456789101112131415161718&gt; &lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;&gt; RangeIndex: 891 entries, 0 to 890&gt; Data columns (total 12 columns):&gt; PassengerId 891 non-null int64&gt; Survived 891 non-null int64&gt; Pclass 891 non-null int64&gt; Name 891 non-null object&gt; Sex 891 non-null object&gt; Age 891 non-null float64&gt; SibSp 891 non-null int64&gt; Parch 891 non-null int64&gt; Ticket 891 non-null object&gt; Fare 891 non-null float64&gt; Embarked 891 non-null object&gt; Deceased 891 non-null int64&gt; dtypes: float64(2), int64(6), object(4)&gt; memory usage: 83.6+ KB&gt; 模型搭建&emsp;&emsp;现在我们把数据的X，Y进行分离，这里我们只选取了6个标签作为X，如果想让结果尽可能准确，请读者自行完善。 12dataset_X = train_data[['Sex','Age','Pclass','SibSp','Parch','Fare']]dataset_Y = train_data[['Deceased','Survived']] &emsp;&emsp;这里，我们进行训练集和验证集的划分，在训练过程中，我们可以更好的观察训练情况，避免过拟合 12345from sklearn.model_selection import train_test_splitX_train,X_val,Y_train,Y_val = train_test_split(dataset_X.as_matrix(), dataset_Y.as_matrix(), test_size = 0.2, random_state = 42) &emsp;&emsp;做完以上工作，我们就可以开始搭建神经网络了，这里，我搭建的是一个简单两层的神经网络，激活函数使用的是线性整流函数Relu，并使用了交叉验证和Adam优化器（也可以使用梯度下降进行优化），设置学习率为0.001 12345678910111213x = tf.placeholder(tf.float32,shape = [None,6],name = 'input')y = tf.placeholder(tf.float32,shape = [None,2],name = 'label')weights1 = tf.Variable(tf.random_normal([6,6]),name = 'weights1')bias1 = tf.Variable(tf.zeros([6]),name = 'bias1')a = tf.nn.relu(tf.matmul(x,weights1) + bias1)weights2 = tf.Variable(tf.random_normal([6,2]),name = 'weights2')bias2 = tf.Variable(tf.zeros([2]),name = 'bias2')z = tf.matmul(a,weights2) + bias2y_pred = tf.nn.softmax(z)cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=z))correct_pred = tf.equal(tf.argmax(y,1),tf.argmax(y_pred,1))acc_op = tf.reduce_mean(tf.cast(correct_pred,tf.float32))train_op = tf.train.AdamOptimizer(0.001).minimize(cost) &emsp;&emsp;下面开始训练，训练之前我先定义了个Saver，epoch为30次 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980# 存档入口saver = tf.train.Saver()# 在Saver声明之后定义的变量将不会被存储# non_storable_variable = tf.Variable(777)ckpt_dir = './ckpt_dir'if not os.path.exists(ckpt_dir): os.makedirs(ckpt_dir)with tf.Session() as sess: tf.global_variables_initializer().run() ckpt = tf.train.latest_checkpoint(ckpt_dir) if ckpt: print('Restoring from checkpoint: %s' % ckpt) saver.restore(sess, ckpt) for epoch in range(30): total_loss = 0. for i in range(len(X_train)): feed_dict = &#123;x: [X_train[i]],y:[Y_train[i]]&#125; _,loss = sess.run([train_op,cost],feed_dict=feed_dict) total_loss +=loss print('Epoch: %4d, total loss = %.12f' % (epoch,total_loss)) if epoch % 10 == 0: accuracy = sess.run(acc_op,feed_dict=&#123;x:X_val,y:Y_val&#125;) print("Accuracy on validation set: %.9f" % accuracy) saver.save(sess, ckpt_dir + '/logistic.ckpt') print('training complete!') accuracy = sess.run(acc_op,feed_dict=&#123;x:X_val,y:Y_val&#125;) print("Accuracy on validation set: %.9f" % accuracy) pred = sess.run(y_pred,feed_dict=&#123;x:X_val&#125;) correct = np.equal(np.argmax(pred,1),np.argmax(Y_val,1)) numpy_accuracy = np.mean(correct.astype(np.float32)) print("Accuracy on validation set (numpy): %.9f" % numpy_accuracy) saver.save(sess, ckpt_dir + '/logistic.ckpt') ''' 测试数据的清洗和训练数据一样，两者可以共同完成 ''' # 读测试数据 test_data = pd.read_csv('test.csv') #数据清洗, 数据预处理 test_data.loc[test_data['Sex']=='male','Sex'] = 0 test_data.loc[test_data['Sex']=='female','Sex'] = 1 age = test_data[['Age','Sex','Parch','SibSp','Pclass']] age_notnull = age.loc[(test_data.Age.notnull())] age_isnull = age.loc[(test_data.Age.isnull())] X = age_notnull.values[:,1:] Y = age_notnull.values[:,0] rfr = RandomForestRegressor(n_estimators=1000,n_jobs=-1) rfr.fit(X,Y) predictAges = rfr.predict(age_isnull.values[:,1:]) test_data.loc[(test_data.Age.isnull()),'Age'] = predictAges test_data['Embarked'] = test_data['Embarked'].fillna('S') test_data.loc[test_data['Embarked'] == 'S','Embarked'] = 0 test_data.loc[test_data['Embarked'] == 'C','Embarked'] = 1 test_data.loc[test_data['Embarked'] == 'Q','Embarked'] = 2 test_data.drop(['Cabin'],axis=1,inplace=True) #特征选择 X_test = test_data[['Sex', 'Age', 'Pclass', 'SibSp', 'Parch', 'Fare']] #评估模型 predictions = np.argmax(sess.run(y_pred, feed_dict=&#123;x: X_test&#125;), 1) #保存结果 submission = pd.DataFrame(&#123; "PassengerId": test_data["PassengerId"], "Survived": predictions &#125;) submission.to_csv("titanic-submission.csv", index=False) &emsp;&emsp;我们把生成的提交文件在Kaggle官网上进行提交，Score为0.79425，效果还可以，不过还有很多需要改进的地方 参考文章 Kaggle_Titanic生存预测 — 详细流程吐血梳理 谷歌收购 Kaggle 为什么会震动三界（AI、机器学习、数据科学界）？ 《深度学习原理与TensorFlow实践》课程代码]]></content>
      <categories>
        <category>DeepLearning</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>Kaggle</tag>
        <tag>Titanic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow实现Neural Style图像风格转移]]></title>
    <url>%2F2018%2F02%2F01%2FTensorflow_Neural_Style%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;刚开始接触TensorFlow，实践个小项目，也参考了一下其他博主的文章，希望大家提出宝贵意见。 文章中的代码和图片已上传到GitHub(https://github.com/Quanfita/Neural-Style)。 什么是图像风格迁移？&emsp;&emsp;以下每一张图都是一种不同的艺术风格。从直观上我们很难找出这些不同风格的图片有什么可以用准确的语言来描述的去别和联系。如何要把一个图像的风格变成另一种风格更是难以定义的问题。作为程序员，又应该如何去用代码去实现？ (图片来自https://zhuanlan.zhihu.com/p/26746283) &emsp;&emsp;在神经网络出现之后，Gatys的几篇论文中提出了一些解决方法，让机器能够理解风格这样模糊的概念：Texture Synthesis Using Convolutional Neural Networks(1505.07376)，A Neural Algorithm of Artistic Style(1508.06576)，Preserving Color in Neural Artistic Style Transfer(1606.05897v1)。&emsp;&emsp;Neural Style成为了一个非常有意思的深度学习应用：输入一张代表内容的图片和一张代表风格的图片，深度学习网络会输出一张融合了这个风格和内容的新作品。&emsp;&emsp;TensorFlow是Google开源的最流行的深度学习框架。在GitHub上有开源的TensorFlow实现的Neural Style代码(地址)。&emsp;&emsp;我们还是先看一下Neural Style这篇论文介绍了怎样的方法来解决这个问题的吧。（以下为论文中的主要内容） 论文内容首先，有几个概念： 卷积神经网络（CNN）&emsp;&emsp;一张输入的图片，会在卷积神经网的各层以一系列过滤后的图像表示。随着层级的一层一层处理，过滤后的图片会通过向下取样的方式不断减小（比如通过池化层）。这使得每层神经网的神经元数量会原来越小。（也就是层越深，因为经过了池化层，单个feature map会越来越小，于是每层中的神经元数量也会越来越少）。 内容重塑&emsp;&emsp;在只知道该层的输出结果，通过重塑输入图像，可以看到CNN不同阶段的图像信息。在原始的VGG-Network上的5个层级:conv1_1,conv1_2,conv1_3,conv1_4,conv1_5上重塑了输入的图像。 &emsp;&emsp;输入的图像是上图中的一排房子，5个层级分别是a,b,c,d,e。在较低层的图像重构（abc）非常完美；在较高层（de），详细的像素信息丢失了。也就是说，这样做提取出了图片的内容，但是抛弃了像素。 风格重塑&emsp;&emsp;在原始的CNN表征之上(feature map)，建立了一个新的特征空间(feature space)，这个特征空间捕获了输入图像的风格。风格的表征计算了在CNN的不同层级间不用特征之间的相似性。通过在CNN隐层的不同的子集上建立起来的风格的表征，我们重构输入图像的风格。如此，便创造了与输入图像一致的风格而丢弃了全局的内容。 &emsp;&emsp;这篇论文的关键是对于内容和风格的表征在CNN中是可以分开的。可以独立地操作两个表征来产生新的，可感知意义的图像。论文中生成一个图片，混合了来自两个不同图片的内容和风格表征。 &emsp;&emsp;一张图片，它同时符合照片的内容表征，和艺术画的风格表征。原始照片的整体布局被保留了，而颜色和局部的结构却由艺术画提供。 &emsp;&emsp;风格表征是一个多尺度的表征，包括了神经网络的多层。在图2中看到的图像，风格的表征包含了整个神经网络的层级。而风格也可以只包含一小部分较低的层级。（见下面的图，第一行是卷基层1，第5行是卷基层5的输出）。若符合了较高层级中的风格表征，局部的图像结构会大规模地增加，从而使得图像在视觉上更平滑与连贯。 &emsp;&emsp;简言之，作者直接把局部特征看做近似的图片内容，这样就得到了一个把图片内容和图片风格（说白了就是纹理）分开的系统，剩下的就是把一个图片的内容和另一个图片的风格合起来。 &emsp;&emsp;图像的内容和风格并不能被完全地分解开。当风格与内容来自不同的两个图像时，这个被合成的新图像并不存在在同一时刻完美地符合了两个约束。但是，在图像合成中最小化的损失函数分别包括了内容与风格两者，它们被很好地分开了。所以，我们可以平滑地将重点既放在内容上又放在风格上 方法：&emsp;&emsp;假设某一层得到的响应是 F^{l} \in R^{N_{l}\times M_{l}} ,其中 N_{l} 为 l 层filter的个数，M_{l}为filter的大小。F_{ij}^{l}表示的是第 l 层第 i 个filter在位置j的输出。 &emsp;&emsp;\vec{p} 代表提供Content的图像，\vec{x} 表示生成的图像，P^l和F^l分别代表它们对于$l$层的响应，因此l层的Content Loss： L_{content}(\vec{p},\vec{x},l) = \frac{1}{2}\sum_{i,j}{(F_{ij}^{l}-P_{ij}^{l})^{2}}&emsp;&emsp;上面我们提到了，某一层的Style可以用G^{l}\in R^{N_{l}\times N_{l}}来表示，其中 ，即不同filter响应的内积。&emsp;&emsp;\vec{a} 代表提供Style的图像，\vec{x} 表示生成的图像，Al和Gl分别代表它们对于l层的Style，因此l层的Style Loss： E_{l} = \frac{1}{4N_{l}^{2}M_{l}^{2}}\sum_{i,j}{(G_{ij}^{l}-A_{ij}^{l})^{2}}&emsp;&emsp;文章中作者使用了多层来表达Style，所以总的Style Loss为： L_{style}(\vec{a},\vec{x}) = \sum_{l=0}^{L}{w_{l}E_{l}}&emsp;&emsp;定义好了两个Loss之后，就利用优化方法来最小化总的Loss： L_{total}(\vec{p},\vec{a},\vec{x}) = αL_{content}(\vec{p},\vec{x}) + βL_{style}(\vec{a},\vec{x})&emsp;&emsp;其中的α和β分别代表了对Content和Style的侧重，文中作者也对α/β取值的效果进行了实验。 &emsp;&emsp;最终迭代出来的\vec{x}​ 既具有\vec{p}​ 的Content，同时也具有\vec{a}​的Style。实验结果也证明了作者文中方法的有效性。 &emsp;&emsp; α和 β分别是内容和风格在图像重构中的权重因子。α和β分别是内容和风格两个损失的权重。α+β=1.如果α比较大，那么输出后的新图会更多地倾向于内容上的吻合，如果β较大，那么输出的新图会更倾向于与风格的吻合。这两个参数是一个trade-off,可以根据自己需求去调整最好的平衡。论文的作者给出了它调整参数的不同结果，如下图，从左到右四列分别是α/β = 10^{-5}, 10^{-4},10^{-3}, 10^{-2}.也就是α越来越大，的确图像也越来越清晰地呈现出了照片的内容。 代码实现环境介绍 Python3.6 TensorFlow 1.2 VGG19 CPU i5-6200U(笔记本) 下面，简单介绍以下VGG19网络： VGG19 网络结构 &emsp;&emsp;每一层神经网络都会利用上一层的输出来进一步提取更加复杂的特征，直到复杂到能被用来识别物体为止，所以每一层都可以被看做很多个局部特征的提取器。VGG19 在物体识别方面的精度甩了之前的算法一大截，之后的物体识别系统也基本都改用深度学习了。VGG19结构如下： (图片来自https://zhuanlan.zhihu.com/p/26746283) 代码详解： TensorFlow版本的源码主要包含了三个文件：neural_style.py, stylize.py和 vgg.py。 neural_style.py：外部接口函数，定义了函数的主要参数以及部分参数的默认值，包含对图像的读取和存贮，对输入图像进行resize，权值分配等操作，并将参数以及resize的图片传入stylize.py中。 stylize.py：核心代码，包含了训练、优化等过程。 vgg.py：定义了网络模型以及相关的运算。 我们可以使用下面的代码vgg.py读取VGG-19神经网络，用于构造Neural Style模型。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import tensorflow as tfimport numpy as npimport scipy.io#需要使用神经网络层VGG19_LAYERS = ( 'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1', 'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2', 'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3', 'relu3_3', 'conv3_4', 'relu3_4', 'pool3', 'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3', 'relu4_3', 'conv4_4', 'relu4_4', 'pool4', 'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3', 'relu5_3', 'conv5_4', 'relu5_4')##我们需要的信息是每层神经网络的kernels和biasdef load_net(data_path): data = scipy.io.loadmat(data_path) if not all(i in data for i in ('layers', 'classes', 'normalization')): raise ValueError("You're using the wrong VGG19 data. Please follow the instructions in the README to download the correct data.") mean = data['normalization'][0][0][0] mean_pixel = np.mean(mean, axis=(0, 1)) weights = data['layers'][0] return weights, mean_pixeldef net_preloaded(weights, input_image, pooling): net = &#123;&#125; current = input_image for i, name in enumerate(VGG19_LAYERS): kind = name[:4] if kind == 'conv': kernels, bias = weights[i][0][0][0][0] kernels = np.transpose(kernels, (1, 0, 2, 3)) bias = bias.reshape(-1) current = _conv_layer(current, kernels, bias) elif kind == 'relu': current = tf.nn.relu(current) elif kind == 'pool': current = _pool_layer(current, pooling) net[name] = current assert len(net) == len(VGG19_LAYERS) return netdef _conv_layer(input, weights, bias): conv = tf.nn.conv2d(input, tf.constant(weights), strides=(1, 1, 1, 1), padding='SAME') return tf.nn.bias_add(conv, bias)def _pool_layer(input, pooling): if pooling == 'avg': return tf.nn.avg_pool(input, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1), padding='SAME') else: return tf.nn.max_pool(input, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1), padding='SAME')def preprocess(image, mean_pixel): return image - mean_pixeldef unprocess(image, mean_pixel): return image + mean_pixel 在neural_style.py中我们可以看到，定义了非常长多的参数和外部接口。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204import osimport numpy as npimport scipy.miscfrom stylize import stylizeimport mathfrom argparse import ArgumentParserfrom PIL import Image# default argumentsCONTENT_WEIGHT = 5e0CONTENT_WEIGHT_BLEND = 1STYLE_WEIGHT = 5e2TV_WEIGHT = 1e2STYLE_LAYER_WEIGHT_EXP = 1LEARNING_RATE = 1e1BETA1 = 0.9BETA2 = 0.999EPSILON = 1e-08STYLE_SCALE = 1.0ITERATIONS = 1000VGG_PATH = 'imagenet-vgg-verydeep-19.mat'POOLING = 'max'def build_parser(): parser = ArgumentParser() parser.add_argument('--content', dest='content', help='content image', metavar='CONTENT', required=True) parser.add_argument('--styles', dest='styles', nargs='+', help='one or more style images', metavar='STYLE', required=True) parser.add_argument('--output', dest='output', help='output path', metavar='OUTPUT', required=True) parser.add_argument('--iterations', type=int, dest='iterations', help='iterations (default %(default)s)', metavar='ITERATIONS', default=ITERATIONS) parser.add_argument('--print-iterations', type=int, dest='print_iterations', help='statistics printing frequency', metavar='PRINT_ITERATIONS') parser.add_argument('--checkpoint-output', dest='checkpoint_output', help='checkpoint output format, e.g. output%%s.jpg', metavar='OUTPUT') parser.add_argument('--checkpoint-iterations', type=int, dest='checkpoint_iterations', help='checkpoint frequency', metavar='CHECKPOINT_ITERATIONS') parser.add_argument('--width', type=int, dest='width', help='output width', metavar='WIDTH') parser.add_argument('--style-scales', type=float, dest='style_scales', nargs='+', help='one or more style scales', metavar='STYLE_SCALE') parser.add_argument('--network', dest='network', help='path to network parameters (default %(default)s)', metavar='VGG_PATH', default=VGG_PATH) parser.add_argument('--content-weight-blend', type=float, dest='content_weight_blend', help='content weight blend, conv4_2 * blend + conv5_2 * (1-blend) (default %(default)s)', metavar='CONTENT_WEIGHT_BLEND', default=CONTENT_WEIGHT_BLEND) parser.add_argument('--content-weight', type=float, dest='content_weight', help='content weight (default %(default)s)', metavar='CONTENT_WEIGHT', default=CONTENT_WEIGHT) parser.add_argument('--style-weight', type=float, dest='style_weight', help='style weight (default %(default)s)', metavar='STYLE_WEIGHT', default=STYLE_WEIGHT) parser.add_argument('--style-layer-weight-exp', type=float, dest='style_layer_weight_exp', help='style layer weight exponentional increase - weight(layer&lt;n+1&gt;) = weight_exp*weight(layer&lt;n&gt;) (default %(default)s)', metavar='STYLE_LAYER_WEIGHT_EXP', default=STYLE_LAYER_WEIGHT_EXP) parser.add_argument('--style-blend-weights', type=float, dest='style_blend_weights', help='style blending weights', nargs='+', metavar='STYLE_BLEND_WEIGHT') parser.add_argument('--tv-weight', type=float, dest='tv_weight', help='total variation regularization weight (default %(default)s)', metavar='TV_WEIGHT', default=TV_WEIGHT) parser.add_argument('--learning-rate', type=float, dest='learning_rate', help='learning rate (default %(default)s)', metavar='LEARNING_RATE', default=LEARNING_RATE) parser.add_argument('--beta1', type=float, dest='beta1', help='Adam: beta1 parameter (default %(default)s)', metavar='BETA1', default=BETA1) parser.add_argument('--beta2', type=float, dest='beta2', help='Adam: beta2 parameter (default %(default)s)', metavar='BETA2', default=BETA2) parser.add_argument('--eps', type=float, dest='epsilon', help='Adam: epsilon parameter (default %(default)s)', metavar='EPSILON', default=EPSILON) parser.add_argument('--initial', dest='initial', help='initial image', metavar='INITIAL') parser.add_argument('--initial-noiseblend', type=float, dest='initial_noiseblend', help='ratio of blending initial image with normalized noise (if no initial image specified, content image is used) (default %(default)s)', metavar='INITIAL_NOISEBLEND') parser.add_argument('--preserve-colors', action='store_true', dest='preserve_colors', help='style-only transfer (preserving colors) - if color transfer is not needed') parser.add_argument('--pooling', dest='pooling', help='pooling layer configuration: max or avg (default %(default)s)', metavar='POOLING', default=POOLING) return parserdef main(): parser = build_parser() options = parser.parse_args() if not os.path.isfile(options.network): parser.error("Network %s does not exist. (Did you forget to download it?)" % options.network) content_image = imread(options.content) style_images = [imread(style) for style in options.styles] width = options.width if width is not None: new_shape = (int(math.floor(float(content_image.shape[0]) / content_image.shape[1] * width)), width) content_image = scipy.misc.imresize(content_image, new_shape) target_shape = content_image.shape for i in range(len(style_images)): style_scale = STYLE_SCALE if options.style_scales is not None: style_scale = options.style_scales[i] style_images[i] = scipy.misc.imresize(style_images[i], style_scale * target_shape[1] / style_images[i].shape[1]) style_blend_weights = options.style_blend_weights if style_blend_weights is None: # default is equal weights style_blend_weights = [1.0/len(style_images) for _ in style_images] else: total_blend_weight = sum(style_blend_weights) style_blend_weights = [weight/total_blend_weight for weight in style_blend_weights] initial = options.initial if initial is not None: initial = scipy.misc.imresize(imread(initial), content_image.shape[:2]) # Initial guess is specified, but not noiseblend - no noise should be blended if options.initial_noiseblend is None: options.initial_noiseblend = 0.0 else: # Neither inital, nor noiseblend is provided, falling back to random generated initial guess if options.initial_noiseblend is None: options.initial_noiseblend = 1.0 if options.initial_noiseblend &lt; 1.0: initial = content_image if options.checkpoint_output and "%s" not in options.checkpoint_output: parser.error("To save intermediate images, the checkpoint output " "parameter must contain `%s` (e.g. `foo%s.jpg`)") for iteration, image in stylize( network=options.network, initial=initial, initial_noiseblend=options.initial_noiseblend, content=content_image, styles=style_images, preserve_colors=options.preserve_colors, iterations=options.iterations, content_weight=options.content_weight, content_weight_blend=options.content_weight_blend, style_weight=options.style_weight, style_layer_weight_exp=options.style_layer_weight_exp, style_blend_weights=style_blend_weights, tv_weight=options.tv_weight, learning_rate=options.learning_rate, beta1=options.beta1, beta2=options.beta2, epsilon=options.epsilon, pooling=options.pooling, print_iterations=options.print_iterations, checkpoint_iterations=options.checkpoint_iterations ): output_file = None combined_rgb = image if iteration is not None: if options.checkpoint_output: output_file = options.checkpoint_output % iteration else: output_file = options.output if output_file: imsave(output_file, combined_rgb)def imread(path): img = scipy.misc.imread(path).astype(np.float) if len(img.shape) == 2: # grayscale img = np.dstack((img,img,img)) elif img.shape[2] == 4: # PNG with alpha channel img = img[:,:,:3] return imgdef imsave(path, img): img = np.clip(img, 0, 255).astype(np.uint8) Image.fromarray(img).save(path, quality=95)if __name__ == '__main__': main() 核心代码stylize.py，详解如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223# Copyright (c) 2015-2017 Anish Athalye. Released under GPLv3.import vggimport tensorflow as tfimport numpy as npfrom sys import stderrfrom PIL import ImageCONTENT_LAYERS = (&apos;relu4_2&apos;, &apos;relu5_2&apos;)STYLE_LAYERS = (&apos;relu1_1&apos;, &apos;relu2_1&apos;, &apos;relu3_1&apos;, &apos;relu4_1&apos;, &apos;relu5_1&apos;)try: reduceexcept NameError: from functools import reducedef stylize(network, initial, initial_noiseblend, content, styles, preserve_colors, iterations, content_weight, content_weight_blend, style_weight, style_layer_weight_exp, style_blend_weights, tv_weight, learning_rate, beta1, beta2, epsilon, pooling, print_iterations=None, checkpoint_iterations=None): &quot;&quot;&quot; Stylize images. This function yields tuples (iteration, image); `iteration` is None if this is the final image (the last iteration). Other tuples are yielded every `checkpoint_iterations` iterations. :rtype: iterator[tuple[int|None,image]] &quot;&quot;&quot; #content.shape是三维（height, width, channel），这里将维度变成（1, height, width, channel）为了与后面保持一致。 shape = (1,) + content.shape style_shapes = [(1,) + style.shape for style in styles] content_features = &#123;&#125; style_features = [&#123;&#125; for _ in styles] vgg_weights, vgg_mean_pixel = vgg.load_net(network) layer_weight = 1.0 style_layers_weights = &#123;&#125; for style_layer in STYLE_LAYERS: style_layers_weights[style_layer] = layer_weight layer_weight *= style_layer_weight_exp # normalize style layer weights layer_weights_sum = 0 for style_layer in STYLE_LAYERS: layer_weights_sum += style_layers_weights[style_layer] for style_layer in STYLE_LAYERS: style_layers_weights[style_layer] /= layer_weights_sum #首先创建一个image的占位符，然后通过eval()的feed_dict将content_pre传给image，启动net的运算过程，得到了content的feature maps # compute content features in feedforward mode g = tf.Graph() with g.as_default(), g.device(&apos;/cpu:0&apos;), tf.Session() as sess: image = tf.placeholder(&apos;float&apos;, shape=shape) net = vgg.net_preloaded(vgg_weights, image, pooling) content_pre = np.array([vgg.preprocess(content, vgg_mean_pixel)]) for layer in CONTENT_LAYERS: content_features[layer] = net[layer].eval(feed_dict=&#123;image: content_pre&#125;) # compute style features in feedforward mode for i in range(len(styles)): g = tf.Graph() with g.as_default(), g.device(&apos;/cpu:0&apos;), tf.Session() as sess: image = tf.placeholder(&apos;float&apos;, shape=style_shapes[i]) net = vgg.net_preloaded(vgg_weights, image, pooling) style_pre = np.array([vgg.preprocess(styles[i], vgg_mean_pixel)]) for layer in STYLE_LAYERS: features = net[layer].eval(feed_dict=&#123;image: style_pre&#125;) features = np.reshape(features, (-1, features.shape[3])) gram = np.matmul(features.T, features) / features.size style_features[i][layer] = gram initial_content_noise_coeff = 1.0 - initial_noiseblend # make stylized image using backpropogation with tf.Graph().as_default(): if initial is None: noise = np.random.normal(size=shape, scale=np.std(content) * 0.1) initial = tf.random_normal(shape) * 0.256 else: initial = np.array([vgg.preprocess(initial, vgg_mean_pixel)]) initial = initial.astype(&apos;float32&apos;) noise = np.random.normal(size=shape, scale=np.std(content) * 0.1) initial = (initial) * initial_content_noise_coeff + (tf.random_normal(shape) * 0.256) * (1.0 - initial_content_noise_coeff) &apos;&apos;&apos; image = tf.Variable(initial)初始化了一个TensorFlow的变量，即为我们需要训练的对象。注意这里我们训练的对象是一张图像，而不是weight和bias。 &apos;&apos;&apos; image = tf.Variable(initial) net = vgg.net_preloaded(vgg_weights, image, pooling) # content loss content_layers_weights = &#123;&#125; content_layers_weights[&apos;relu4_2&apos;] = content_weight_blend content_layers_weights[&apos;relu5_2&apos;] = 1.0 - content_weight_blend content_loss = 0 content_losses = [] for content_layer in CONTENT_LAYERS: content_losses.append(content_layers_weights[content_layer] * content_weight * (2 * tf.nn.l2_loss( net[content_layer] - content_features[content_layer]) / content_features[content_layer].size)) content_loss += reduce(tf.add, content_losses) # style loss style_loss = 0 &apos;&apos;&apos; 由于style图像可以输入多幅，这里使用for循环。同样的，将style_pre传给image占位符，启动net运算，得到了style的feature maps，由于style为不同filter响应的内积，因此在这里增加了一步：gram = np.matmul(features.T, features) / features.size，即为style的feature。 &apos;&apos;&apos; for i in range(len(styles)): style_losses = [] for style_layer in STYLE_LAYERS: layer = net[style_layer] _, height, width, number = map(lambda i: i.value, layer.get_shape()) size = height * width * number feats = tf.reshape(layer, (-1, number)) gram = tf.matmul(tf.transpose(feats), feats) / size style_gram = style_features[i][style_layer] style_losses.append(style_layers_weights[style_layer] * 2 * tf.nn.l2_loss(gram - style_gram) / style_gram.size) style_loss += style_weight * style_blend_weights[i] * reduce(tf.add, style_losses) # total variation denoising tv_y_size = _tensor_size(image[:,1:,:,:]) tv_x_size = _tensor_size(image[:,:,1:,:]) tv_loss = tv_weight * 2 * ( (tf.nn.l2_loss(image[:,1:,:,:] - image[:,:shape[1]-1,:,:]) / tv_y_size) + (tf.nn.l2_loss(image[:,:,1:,:] - image[:,:,:shape[2]-1,:]) / tv_x_size)) # overall loss &apos;&apos;&apos; 接下来定义了Content Loss和Style Loss，结合文中的公式很容易看懂，在代码中，还增加了total variation denoising，因此总的loss = content_loss + style_loss + tv_loss &apos;&apos;&apos; loss = content_loss + style_loss + tv_loss # optimizer setup #创建train_step，使用Adam优化器，优化对象是上面的loss #优化过程，通过迭代使用train_step来最小化loss，最终得到一个best，即为训练优化的结果 train_step = tf.train.AdamOptimizer(learning_rate, beta1, beta2, epsilon).minimize(loss) def print_progress(): stderr.write(&apos; content loss: %g\n&apos; % content_loss.eval()) stderr.write(&apos; style loss: %g\n&apos; % style_loss.eval()) stderr.write(&apos; tv loss: %g\n&apos; % tv_loss.eval()) stderr.write(&apos; total loss: %g\n&apos; % loss.eval()) # optimization best_loss = float(&apos;inf&apos;) best = None with tf.Session() as sess: sess.run(tf.global_variables_initializer()) stderr.write(&apos;Optimization started...\n&apos;) if (print_iterations and print_iterations != 0): print_progress() for i in range(iterations): stderr.write(&apos;Iteration %4d/%4d\n&apos; % (i + 1, iterations)) train_step.run() last_step = (i == iterations - 1) if last_step or (print_iterations and i % print_iterations == 0): print_progress() if (checkpoint_iterations and i % checkpoint_iterations == 0) or last_step: this_loss = loss.eval() if this_loss &lt; best_loss: best_loss = this_loss best = image.eval() img_out = vgg.unprocess(best.reshape(shape[1:]), vgg_mean_pixel) if preserve_colors and preserve_colors == True: original_image = np.clip(content, 0, 255) styled_image = np.clip(img_out, 0, 255) # Luminosity transfer steps: # 1. Convert stylized RGB-&gt;grayscale accoriding to Rec.601 luma (0.299, 0.587, 0.114) # 2. Convert stylized grayscale into YUV (YCbCr) # 3. Convert original image into YUV (YCbCr) # 4. Recombine (stylizedYUV.Y, originalYUV.U, originalYUV.V) # 5. Convert recombined image from YUV back to RGB # 1 styled_grayscale = rgb2gray(styled_image) styled_grayscale_rgb = gray2rgb(styled_grayscale) # 2 styled_grayscale_yuv = np.array(Image.fromarray(styled_grayscale_rgb.astype(np.uint8)).convert(&apos;YCbCr&apos;)) # 3 original_yuv = np.array(Image.fromarray(original_image.astype(np.uint8)).convert(&apos;YCbCr&apos;)) # 4 w, h, _ = original_image.shape combined_yuv = np.empty((w, h, 3), dtype=np.uint8) combined_yuv[..., 0] = styled_grayscale_yuv[..., 0] combined_yuv[..., 1] = original_yuv[..., 1] combined_yuv[..., 2] = original_yuv[..., 2] # 5 img_out = np.array(Image.fromarray(combined_yuv, &apos;YCbCr&apos;).convert(&apos;RGB&apos;)) yield ( (None if last_step else i), img_out )def _tensor_size(tensor): from operator import mul return reduce(mul, (d.value for d in tensor.get_shape()), 1)def rgb2gray(rgb): return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])def gray2rgb(gray): w, h = gray.shape rgb = np.empty((w, h, 3), dtype=np.float32) rgb[:, :, 2] = rgb[:, :, 1] = rgb[:, :, 0] = gray return rgb 实现效果我们的原图是这样的： 风格是这样的： 我们在cmd命令行中打入下面代码(我的图片都放在examples/下）： python neural_style.py —content examples/cat.jpg —styles examples/2-style1.jpg —output y-output.jpg 然后我们看到计算机已经开始进行风格转移: （ps:我可怜的笔记本不停地跑了两个小时） 转移结束后我们可以看到输出的图片是这样的： Neural Style很有趣，我们可以通过改变参数去做很多风格的测试，会有不一样的效果。 参考资料 图像风格迁移 (Neural Style) 简史 TensorFlow实战：Neural Style 【Paper翻译】A Neural Algorithm Artistic Style]]></content>
      <categories>
        <category>DeepLearning</category>
      </categories>
      <tags>
        <tag>TensorFlow</tag>
        <tag>Neural Style</tag>
        <tag>Neural Networks</tag>
      </tags>
  </entry>
</search>
