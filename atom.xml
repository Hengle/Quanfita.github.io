<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Quanfita&#39;s Blogs</title>
  
  <subtitle>Eloim Essaim Eloim Essaim</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://quanfita.cn/"/>
  <updated>2018-09-10T00:12:35.259Z</updated>
  <id>http://quanfita.cn/</id>
  
  <author>
    <name>Quanfita</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>计算机网络笔记整理——物理层</title>
    <link href="http://quanfita.cn/2018/09/10/physical/"/>
    <id>http://quanfita.cn/2018/09/10/physical/</id>
    <published>2018-09-10T00:06:54.718Z</published>
    <updated>2018-09-10T00:12:35.259Z</updated>
    
    <content type="html"><![CDATA[<p>物理层考虑的是怎样才能在连接各种计算机的传输媒体上<strong>传输数据比特流</strong>，而不是指具体的传输媒体。</p><p>物理层协议也称为物理层<strong>规程</strong>。</p><p><strong>主要任务：确定与传输媒体的接口的一些特性。</strong></p><h2 id="一、物理层的特性"><a href="#一、物理层的特性" class="headerlink" title="一、物理层的特性"></a>一、物理层的特性</h2><p><strong>机械特性：</strong>接口所用接线器的形状和尺寸、引脚数目和排列、固定和锁定装置等。</p><p><strong>电气特性：</strong>在接口电缆的各条线上出现的电压的范围。</p><p><strong>功能特性：</strong>某条线上出现的某一电平的电压的意义。</p><p><strong>过程特性：</strong>对于不同功能的各种可能事件的出现顺序。</p><h2 id="二、数据通信的基础知识"><a href="#二、数据通信的基础知识" class="headerlink" title="二、数据通信的基础知识"></a>二、数据通信的基础知识</h2><h3 id="1、数据通信系统的模型"><a href="#1、数据通信系统的模型" class="headerlink" title="1、数据通信系统的模型"></a>1、数据通信系统的模型</h3><p>一个数据通信系统包括三大部分：<strong>源系统</strong>（或<strong>发送端、发送方</strong>）、<strong>传输系统</strong>（或<strong>传输网络</strong>）和<strong>目的系统</strong>（或<strong>接收端、接收方</strong>）。</p><p><img src="/2018/09/10/physical/" alt="12VKORZ63FTN~0VQ{O4M%T"></p><p><strong>常用术语：</strong></p><p><strong>数据 (data)</strong> —— 运送消息的实体。</p><p><strong>信号 (signal)</strong> —— 数据的电气的或电磁的表现。 </p><p><strong>模拟信号 (analogous signal)，或连续信号</strong>—— 代表消息的参数的取值是连续的。 </p><p><strong>数字信号 (digital signal)，或离散信号</strong> —— 代表消息的参数的取值是离散的。 </p><p><strong>码元 (code)</strong> —— 在使用时间域（或简称为时域）的波形表示数字信号时，代表不同离散数值的基本波形。</p><h3 id="2、有关信道的几个基本概念"><a href="#2、有关信道的几个基本概念" class="headerlink" title="2、有关信道的几个基本概念"></a>2、有关信道的几个基本概念</h3><p><strong>信道</strong> —— 一般用来表示向某一个方向传送信息的媒体。</p><p><strong>单向通信（单工通信）</strong>——只能有一个方向的通信而没有反方向的交互。</p><p><strong>双向交替通信（半双工通信）</strong>——通信的双方都可以发送信息，但不能双方同时发送(当然也就不能同时接收)。</p><p><strong>双向同时通信（全双工通信）</strong>——通信的双方可以同时发送和接收信息。 </p><p><strong>基带信号（即基本频带信号）</strong>—— 来自信源的信号。</p><p><strong>调制</strong>——为了解决“基带信号往往包含有较多的低频成分，甚至有直流成分，而许多信道并不能传输这种低频分量或直流分量。”的问题。必须对基带信号进行调制。调制分为两大类，即<strong>基带调制</strong>和<strong>带通调制</strong>。</p><p>​               基带调制：仅对基带信号的波形进行变换，使它能够与信道特性相适应。<strong>变换后的信号仍然是基带信号</strong>。把这种过程称为<strong>编码</strong>。</p><p>​               带通调制：使用<strong>载波</strong>进行调制，把基带信号的频率范围搬移到较高的频段，并转换为<strong>模拟信号</strong>，这样就能够更好地在模拟信道中传输（即仅在一段频率范围内能够通过信道） 。</p><p>​               带通信号 ：经过载波调制后的信号。</p><h4 id="（1）常用的编码方式"><a href="#（1）常用的编码方式" class="headerlink" title="（1）常用的编码方式"></a>（1）常用的编码方式</h4><p><img src="/2018/09/10/physical/" alt="QQ图片20180809094700"></p><p><strong>不归零制</strong>：正电平代表 1，负电平代表 0。</p><p><strong>归零制</strong>：正脉冲代表 1，负脉冲代表 0。</p><p><strong>曼彻斯特编码</strong>：位周期中心的向上跳变代表 0，位周期中心的向下跳变代表 1。但也可反过来定义。</p><p><strong>差分曼彻斯特编码</strong>：在每一位的中心处始终都有跳变。位开始边界有跳变代表 0，而位开始边界没有跳变代表 1。</p><h4 id="（2）基本的带通调制方法"><a href="#（2）基本的带通调制方法" class="headerlink" title="（2）基本的带通调制方法"></a>（2）基本的带通调制方法</h4><p><img src="/2018/09/10/physical/" alt="QQ图片20180809094929"></p><p><strong>调幅(AM)：</strong>载波的振幅随基带数字信号而变化。 </p><p><strong>调频(FM)：</strong>载波的频率随基带数字信号而变化。</p><p><strong>调相(PM) ：</strong>载波的初始相位随基带数字信号而变化。8</p><h3 id="3、信道的极限容量"><a href="#3、信道的极限容量" class="headerlink" title="3、信道的极限容量"></a>3、信道的极限容量</h3><p>从概念上讲，限制码元在信道上的传输速率的因素有以下两个：</p><p>（1）<strong>信道能够通过的频率范围</strong></p><p>（2）<strong>信噪比</strong>——就是信号的平均功率和噪声的平均功率之比。常记为 <strong>S/N</strong>，并用分贝 (dB) 作为度量单位。即：</p><p>​                                                               <strong>信噪比(dB) = 10 log10(S/N)    (dB)</strong> </p><p>​          在1948年，信息论的创始人香农（Shannon）推导出了著名的<strong>香农公式</strong>。香农公式指出：<strong>信道的极限信息传输速率C</strong>。</p><p>​                                                                        <strong>C = W log2(1+S/N)    (bit/s)</strong> </p><p>其中：    W 为信道的带宽（以 Hz 为单位）；        </p><p>​                S 为信道内所传信号的平均功率；        </p><p>​                N 为信道内部的高斯噪声功率。  </p><p>​           <strong>香农公式表明</strong>：</p><p>​           （1）信道的带宽或信道中的信噪比越大，则信息的极限传输速率就越高。 </p><p>​           （2）只要信息传输速率低于信道的极限信息传输速率，就一定可以找到某种办法来实现无差错的传输。 </p><h2 id="三、物理层下面的传输媒体"><a href="#三、物理层下面的传输媒体" class="headerlink" title="三、物理层下面的传输媒体"></a>三、物理层下面的传输媒体</h2><p><strong>传输媒体</strong>也称为传输介质或传输媒介，它就是数据传输系统中在发送器和接收器之间的物理通路。</p><p>传输媒体可分为两大类，即<strong>导引型传输媒体</strong>和<strong>非导引型传输媒体</strong>。</p><h3 id="1、导引型传输媒体"><a href="#1、导引型传输媒体" class="headerlink" title="1、导引型传输媒体"></a>1、导引型传输媒体</h3><p><strong>双绞线</strong>——最常用的传输媒体。模拟传输和数字传输都可以使用双绞线，其通信距离一般为几到十几公里。</p><p>​                   双绞线分为<strong>屏蔽双绞线STP</strong>和<strong>无屏蔽双绞线 UTP</strong> 。</p><p><strong>同轴电缆</strong>——具有很好的抗干扰特性，被广泛用于传输较高速率的数据。同轴电缆的带宽取决于电缆的质量。</p><p>​                       <strong>50Ω同轴电缆 —— LAN / 数字传输常用</strong></p><p>​                       <strong>75Ω同轴电缆 —— 有线电视 / 模拟传输常用</strong></p><p><strong>光缆</strong>——<strong>光纤</strong>是光纤通信的传输媒体。光纤分为<strong>多模光纤</strong>和<strong>单模光纤</strong>。</p><p>​                <strong>多模光纤</strong>——可以存在多条不同角度入射的光线在一条光纤中传输。</p><p>​                <strong>单模光纤</strong>——若光纤的直径减小到只有一个光的波长，则光纤就像一根波导那样，它可使光线一直向前传播，而不会产生多次反射。</p><p>​               <strong>光纤的优点：</strong>（1）通信容量非常大。</p><p>​                                      （2）传输损耗小，中继距离长。</p><p>​                                      （3）抗雷电和电磁干扰性能好。</p><p>​                                      （4）无串音干扰，保密性好。</p><p>​                                      （5）体积小，重量轻。</p><h3 id="2、非引导型传输媒体"><a href="#2、非引导型传输媒体" class="headerlink" title="2、非引导型传输媒体"></a>2、非引导型传输媒体</h3><p>传统的微波通信主要有两种方式，即<strong>地面微波通信</strong>和<strong>卫星通信</strong>。</p><h2 id="四、信道复用技术"><a href="#四、信道复用技术" class="headerlink" title="四、信道复用技术"></a>四、信道复用技术</h2><h3 id="1、频分复用、时分复用和统计时分复用"><a href="#1、频分复用、时分复用和统计时分复用" class="headerlink" title="1、频分复用、时分复用和统计时分复用"></a>1、频分复用、时分复用和统计时分复用</h3><p><strong>复用</strong>是通信技术中的基本概念。它允许用户使用一个共享信道进行通信，降低成本，提高利用率。</p><p>最基本的复用就是<strong>频分复用FDM</strong>和<strong>时分复用TDM</strong>。</p><p><strong>频分复用的所有用户在同意的时间占用不同的带宽资源。</strong></p><p><strong>时分复用的所以用户是在不同意的时间占用同样的频带宽度。</strong></p><h3 id="2、波分复用"><a href="#2、波分复用" class="headerlink" title="2、波分复用"></a>2、波分复用</h3><p><strong>波分复用就是光的频分复用</strong></p><h3 id="3、码分复用"><a href="#3、码分复用" class="headerlink" title="3、码分复用"></a>3、码分复用</h3><p>码分复用是一种共享信道方法，更常用的名词是<strong>码分多址(CDMA)</strong>，各用户使用经过特殊挑选的不同码型，因此各用户之间不会造成干扰，有很强的抗干扰能力，其频谱类似于白噪声，不易被敌人发现</p><p>在CDMA中，每一个比特时间再划分为m个短的间隔，称为码片；每一个站被指派一个唯一的m bit码片序列（一般为64bit/128bit）</p><p>扩频通信有两大种：<strong>直接序列扩频(DSSS)</strong>和<strong>跳频扩频(FHSS)</strong></p><p>两个不同站的码片序列正交，就是向量$S$ 和$T$ 的规格化<strong>内积</strong>都是0：</p><script type="math/tex; mode=display">S\cdot T\equiv \frac{1}{m}\sum\limits_{i=1}^mS_iT_i=0</script><p>任何一个码片向量和该码片向量自己的规格化内积都是1：</p><script type="math/tex; mode=display">S\cdot S = \frac{1}{m}\sum\limits_{i=1}^{m}S_iS_i = \frac{1}{m}\sum\limits_{i=1}^{m}S_i^2 = \frac{1}{m}\sum\limits_{i=1}^{m}(\pm 1)^2 = 1</script><h2 id="五、数字传输系统"><a href="#五、数字传输系统" class="headerlink" title="五、数字传输系统"></a>五、数字传输系统</h2><p>早期数字传输系统的缺点：</p><ol><li>速率标准不统一：多路复用的速率体系有两个互不兼容的国际标准，北美和日本的T1速率（1.544Mbit/s）和欧洲的E1速率（2.048Mbit/s）</li><li>不是同步传输</li></ol><p>1988年美国首先推出了一个数字传输标准，叫做<strong>同步光纤网(SONET)</strong>。ITU-T以美国标准SONET为基础，制定出国际标准<strong>同步数字系列(SDH)</strong></p><h2 id="六、宽带接入技术"><a href="#六、宽带接入技术" class="headerlink" title="六、宽带接入技术"></a>六、宽带接入技术</h2><p>非对称数字用户线ADSL技术是用数字技术对现有的模拟电话用户线进行改造。</p><p>第二代ADSL改进的地方：</p><ol><li>通过提高调制效率得到了更高的数据率。</li><li>采用无缝速率自适应技术，可在运营中不中断通信和不产生误码的情况下，根据线路的实时状况，自适应地调整数据率</li><li>改善线路质量评测和故障定位功能，这对提高网络的运行维护水平具有非常重要的意义</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;物理层考虑的是怎样才能在连接各种计算机的传输媒体上&lt;strong&gt;传输数据比特流&lt;/strong&gt;，而不是指具体的传输媒体。&lt;/p&gt;
&lt;p&gt;物理层协议也称为物理层&lt;strong&gt;规程&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;主要任务：确定与传输媒体的接口的一些特性
      
    
    </summary>
    
      <category term="note" scheme="http://quanfita.cn/categories/note/"/>
    
    
      <category term="计算机网络" scheme="http://quanfita.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>计算机网络笔记整理——数据链路层</title>
    <link href="http://quanfita.cn/2018/09/10/link/"/>
    <id>http://quanfita.cn/2018/09/10/link/</id>
    <published>2018-09-10T00:01:47.532Z</published>
    <updated>2018-09-10T00:12:18.781Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数据链路层"><a href="#数据链路层" class="headerlink" title="数据链路层"></a>数据链路层</h1><p>数据链路层使用的信道主要有以下两种类型：</p><p>（1）<strong>点对点信道</strong>。这种信道使用一对一的点对点通信方式。</p><p>（2）<strong>广播信道</strong>。这种信道使用一对多的广播通信方式，因此过程比较复杂。</p><h2 id="一、使用点对点信道的数据链路层"><a href="#一、使用点对点信道的数据链路层" class="headerlink" title="一、使用点对点信道的数据链路层"></a>一、使用点对点信道的数据链路层</h2><h3 id="1、数据链路和数据帧"><a href="#1、数据链路和数据帧" class="headerlink" title="1、数据链路和数据帧"></a>1、数据链路和数据帧</h3><p><strong>链路</strong>——一条无源的点到点的物理线路段，中间没有任何其他的交换结点。</p><p>​               一条链路只是一条通路的一个组成部分。</p><p><strong>数据链路</strong>——除了物理线路外，还必须有通信协议来控制这些数据的传输。若把实现这些协议的硬件和软件加到链      路上，就构成了数据链路。一般的适配器都包括了<strong>数据链路层</strong>和<strong>物理层</strong>这两层的功能。 </p><p><strong>帧</strong>——数据链路层的协议数据单元。</p><p><img src="https://img-blog.csdn.net/2018091007472435?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><h3 id="2、三个基本问题"><a href="#2、三个基本问题" class="headerlink" title="2、三个基本问题"></a>2、三个基本问题</h3><p><strong>（1）封装成帧</strong>——就是在一段数据的前后分别添加首部和尾部，然后就构成了一个帧。确定帧的界限。首部和尾部的一个重要作用就是进行<strong>帧定界</strong>。<strong>数据部分长度上限称为最大传送单元MTU。</strong></p><p><img src="https://img-blog.csdn.net/20180910074739532?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p><strong>（2）透明传输</strong>——“透明”表示：<strong>某一个实际存在的事物看起来却好像不存在一样。</strong></p><p>解决透明传输的方法：</p><p>A.  <strong>比特填充</strong></p><p>B.  <strong>字节填充</strong>（或<strong>字符填充</strong>）：发送端的数据链路层在数据中出现控制字符“SOH”或“EOT”的前面插入一个<strong>转义字符“ESC“</strong>(其十六进制编码是 1B，二进制是00011011)。接收端的数据链路层在将数据送往网络层之前删除插入的转义字符。</p><p><img src="https://img-blog.csdn.net/20180910074755756?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p><strong>(3)  差错填充</strong>——在传输过程中可能会产生<strong>比特差错</strong>：1 可能会变成 0 而 0 也可能变成 1。</p><p>​                             在一段时间内，传输错误的比特占所传输比特总数的比率称为<strong>误码率 BER</strong>。</p><p>​                             在数据链路层传送的帧中，广泛使用了<strong>循环冗余检验 CRC</strong> 的检错技术。</p><p>​                             CRC是一种<strong>检错方法</strong>，FCS是添加在数据后面的<strong>冗余码</strong>。</p><p><strong>冗余码的计算：</strong></p><p>假设待传送的一组数据 M = 101001（现在 k = 6）。我们在 M 的后面再添加供差错检测用的 n 位冗余码一起发送。  </p><p>A. 用二进制的模 2 运算进行 2n 乘 M 的运算，这相当于在 M 后面添加 n 个 0。</p><p>B. 得到的 (k + n) 位的数除以事先选定好的长度为 (n + 1) 位的除数 P，得出商是 Q 而余数是 R，余数 R 比除数 P 少 1 位，即 R 是 n 位。 </p><p>C. 将余数 R 作为冗余码拼接在数据 M 后面发送出去。</p><p>在数据后面添加上的冗余码称为<strong>帧检验序列 FCS</strong>。</p><p>而<strong>循环冗余检测CRC</strong>和<strong>帧检验序列FCS</strong>并不等同，其中CRC是一种<strong>检错方法</strong>，FCS是添加在数据后面的<strong>冗余码</strong>。</p><p>总之，在接收端对收到的每一帧经过CRC检验后，<strong>有以下两种情况</strong>：</p><p>A. 若得出的余数 R = 0，则判定这个帧没有差错，就<strong>接受</strong> 。</p><p>B. 若余数 R  0，则判定这个帧有差错，就<strong>丢弃</strong>。</p><p><strong> 注意  </strong></p><p>仅用循环冗余检验 CRC 差错检测技术只能做到<strong>无差错接受</strong>。“无差错接受”是指：“<strong>凡是接受的帧（即不包括丢弃的帧），我们都能以非常接近于 1 的概率认为这些帧在传输过程中没有产生差错</strong>”。</p><h2 id="二、点对点协议PPP"><a href="#二、点对点协议PPP" class="headerlink" title="二、点对点协议PPP"></a>二、点对点协议PPP</h2><h3 id="1、PPP协议的特点"><a href="#1、PPP协议的特点" class="headerlink" title="1、PPP协议的特点"></a>1、PPP协议的特点</h3><p>（1）PPP协议应满足的需求</p><p><strong>简单 —— 这是首要的要求。</strong></p><p><strong>封装成帧</strong> —— 必须规定特殊的字符作为帧定界符。</p><p><strong>透明性</strong> —— 必须保证数据传输的透明性。</p><p><strong>多种网络层协议</strong> —— 能够在同一条物理链路上同时支持多种网络层协议。</p><p><strong>多种类型链路</strong> —— 能够在多种类型的链路上运行。</p><p><strong>差错检测</strong> —— 能够对接收端收到的帧进行检测，并立即丢弃有差错的帧。</p><p><strong>检测连接状态</strong> —— 能够及时自动检测出链路是否处于正常工作状态。</p><p><strong>最大传送单元</strong> —— 必须对每一种类型的点对点链路设置最大传送单元  MTU 的标准默认值，促进各种实现之间的互操作性。</p><p><strong>网络层地址协商</strong> —— 必须提供一种机制使通信的两个网络层实体能够通过协商知道或能够配置彼此的网络层地址。</p><p><strong>数据压缩协商</strong> —— 必须提供一种方法来协商使用数据压缩算法。</p><p>（2）PPP 协议不需要的功能</p><p>纠错，流量控制，序号，多点线路，半双工或单工链路</p><p>（3）PPP协议的三个组成部分</p><p>A. 一个将 IP 数据报封装到串行链路的方法。</p><p>B. <strong>链路控制协议 LCP</strong>。</p><p>C. <strong>网络控制协议 NCP</strong>。</p><h3 id="2、PPP协议的帧格式"><a href="#2、PPP协议的帧格式" class="headerlink" title="2、PPP协议的帧格式"></a>2、PPP协议的帧格式</h3><p>（1）各字段的意义</p><p><img src="https://img-blog.csdn.net/20180910074821757?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p>PPP 帧的首部和尾部分别为 4 个字段和 2 个字段。</p><p><strong>标志字段 F = 0x7E</strong> （符号“0x”表示后面的字符是用十六进制表示。十六进制的 7E 的二进制表示是 01111110）</p><p><strong>地址字段 A</strong> 只置为 0xFF。地址字段实际上并不起作用。</p><p><strong>控制字段 C</strong> 通常置为 0x03。PPP 是面向字节的，所有的 PPP 帧的长度都是整数字节。</p><p>（2）字节填充</p><p>A.  将信息字段中出现的每一个 0x7E 字节转变成为 2 字节序列 (0x7D, 0x5E)。 </p><p>B.  若信息字段中出现一个 0x7D 的字节, 则将其转变成为 2 字节序列 (0x7D, 0x5D)。</p><p>C.  若信息字段中出现 ASCII 码的控制字符（即数值小于 0x20 的字符），则在该字符前面要加入一个 0x7D 字节，同时将该字符的编码加以改变。  </p><p>（3）零比特填充</p><p>在发送端，只要发现有 5 个连续 1，则立即填入一个 0。</p><p>在接收端对帧中的比特流进行扫描。每当发现 5 个连续1时，就把这 5 个连续 1 后的一个 0 删除。</p><h3 id="3、PPP协议的工作态度"><a href="#3、PPP协议的工作态度" class="headerlink" title="3、PPP协议的工作态度"></a>3、PPP协议的工作态度</h3><p><img src="https://img-blog.csdn.net/20180910074928483?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><h2 id="三、使用广播信道的数据链路层"><a href="#三、使用广播信道的数据链路层" class="headerlink" title="三、使用广播信道的数据链路层"></a>三、使用广播信道的数据链路层</h2><h3 id="1、局域网的数据链路层"><a href="#1、局域网的数据链路层" class="headerlink" title="1、局域网的数据链路层"></a>1、局域网的数据链路层</h3><p>（1）局域网最主要的特点是：<strong>网络为一个单位所拥有；地理范围和站点数目均有限。</strong> </p><p>（2）局域网具有如下主要优点：</p><p>​         A. 具有广播功能，从一个站点可很方便地访问全网。局域网上的主机可共享连接在局域网上的各种硬件和软件资源。</p><p>​        B. 便于系统的扩展和逐渐地演变，各设备的位置可灵活调整和改变。</p><p>​        C. 提高了系统的可靠性、可用性和残存性。</p><p>（3）媒体共享技术：<strong>静态划分信道</strong>，<strong>动态媒体接入控制</strong>（又称多点接入）。其中静态划分信道又分为：<strong>频分复用</strong>、<strong>时分复用</strong>、<strong>波分复用</strong>、<strong>码分复用</strong>；动态媒体接入控制又分为：<strong>随机接入</strong>和<strong>受控接入</strong>。</p><p>为了使数据链路层能更好地适应多种局域网标准，IEEE 802 委员会就将局域网的数据链路层拆成两个子层：<strong>逻辑链路控制 LLC子层</strong>；<strong>媒体接入控制 MAC子层</strong>。</p><h3 id="2、CSMA-CD协议"><a href="#2、CSMA-CD协议" class="headerlink" title="2、CSMA/CD协议"></a>2、CSMA/CD协议</h3><p>（1）为了通信的简便，以太网采取了两种重要的措施：</p><p>A. 采用较为灵活的<strong>无连接</strong>的工作方式</p><p>B. 以太网发送的数据都使用<strong>曼彻斯特编码</strong></p><p>（2）CSMA/CD协议的要点</p><p><strong>多点接入</strong>——表示许多计算机以多点接入的方式连接在一根总线上。</p><p><strong>载波监听</strong>——指每一个站在发送数据之前先要检测一下总线上是否有其他计算机在发送数据，如果有，则暂时不要发送数据，以免发生碰撞。</p><p><strong>碰撞检测</strong>（即<strong>边发送边监听</strong>）——计算机边发送数据边检测信道上的信号电压大小。</p><p>使用 CSMA/CD 协议的以太网不能进行全双工通信而<strong>只能进行双向交替通信（半双工通信）</strong>。</p><p>（3）二进制指数类型退避算法</p><p>A. 发生碰撞的站在停止发送数据后，要推迟（退避）一个<strong>随机时间</strong>才能再发送数据。</p><p>B. <strong>基本退避时间取为争用期 2t</strong>。以太网的端到端往返时延 2t 称为<strong>争用期</strong>，或碰撞窗口。</p><p>C. 从整数集合 [0, 1, … , (2k 1)] 中<strong>随机</strong>地取出一个数，记为 r。重传所需的时延就是 r 倍的基本退避时间。</p><p>D. 参数 k 按下面的公式计算：              </p><p>​                                                            <strong>k = Min[重传次数, 10]</strong></p><p>E. 当 k =&lt; 10 时，参数 k 等于重传次数。</p><p>F. 当重传达 16 次仍不能成功时即丢弃该帧，并向高层报告。 </p><p>综上所述，CSMA/CD协议的要点如下：</p><p>A. <strong>准备发送</strong>。但在发送之前，必须先检测信道。</p><p>B. <strong>检测信道</strong>。若检测到信道忙，则应不停地检测，一直等待信道转为空闲。若检测到信道空闲，并在 96 比特时间</p><p>内信道保持空闲（保证了帧间最小间隔），就发送这个帧。</p><p>C. <strong>检查碰撞</strong>。在发送过程中仍不停地检测信道，即网络适配器要边发送边监听。</p><p>这里只有两种可能性：</p><p>①发送成功：在争用期内一直未检测到碰撞。这个帧肯定能够发送成功。发送完毕后，其他什么也不做。然后回到 (1)。</p><p>②发送失败：在争用期内检测到碰撞。这时立即停止发送数据，并按规定发送人为干扰信号。适配器接着就执行指数退避算法，等待 r 倍 512 比特时间后，返回到步骤 (2)，继续检测信道。但若重传达 16 次仍不能成功，则停止重传而向上报错。</p><h3 id="3、使用集线器的星形拓扑"><a href="#3、使用集线器的星形拓扑" class="headerlink" title="3、使用集线器的星形拓扑"></a>3、使用集线器的星形拓扑</h3><p><strong>集线器</strong>——采用双绞线的以太网采用星形拓扑，在星形的中心则增加了一种可靠性非常高的设备。</p><p><strong>集线器的特点：</strong></p><p>(1) 集线器是使用电子器件来模拟实际电缆线的工作，因此整个系统仍然像一个传统的以太网那样运行。 </p><p>(2) 使用集线器的以太网在<strong>逻辑上仍是一个总线网</strong>，各站<strong>共享逻辑上的总线</strong>，使用的还是CSMA/CD协议。网络中的各站必须竞争对传输媒体的控制，并且在同一个时刻至少只允许一个站发送数据。</p><p>(3) 集线器很像一个<strong>多接口的转发器</strong>，<strong>工作在物理层</strong>。</p><p>(4) 集线器采用了专门的芯片，进行自适应串音回波抵消，减少了近端串音。</p><h3 id="4、以太网的信道利用率"><a href="#4、以太网的信道利用率" class="headerlink" title="4、以太网的信道利用率"></a>4、以太网的信道利用率</h3><p><strong>以太网总的信道利用率并不能达到100%</strong> 。</p><p>要提高以太网的信道利用率，就必须减小t与 <script type="math/tex">T_0</script> 之比。在以太网中定义了参数 <script type="math/tex">α</script>，它是以太网单程端到端时延t 与帧的发送时间 <script type="math/tex">T_0</script> 之比： </p><script type="math/tex; mode=display">α=\frac{t}{T_0}</script><p><script type="math/tex">α →0</script>，表示一发生碰撞就立即可以检测出来， 并立即停止发送，因而信道利用率很高。</p><p><script type="math/tex">α</script> 越大，表明争用期所占的比例增大，每发生一次碰撞就浪费许多信道资源，使得信道利用率明显降低。 </p><h3 id="5、以太网的MAC层"><a href="#5、以太网的MAC层" class="headerlink" title="5、以太网的MAC层"></a>5、以太网的MAC层</h3><p>（1）MAC层的硬件地址</p><p>在局域网中，<strong>硬件地址</strong>又称为<strong>物理地址</strong>或<strong>MAC地址</strong>。</p><p> IEEE 规定地址字段的第一字节的最低位为 I/G 位。I/G 表示 Individual / Group。</p><p>当 I/G位 = 0 时，地址字段表示一个<strong>单站地址</strong>。</p><p>当 I/G位 = 1 时，表示<strong>组地址</strong>，用来进行<strong>多播</strong>（以前曾译为组播）。此时，IEEE 只分配地址字段前三个字节中的 23 位。</p><p>IEEE 把地址字段第一字节的最低第 2 位规定为 G/L 位，表示 Global / Local。</p><p>当 G/L位 = 0 时，是<strong>全球管理</strong>（保证在全球没有相同的地址），厂商向IEEE购买的 OUI 都属于全球管理。</p><p>当 G/L位 = 1 时， 是<strong>本地管理</strong>，这时用户可任意分配网络上的地址。</p><p>适配器从网络上每收到一个 MAC 帧就首先用硬件检查 MAC 帧中的 MAC 地址。如果是<strong>发往本站的帧</strong>则收下，然后再进行其他的处理。否则就将此帧丢弃，不再进行其他的处理。</p><p>“发往本站的帧”包括以下三种帧：</p><p> <strong>单播帧</strong>（一对一）—— 收到的帧的MAC地址与本站的硬件地址相同。</p><p><strong>广播帧</strong>（一对全体）—— 发送给本局域网上所有站点的帧（全1地址）。</p><p><strong>多播帧</strong>（一对多）—— 发送给本局域网上一部分站点的帧。</p><p>（2）MAC帧的格式</p><p>常用的以太网 MAC 帧格式有<strong>两种标准</strong> ：DIX Ethernet V2 标准和IEEE 的 802.3 标准。</p><h2 id="四、扩展的以太网"><a href="#四、扩展的以太网" class="headerlink" title="四、扩展的以太网"></a>四、扩展的以太网</h2><h3 id="1、在物理层扩展以太网"><a href="#1、在物理层扩展以太网" class="headerlink" title="1、在物理层扩展以太网"></a>1、在物理层扩展以太网</h3><p><strong>使用光纤扩展</strong>——主机使用光纤（通常是一对光纤）和一对光纤调制解调器连接到集线器。很容易使主机和几公里以外的集线器相连接。<br><img src="https://img-blog.csdn.net/20180910075010808?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p><strong>使用集线器扩展</strong>——使用多个集线器可连成更大的、多级星形结构的以太网。</p><p><img src="https://img-blog.csdn.net/20180910075029116?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p><strong>优点</strong>：</p><p>（1）使原来属于不同碰撞域的以太网上的计算机能够进行跨碰撞域的通信。</p><p>扩大了以太网覆盖的地理范围。</p><p><strong>缺点</strong>： </p><p>（1）碰撞域增大了，但总的吞吐量并未提高。</p><p>（2）如果不同的碰撞域使用不同的数据率，那么就不能用集线器将它们互连起来。   </p><h3 id="2、在数据链路层扩展以太网"><a href="#2、在数据链路层扩展以太网" class="headerlink" title="2、在数据链路层扩展以太网"></a>2、在数据链路层扩展以太网</h3><p>最初人们使用的是<strong>网桥</strong>。网桥对收到的帧根据MAC帧的<strong>目的地址</strong>进行<strong>转发</strong>和<strong>过滤</strong>。</p><p>1990年开始使用<strong>交换式集线器</strong>，又被称为以太网<strong>交换机</strong>或<strong>第二层交换机</strong>。</p><p><strong>以太网交换机的特点：</strong></p><p>以太网交换机实质上是一个<strong>多接口的网桥</strong>。</p><p>每个接口都直接与一个单台主机或另一个以太网交换机相连，并且一般都工作在<strong>全双工方式</strong>。</p><p>以太网交换机具有<strong>并行性</strong>。</p><p>相互通信的主机都是<strong>独占传输媒体，无碰撞的传输数据</strong>。</p><p>以太网交换机的<strong>接口有存储器</strong>，能在输出端口繁忙时把到来的帧进行缓存。</p><p>以太网交换机是一种<strong>即插即用</strong>设备，其内部的帧交换表（又称为地址表）是通过<strong>自学习算法</strong>自动地逐渐建立起来的。</p><p>以太网交换机使用了<strong>专用的交换结构芯片</strong>，用硬件转发，其转发速率要比使用软件转发的网桥快很多。</p><h2 id="五、高速以太网"><a href="#五、高速以太网" class="headerlink" title="五、高速以太网"></a>五、高速以太网</h2><h3 id="1、100BASE-T以太网"><a href="#1、100BASE-T以太网" class="headerlink" title="1、100BASE-T以太网"></a>1、100BASE-T以太网</h3><p>100BASE-T 以太网又称为<strong>快速以太网</strong>。1995 年IEEE已把 100BASE-T 的快速以太网定为正式标准，其代号为 <strong>IEEE 802.3u。</strong></p><p>CSMA/CD协议对全双工方式工作的快速以太网是<strong>不起作用</strong>的，但是在半双工工作时则<strong>一定</strong>要使用CSMA/CD协议。</p><h3 id="2、吉比特以太网"><a href="#2、吉比特以太网" class="headerlink" title="2、吉比特以太网"></a>2、吉比特以太网</h3><p><strong>特点：</strong></p><p>(1)允许在 1 Gbit/s 下以全双工和半双工两种方式工作。</p><p>(2)使用 IEEE 802.3 协议规定的帧格式。</p><p><strong>(3)在半双工方式下使用 CSMA/CD 协议，全双工方式不使用 CSMA/CD 协议。</strong></p><p>(4)与 10BASE-T 和 100BASE-T 技术向后兼容。<br><img src="https://img-blog.csdn.net/20180910075045291?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p>为保持 64 字节最小帧长度，以及 100 米的网段的最大长度，吉比特以太网增加了两个功能：<strong>载波延伸</strong>和<strong>分组突发</strong></p><h3 id="3、10吉比特以太网（10GE）和更快的以太网"><a href="#3、10吉比特以太网（10GE）和更快的以太网" class="headerlink" title="3、10吉比特以太网（10GE）和更快的以太网"></a>3、10吉比特以太网（10GE）和更快的以太网</h3><p>10GE<strong>只工作在全双工方式</strong>，因此<strong>不存在争用问题 ，当然也不使用CSMA/CD协议</strong>。</p><p>以太网从 10 Mbit/s 到 100 Gbit/s 的演进证明了以太网是：</p><p>（1）可扩展的（从 10 Mbit/s 到 100 Gbit/s）；</p><p>（2）灵活的（多种传输媒体、全/半双工、共享/交换）；</p><p>（3）易于安装；</p><p>（4）稳健性好。 </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;数据链路层&quot;&gt;&lt;a href=&quot;#数据链路层&quot; class=&quot;headerlink&quot; title=&quot;数据链路层&quot;&gt;&lt;/a&gt;数据链路层&lt;/h1&gt;&lt;p&gt;数据链路层使用的信道主要有以下两种类型：&lt;/p&gt;
&lt;p&gt;（1）&lt;strong&gt;点对点信道&lt;/strong&gt;。这种信道
      
    
    </summary>
    
      <category term="note" scheme="http://quanfita.cn/categories/note/"/>
    
    
      <category term="计算机网络" scheme="http://quanfita.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow实现写诗机器人</title>
    <link href="http://quanfita.cn/2018/08/09/TensorFlow_poem/"/>
    <id>http://quanfita.cn/2018/08/09/TensorFlow_poem/</id>
    <published>2018-08-09T09:08:34.511Z</published>
    <updated>2018-08-09T11:32:10.383Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>&emsp;&emsp;在这篇博客中，我们要使用RNN生成藏头诗，你给它输入一些古诗词，它会学着生成和前面相关联的字词。同样的如果你把训练数据换成一堆姓名，它也会学着生成姓名；给它训练一堆音乐，它会学着生成音乐，甚至可以给它训练源代码。</p><p>&emsp;&emsp;我们使用文本文件做为输入、训练RNN模型，然后使用它生成和训练数据类似的文本。</p>&emsp;&emsp; 项目地址：[GitHub](https://github.com/Quanfita/TensorFlow-Poems)使用的数据集：全唐诗(43030首)：https://pan.baidu.com/s/1o7QlUhO**环境介绍：**- python 3.6- TensorFlow 1.2- i5-6200U(笔记本)##代码分析：首先，对使用的神经网络进行简单的介绍：**RNN：**<p>&emsp;&emsp;RNN这种网络的内部状态可以展示动态时序行为。不同于前馈神经网络的是，RNN让我们可以利用它内部的记忆来处理任意时序的输入序列。简言之，RNN是为了对序列数据进行建模而产生的。</p>**什么是样本序列性？**<p>&emsp;&emsp;如果样本间存在顺序关系，每个样本和它之前的样本存在关联，那么我们就可以说这些样本具有序列性。比如说，在文本中，一个词和它前面的词是有关联的；在气象数据中，一天的气温和前几天的气温是有关联的。</p><p>&emsp;&emsp;在实现过程中我们将整个程序分成5个文件，分别是main.py、poems.py、tangpoems.py、model.py、cleancn.py</p>- main.py：定义了参数和接口，可以训练或作诗- poems.py：导入训练文件开始训练- tang_poems.py：主要功能函数，包括了数据训练，和作诗过程- model.py：保存训练好的模型- cleancn.py：这里对中文字符进行处理<p>&emsp;&emsp;这里我们来看一下main.py中 的参数和接口：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_args</span><span class="params">()</span>:</span></span><br><span class="line">    parser = argparse.ArgumentParser(description=<span class="string">'Intelligence Poem Writer.'</span>)</span><br><span class="line">    help_ = <span class="string">'choose to train or generate.'</span></span><br><span class="line">    <span class="comment">#参数--train可以训练--no-train可以让机器人作诗</span></span><br><span class="line">    parser.add_argument(<span class="string">'--train'</span>, dest=<span class="string">'train'</span>, action=<span class="string">'store_true'</span>, help=help_)</span><br><span class="line">    parser.add_argument(<span class="string">'--no-train'</span>, dest=<span class="string">'train'</span>, action=<span class="string">'store_false'</span>, help=help_)</span><br><span class="line">    parser.set_defaults(train=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    args_ = parser.parse_args()</span><br><span class="line">    <span class="keyword">return</span> args_</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    args = parse_args()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">import</span> tang_poems</span><br><span class="line">    <span class="keyword">if</span> args.train:</span><br><span class="line">        tang_poems.main(<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        tang_poems.main(<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><p>&emsp;&emsp;接下来是主要的文件tangpoems.py：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> rnn_model</span><br><span class="line"><span class="keyword">from</span> poems <span class="keyword">import</span> process_poems, generate_batch</span><br><span class="line"><span class="keyword">import</span> heapq</span><br><span class="line"></span><br><span class="line">tf.app.flags.DEFINE_integer(<span class="string">'batch_size'</span>, <span class="number">64</span>, <span class="string">'batch size.'</span>)</span><br><span class="line">tf.app.flags.DEFINE_float(<span class="string">'learning_rate'</span>, <span class="number">0.01</span>, <span class="string">'learning rate.'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set this to 'main.py' relative path</span></span><br><span class="line">tf.app.flags.DEFINE_string(<span class="string">'checkpoints_dir'</span>, os.path.abspath(<span class="string">'./checkpoints/'</span>), <span class="string">'checkpoints save path.'</span>)</span><br><span class="line">tf.app.flags.DEFINE_string(<span class="string">'file_path'</span>, os.path.abspath(<span class="string">'./poems.txt'</span>), <span class="string">'file name of poems.'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tf.app.flags.DEFINE_string(<span class="string">'model_prefix'</span>, <span class="string">'poems'</span>, <span class="string">'model save prefix.'</span>)</span><br><span class="line"></span><br><span class="line">tf.app.flags.DEFINE_integer(<span class="string">'epochs'</span>, <span class="number">50</span>, <span class="string">'train how many epochs.'</span>)</span><br><span class="line"></span><br><span class="line">FLAGS = tf.app.flags.FLAGS</span><br><span class="line"></span><br><span class="line">start_token = <span class="string">'G'</span></span><br><span class="line">end_token = <span class="string">'E'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#开始训练</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_training</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(os.path.dirname(FLAGS.checkpoints_dir)):</span><br><span class="line">        os.mkdir(os.path.dirname(FLAGS.checkpoints_dir))</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(FLAGS.checkpoints_dir):</span><br><span class="line">        os.mkdir(FLAGS.checkpoints_dir)</span><br><span class="line"></span><br><span class="line">    poems_vector, word_to_int, vocabularies = process_poems(FLAGS.file_path)</span><br><span class="line">    batches_inputs, batches_outputs = generate_batch(FLAGS.batch_size, poems_vector, word_to_int)</span><br><span class="line"></span><br><span class="line">    input_data = tf.placeholder(tf.int32, [FLAGS.batch_size, <span class="keyword">None</span>])</span><br><span class="line">    output_targets = tf.placeholder(tf.int32, [FLAGS.batch_size, <span class="keyword">None</span>])</span><br><span class="line"></span><br><span class="line">    end_points = rnn_model(model=<span class="string">'lstm'</span>, input_data=input_data, output_data=output_targets, vocab_size=len(</span><br><span class="line">        vocabularies), rnn_size=<span class="number">128</span>, num_layers=<span class="number">2</span>, batch_size=<span class="number">64</span>, learning_rate=FLAGS.learning_rate)</span><br><span class="line"></span><br><span class="line">    saver = tf.train.Saver(tf.global_variables())</span><br><span class="line">    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        <span class="comment"># sess = tf_debug.LocalCLIDebugWrapperSession(sess=sess)</span></span><br><span class="line">        <span class="comment"># sess.add_tensor_filter("has_inf_or_nan", tf_debug.has_inf_or_nan)</span></span><br><span class="line">        sess.run(init_op)</span><br><span class="line"></span><br><span class="line">        start_epoch = <span class="number">0</span></span><br><span class="line">        checkpoint = tf.train.latest_checkpoint(FLAGS.checkpoints_dir)</span><br><span class="line">        <span class="keyword">if</span> checkpoint:</span><br><span class="line">            saver.restore(sess, checkpoint)</span><br><span class="line">            print(<span class="string">"[INFO] restore from the checkpoint &#123;0&#125;"</span>.format(checkpoint))</span><br><span class="line">            start_epoch += int(checkpoint.split(<span class="string">'-'</span>)[<span class="number">-1</span>])</span><br><span class="line">        print(<span class="string">'[INFO] start training...'</span>)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">for</span> epoch <span class="keyword">in</span> range(start_epoch, FLAGS.epochs):</span><br><span class="line">                n = <span class="number">0</span></span><br><span class="line">                n_chunk = len(poems_vector) // FLAGS.batch_size</span><br><span class="line">                <span class="keyword">for</span> batch <span class="keyword">in</span> range(n_chunk):</span><br><span class="line">                    loss, _, _ = sess.run([</span><br><span class="line">                        end_points[<span class="string">'total_loss'</span>],</span><br><span class="line">                        end_points[<span class="string">'last_state'</span>],</span><br><span class="line">                        end_points[<span class="string">'train_op'</span>]</span><br><span class="line">                    ], feed_dict=&#123;input_data: batches_inputs[n], output_targets: batches_outputs[n]&#125;)</span><br><span class="line">                    n += <span class="number">1</span></span><br><span class="line">                    print(<span class="string">'[INFO] Epoch: %d , batch: %d , training loss: %.6f'</span> % (epoch, batch, loss))</span><br><span class="line">                <span class="keyword">if</span> epoch % <span class="number">6</span> == <span class="number">0</span>:</span><br><span class="line">                    saver.save(sess, os.path.join(FLAGS.checkpoints_dir, FLAGS.model_prefix), global_step=epoch)</span><br><span class="line">        <span class="keyword">except</span> KeyboardInterrupt:</span><br><span class="line">            print(<span class="string">'[INFO] Interrupt manually, try saving checkpoint for now...'</span>)</span><br><span class="line">            saver.save(sess, os.path.join(FLAGS.checkpoints_dir, FLAGS.model_prefix), global_step=epoch)</span><br><span class="line">            print(<span class="string">'[INFO] Last epoch were saved, next time will start from epoch &#123;&#125;.'</span>.format(epoch))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_word</span><span class="params">(predict, vocabs)</span>:</span></span><br><span class="line">    t = np.cumsum(predict)</span><br><span class="line">    s = np.sum(predict)</span><br><span class="line">    sample = int(np.searchsorted(t, np.random.rand(<span class="number">1</span>) * s))</span><br><span class="line">    <span class="keyword">if</span> sample &gt; len(vocabs):</span><br><span class="line">        sample = len(vocabs) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> vocabs[sample]</span><br><span class="line"></span><br><span class="line"><span class="comment">#调用模型生成诗句</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_poem</span><span class="params">(begin_word)</span>:</span></span><br><span class="line">    batch_size = <span class="number">1</span></span><br><span class="line">    print(<span class="string">'[INFO] loading corpus from %s'</span> % FLAGS.file_path)</span><br><span class="line">    poems_vector, word_int_map, vocabularies = process_poems(FLAGS.file_path)</span><br><span class="line"></span><br><span class="line">    input_data = tf.placeholder(tf.int32, [batch_size, <span class="keyword">None</span>])</span><br><span class="line"></span><br><span class="line">    end_points = rnn_model(model=<span class="string">'lstm'</span>, input_data=input_data, output_data=<span class="keyword">None</span>, vocab_size=len(</span><br><span class="line">        vocabularies), rnn_size=<span class="number">128</span>, num_layers=<span class="number">2</span>, batch_size=<span class="number">64</span>, learning_rate=FLAGS.learning_rate)</span><br><span class="line"></span><br><span class="line">    saver = tf.train.Saver(tf.global_variables())</span><br><span class="line">    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        sess.run(init_op)</span><br><span class="line"></span><br><span class="line">        checkpoint = tf.train.latest_checkpoint(FLAGS.checkpoints_dir)</span><br><span class="line">        saver.restore(sess, checkpoint)</span><br><span class="line"></span><br><span class="line">        x = np.array([list(map(word_int_map.get, start_token))])</span><br><span class="line"></span><br><span class="line">        [predict, last_state] = sess.run([end_points[<span class="string">'prediction'</span>], end_points[<span class="string">'last_state'</span>]],</span><br><span class="line">                                         feed_dict=&#123;input_data: x&#125;)</span><br><span class="line">        <span class="keyword">if</span> begin_word:</span><br><span class="line">            word = begin_word</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            word = to_word(predict, vocabularies)</span><br><span class="line">        poem = <span class="string">''</span></span><br><span class="line">        <span class="keyword">while</span> word != end_token:</span><br><span class="line">            poem += word</span><br><span class="line">            x = np.zeros((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">            x[<span class="number">0</span>, <span class="number">0</span>] = word_int_map[word]</span><br><span class="line">            [predict, last_state] = sess.run([end_points[<span class="string">'prediction'</span>], end_points[<span class="string">'last_state'</span>]],</span><br><span class="line">                                             feed_dict=&#123;input_data: x, end_points[<span class="string">'initial_state'</span>]: last_state&#125;)</span><br><span class="line">            word = to_word(predict, vocabularies)</span><br><span class="line">        <span class="comment"># word = words[np.argmax(probs_)]</span></span><br><span class="line">        <span class="keyword">return</span> poem</span><br><span class="line"></span><br><span class="line"><span class="comment">#这里将生成的诗句，按照中文诗词的格式输出</span></span><br><span class="line"><span class="comment">#同时方便接入应用</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pretty_print_poem</span><span class="params">(poem)</span>:</span></span><br><span class="line">    poem_sentences = poem.split(<span class="string">'。'</span>)</span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> poem_sentences:</span><br><span class="line">        <span class="keyword">if</span> s != <span class="string">''</span> <span class="keyword">and</span> len(s) &gt; <span class="number">10</span>:</span><br><span class="line">            print(s + <span class="string">'。'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(is_train)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> is_train:</span><br><span class="line">        print(<span class="string">'[INFO] train tang poem...'</span>)</span><br><span class="line">        run_training()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'[INFO] write tang poem...'</span>)</span><br><span class="line"></span><br><span class="line">        begin_word = input(<span class="string">'开始作诗，请输入起始字:'</span>)</span><br><span class="line">        poem2 = gen_poem(begin_word)</span><br><span class="line">        pretty_print_poem(poem2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    tf.app.run()</span><br></pre></td></tr></table></figure><p>&emsp;&emsp;在poems.py中有两部分：process_poems()和generate_batch()，其中process_poems()对数据进行了导入、排序等操作，generate_batch()将数据分块，为训练做准备：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">start_token = <span class="string">'G'</span></span><br><span class="line">end_token = <span class="string">'E'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_poems</span><span class="params">(file_name)</span>:</span></span><br><span class="line">    <span class="comment"># 诗集</span></span><br><span class="line">    poems = []</span><br><span class="line">    <span class="keyword">with</span> open(file_name, <span class="string">"r"</span>, encoding=<span class="string">'utf-8'</span>, ) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f.readlines():</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                title, content = line.strip().split(<span class="string">':'</span>)</span><br><span class="line">                content = content.replace(<span class="string">' '</span>, <span class="string">''</span>)</span><br><span class="line">                <span class="keyword">if</span> <span class="string">'_'</span> <span class="keyword">in</span> content <span class="keyword">or</span> <span class="string">'('</span> <span class="keyword">in</span> content <span class="keyword">or</span> <span class="string">'（'</span> <span class="keyword">in</span> content <span class="keyword">or</span> <span class="string">'《'</span> <span class="keyword">in</span> content <span class="keyword">or</span> <span class="string">'['</span> <span class="keyword">in</span> content <span class="keyword">or</span> \</span><br><span class="line">                        start_token <span class="keyword">in</span> content <span class="keyword">or</span> end_token <span class="keyword">in</span> content:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">if</span> len(content) &lt; <span class="number">5</span> <span class="keyword">or</span> len(content) &gt; <span class="number">79</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                content = start_token + content + end_token</span><br><span class="line">                poems.append(content)</span><br><span class="line">            <span class="keyword">except</span> ValueError <span class="keyword">as</span> e:</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line">    <span class="comment"># 按诗的字数排序</span></span><br><span class="line">    poems = sorted(poems, key=<span class="keyword">lambda</span> l: len(line))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 统计每个字出现次数</span></span><br><span class="line">    all_words = []</span><br><span class="line">    <span class="keyword">for</span> poem <span class="keyword">in</span> poems:</span><br><span class="line">        all_words += [word <span class="keyword">for</span> word <span class="keyword">in</span> poem]</span><br><span class="line">    <span class="comment"># 这里根据包含了每个字对应的频率</span></span><br><span class="line">    counter = collections.Counter(all_words)</span><br><span class="line">    count_pairs = sorted(counter.items(), key=<span class="keyword">lambda</span> x: -x[<span class="number">1</span>])</span><br><span class="line">    words, _ = zip(*count_pairs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 取前多少个常用字</span></span><br><span class="line">    words = words[:len(words)] + (<span class="string">' '</span>,)</span><br><span class="line">    <span class="comment"># 每个字映射为一个数字ID</span></span><br><span class="line">    word_int_map = dict(zip(words, range(len(words))))</span><br><span class="line">    poems_vector = [list(map(<span class="keyword">lambda</span> word: word_int_map.get(word, len(words)), poem)) <span class="keyword">for</span> poem <span class="keyword">in</span> poems]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> poems_vector, word_int_map, words</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_batch</span><span class="params">(batch_size, poems_vec, word_to_int)</span>:</span></span><br><span class="line">    <span class="comment"># 每次取64首诗进行训练</span></span><br><span class="line">    n_chunk = len(poems_vec) // batch_size</span><br><span class="line">    x_batches = []</span><br><span class="line">    y_batches = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n_chunk):</span><br><span class="line">        start_index = i * batch_size</span><br><span class="line">        end_index = start_index + batch_size</span><br><span class="line"></span><br><span class="line">        batches = poems_vec[start_index:end_index]</span><br><span class="line">        <span class="comment"># 找到这个batch的所有poem中最长的poem的长度</span></span><br><span class="line">        length = max(map(len, batches))</span><br><span class="line">        <span class="comment"># 填充一个这么大小的空batch，空的地方放空格对应的index标号</span></span><br><span class="line">        x_data = np.full((batch_size, length), word_to_int[<span class="string">' '</span>], np.int32)</span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> range(batch_size):</span><br><span class="line">            <span class="comment"># 每一行就是一首诗，在原本的长度上把诗还原上去</span></span><br><span class="line">            x_data[row, :len(batches[row])] = batches[row]</span><br><span class="line">        y_data = np.copy(x_data)</span><br><span class="line">        <span class="comment"># y的话就是x向左边也就是前面移动一个</span></span><br><span class="line">        y_data[:, :<span class="number">-1</span>] = x_data[:, <span class="number">1</span>:]</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        x_data             y_data</span></span><br><span class="line"><span class="string">        [6,2,4,6,9]       [2,4,6,9,9]</span></span><br><span class="line"><span class="string">        [1,4,2,8,5]       [4,2,8,5,5]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        x_batches.append(x_data)</span><br><span class="line">        y_batches.append(y_data)</span><br><span class="line">    <span class="keyword">return</span> x_batches, y_batches</span><br></pre></td></tr></table></figure><p>&emsp;&emsp;model.py进行了RNN模型的定义，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_model</span><span class="params">(model, input_data, output_data, vocab_size, rnn_size=<span class="number">128</span>, num_layers=<span class="number">2</span>, batch_size=<span class="number">64</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">              learning_rate=<span class="number">0.01</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    construct rnn seq2seq model.</span></span><br><span class="line"><span class="string">    :param model: model class 模型种类</span></span><br><span class="line"><span class="string">    :param input_data: input data placeholder 输入</span></span><br><span class="line"><span class="string">    :param output_data: output data placeholder 输出</span></span><br><span class="line"><span class="string">    :param vocab_size: 词长度</span></span><br><span class="line"><span class="string">    :param rnn_size: 一个RNN单元的大小</span></span><br><span class="line"><span class="string">    :param num_layers: RNN层数</span></span><br><span class="line"><span class="string">    :param batch_size: 步长</span></span><br><span class="line"><span class="string">    :param learning_rate: 学习速率</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    end_points = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rnn_cell</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">if</span> model == <span class="string">'rnn'</span>:</span><br><span class="line">            cell_fun = tf.contrib.rnn.BasicRNNCell</span><br><span class="line">        <span class="keyword">elif</span> model == <span class="string">'gru'</span>:</span><br><span class="line">            cell_fun = tf.contrib.rnn.GRUCell</span><br><span class="line">        <span class="keyword">elif</span> model == <span class="string">'lstm'</span>:</span><br><span class="line">            cell_fun = tf.contrib.rnn.BasicLSTMCell</span><br><span class="line">        cell = cell_fun(rnn_size, state_is_tuple=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">return</span> cell</span><br><span class="line">    cell = tf.contrib.rnn.MultiRNNCell([rnn_cell() <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_layers)], state_is_tuple=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> output_data <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        initial_state = cell.zero_state(batch_size, tf.float32)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        initial_state = cell.zero_state(<span class="number">1</span>, tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.device(<span class="string">"/cpu:0"</span>):</span><br><span class="line">        embedding = tf.get_variable(<span class="string">'embedding'</span>, initializer=tf.random_uniform(</span><br><span class="line">            [vocab_size + <span class="number">1</span>, rnn_size], <span class="number">-1.0</span>, <span class="number">1.0</span>))</span><br><span class="line">        inputs = tf.nn.embedding_lookup(embedding, input_data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [batch_size, ?, rnn_size] = [64, ?, 128]</span></span><br><span class="line">    outputs, last_state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state)</span><br><span class="line">    output = tf.reshape(outputs, [<span class="number">-1</span>, rnn_size])</span><br><span class="line"></span><br><span class="line">    weights = tf.Variable(tf.truncated_normal([rnn_size, vocab_size + <span class="number">1</span>]))</span><br><span class="line">    bias = tf.Variable(tf.zeros(shape=[vocab_size + <span class="number">1</span>]))</span><br><span class="line">    logits = tf.nn.bias_add(tf.matmul(output, weights), bias=bias)</span><br><span class="line">    <span class="comment"># [?, vocab_size+1]</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> output_data <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="comment"># output_data must be one-hot encode</span></span><br><span class="line">        labels = tf.one_hot(tf.reshape(output_data, [<span class="number">-1</span>]), depth=vocab_size + <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># should be [?, vocab_size+1]</span></span><br><span class="line"></span><br><span class="line">        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)</span><br><span class="line">        <span class="comment"># loss shape should be [?, vocab_size+1]</span></span><br><span class="line">        total_loss = tf.reduce_mean(loss)</span><br><span class="line">        train_op = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)</span><br><span class="line"></span><br><span class="line">        end_points[<span class="string">'initial_state'</span>] = initial_state</span><br><span class="line">        end_points[<span class="string">'output'</span>] = output</span><br><span class="line">        end_points[<span class="string">'train_op'</span>] = train_op</span><br><span class="line">        end_points[<span class="string">'total_loss'</span>] = total_loss</span><br><span class="line">        end_points[<span class="string">'loss'</span>] = loss</span><br><span class="line">        end_points[<span class="string">'last_state'</span>] = last_state</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        prediction = tf.nn.softmax(logits)</span><br><span class="line"></span><br><span class="line">        end_points[<span class="string">'initial_state'</span>] = initial_state</span><br><span class="line">        end_points[<span class="string">'last_state'</span>] = last_state</span><br><span class="line">        end_points[<span class="string">'prediction'</span>] = prediction</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> end_points</span><br></pre></td></tr></table></figure><p></p><p>&emsp;&emsp;最后cleancn.py实现了初期的数据处理工作：</p><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">this script using for clean Chinese corpus.</span></span><br><span class="line"><span class="string">you can set level for clean, i.e.:</span></span><br><span class="line"><span class="string">level='all', will clean all character that not Chinese, include punctuations</span></span><br><span class="line"><span class="string">level='normal', this will generate corpus like normal use, reserve alphabets and numbers</span></span><br><span class="line"><span class="string">level='clean', this will remove all except Chinese and Chinese punctuations</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">besides, if you want remove complex Chinese characters, just set this to be true:</span></span><br><span class="line"><span class="string">simple_only=True</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cn_punctuation_set = [<span class="string">'，'</span>, <span class="string">'。'</span>, <span class="string">'！'</span>, <span class="string">'？'</span>, <span class="string">'"'</span>, <span class="string">'"'</span>, <span class="string">'、'</span>]</span><br><span class="line">en_punctuation_set = [<span class="string">','</span>, <span class="string">'.'</span>, <span class="string">'?'</span>, <span class="string">'!'</span>, <span class="string">'"'</span>, <span class="string">'"'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean_cn_corpus</span><span class="params">(file_name, clean_level=<span class="string">'all'</span>, simple_only=True, is_save=True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    clean Chinese corpus.</span></span><br><span class="line"><span class="string">    :param file_name:</span></span><br><span class="line"><span class="string">    :param clean_level:</span></span><br><span class="line"><span class="string">    :param simple_only:</span></span><br><span class="line"><span class="string">    :param is_save:</span></span><br><span class="line"><span class="string">    :return: clean corpus in list type.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> os.path.dirname(file_name):</span><br><span class="line">        base_dir = os.path.dirname(file_name)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'not set dir. please check'</span>)</span><br><span class="line"></span><br><span class="line">    save_file = os.path.join(base_dir, os.path.basename(file_name).split(<span class="string">'.'</span>)[<span class="number">0</span>] + <span class="string">'_cleaned.txt'</span>)</span><br><span class="line">    <span class="keyword">with</span> open(file_name, <span class="string">'r+'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        clean_content = []</span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> f.readlines():</span><br><span class="line">            </span><br><span class="line">            l = l.strip()</span><br><span class="line">            <span class="keyword">if</span> l == <span class="string">''</span>:</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                l = list(l)</span><br><span class="line">                should_remove_words = []</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">for</span> w <span class="keyword">in</span> l:</span><br><span class="line">                    <span class="keyword">if</span> <span class="keyword">not</span> should_reserve(w, clean_level):</span><br><span class="line">                        should_remove_words.append(w)</span><br><span class="line">                clean_line = [c <span class="keyword">for</span> c <span class="keyword">in</span> l <span class="keyword">if</span> c <span class="keyword">not</span> <span class="keyword">in</span> should_remove_words]</span><br><span class="line">                clean_line = <span class="string">''</span>.join(clean_line)</span><br><span class="line">                <span class="keyword">if</span> clean_line != <span class="string">''</span>:</span><br><span class="line">                    clean_content.append(clean_line)</span><br><span class="line">    <span class="keyword">if</span> is_save:</span><br><span class="line">        <span class="keyword">with</span> open(save_file, <span class="string">'w+'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> l <span class="keyword">in</span> clean_content:</span><br><span class="line">                f.write(l + <span class="string">'\n'</span>)</span><br><span class="line">        print(<span class="string">'[INFO] cleaned file have been saved to %s.'</span> % save_file)</span><br><span class="line">    <span class="keyword">return</span> clean_content</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">should_reserve</span><span class="params">(w, clean_level)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> w == <span class="string">' '</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> clean_level == <span class="string">'all'</span>:</span><br><span class="line">            <span class="comment"># only reserve Chinese characters</span></span><br><span class="line">            <span class="keyword">if</span> w <span class="keyword">in</span> cn_punctuation_set <span class="keyword">or</span> w <span class="keyword">in</span> string.punctuation <span class="keyword">or</span> is_alphabet(w):</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> is_chinese(w)</span><br><span class="line">        <span class="keyword">elif</span> clean_level == <span class="string">'normal'</span>:</span><br><span class="line">            <span class="comment"># reserve Chinese characters, English alphabet, number</span></span><br><span class="line">            <span class="keyword">if</span> is_chinese(w) <span class="keyword">or</span> is_alphabet(w) <span class="keyword">or</span> is_number(w):</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">            <span class="keyword">elif</span> w <span class="keyword">in</span> cn_punctuation_set <span class="keyword">or</span> w <span class="keyword">in</span> en_punctuation_set:</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">elif</span> clean_level == <span class="string">'clean'</span>:</span><br><span class="line">            <span class="keyword">if</span> is_chinese(w):</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">            <span class="keyword">elif</span> w <span class="keyword">in</span> cn_punctuation_set:</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> <span class="string">"clean_level not support %s, please set for all, normal, clean"</span> % clean_level</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_chinese</span><span class="params">(uchar)</span>:</span></span><br><span class="line">    <span class="string">"""is chinese"""</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">u'\u4e00'</span> &lt;= uchar &lt;= <span class="string">u'\u9fa5'</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_number</span><span class="params">(uchar)</span>:</span></span><br><span class="line">    <span class="string">"""is number"""</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">u'\u0030'</span> &lt;= uchar &lt;= <span class="string">u'\u0039'</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_alphabet</span><span class="params">(uchar)</span>:</span></span><br><span class="line">    <span class="string">"""is alphabet"""</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="string">u'\u0041'</span> &lt;= uchar &lt;= <span class="string">u'\u005a'</span>) <span class="keyword">or</span> (<span class="string">u'\u0061'</span> &lt;= uchar &lt;= <span class="string">u'\u007a'</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">semi_angle_to_sbc</span><span class="params">(uchar)</span>:</span></span><br><span class="line">    <span class="string">"""半角转全角"""</span></span><br><span class="line">    inside_code = ord(uchar)</span><br><span class="line">    <span class="keyword">if</span> inside_code &lt; <span class="number">0x0020</span> <span class="keyword">or</span> inside_code &gt; <span class="number">0x7e</span>:</span><br><span class="line">        <span class="keyword">return</span> uchar</span><br><span class="line">    <span class="keyword">if</span> inside_code == <span class="number">0x0020</span>:</span><br><span class="line">        inside_code = <span class="number">0x3000</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        inside_code += <span class="number">0xfee0</span></span><br><span class="line">    <span class="keyword">return</span> chr(inside_code)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sbc_to_semi_angle</span><span class="params">(uchar)</span>:</span></span><br><span class="line">    <span class="string">"""全角转半角"""</span></span><br><span class="line">    inside_code = ord(uchar)</span><br><span class="line">    <span class="keyword">if</span> inside_code == <span class="number">0x3000</span>:</span><br><span class="line">        inside_code = <span class="number">0x0020</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        inside_code -= <span class="number">0xfee0</span></span><br><span class="line">    <span class="keyword">if</span> inside_code &lt; <span class="number">0x0020</span> <span class="keyword">or</span> inside_code &gt; <span class="number">0x7e</span>:</span><br><span class="line">        <span class="keyword">return</span> uchar</span><br><span class="line">    <span class="keyword">return</span> chr(inside_code)</span><br></pre></td></tr></table></figure><p></p><h2 id="实现效果"><a href="#实现效果" class="headerlink" title="实现效果"></a>实现效果</h2><p><strong>我们在命令行中输入：</strong></p><blockquote><p>python main.py —train </p></blockquote><p><strong>开始训练数据：</strong></p><p><img src="https://img-blog.csdn.net/20180809152554661?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p><strong>训练结束后，我们继续输入命令，让机器人开始作诗：</strong></p><blockquote><p>python main.py —no-train</p></blockquote><p>假如我们让机器人作一首以“春”字开头的诗。</p><p><strong>我们可以看到机器人为我们写好了一首诗：</strong></p><p><img src="https://img-blog.csdn.net/20180809152609899?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><h2 id="相关文章："><a href="#相关文章：" class="headerlink" title="相关文章："></a>相关文章：</h2><ol><li><a href="http://blog.csdn.net/u014365862/article/details/53868544" target="_blank" rel="noopener">TensorFlow7: 基于RNN生成古诗词</a></li><li><a href="https://baijiahao.baidu.com/s?id=1588795608910133689&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">Tensorflow：基于LSTM轻松生成各种古诗</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;p&gt;&amp;emsp;&amp;emsp;在这篇博客中，我们要使用RNN生成藏头诗，你给它输入一些古诗词，它会学着生成和前面相关联的字词。同样的如果你把训练数
      
    
    </summary>
    
      <category term="DeepLearning" scheme="http://quanfita.cn/categories/DeepLearning/"/>
    
    
      <category term="TensorFlow" scheme="http://quanfita.cn/tags/TensorFlow/"/>
    
      <category term="poem" scheme="http://quanfita.cn/tags/poem/"/>
    
      <category term="RNN" scheme="http://quanfita.cn/tags/RNN/"/>
    
      <category term="lstm" scheme="http://quanfita.cn/tags/lstm/"/>
    
  </entry>
  
  <entry>
    <title>机器学习入门之《统计学习方法》笔记整理——提升方法</title>
    <link href="http://quanfita.cn/2018/05/14/boosting/"/>
    <id>http://quanfita.cn/2018/05/14/boosting/</id>
    <published>2018-05-14T12:27:29.909Z</published>
    <updated>2018-05-14T12:33:20.816Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><p>&emsp;&emsp;提升方法的思路是综合多个分类器，得到更准确的分类结果。 说白了就是“三个臭皮匠顶个诸葛亮”。</p><h2 id="提升方法"><a href="#提升方法" class="headerlink" title="提升方法"></a>提升方法</h2><h3 id="提升方法AdaBoost算法"><a href="#提升方法AdaBoost算法" class="headerlink" title="提升方法AdaBoost算法"></a>提升方法AdaBoost算法</h3><p>&emsp;&emsp;提升方法思路比较简单，它意在通过改变训练样本之间相对的权重，从而学习出多个分类器，并将这些分类器进行线性组合，从而提高分类的性能。<br>&emsp;&emsp;从以上思路可以看出，提升方法将一个复杂的任务分配给多个专家进行判断，并且对判断的结果进行综合，这样做要比只让一个专家来判断要好，毕竟大家说好才是真的好。</p><p>&emsp;&emsp;AdaBoost是提升算法的代表，所谓提升算法，指的是一种常用的统计学习方法，应用广泛且有效。在分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提髙分类的性能。</p><h4 id="算法-AdaBoost"><a href="#算法-AdaBoost" class="headerlink" title="算法  (AdaBoost)"></a>算法  (AdaBoost)</h4><p>输入：训练数据集$T = \left \{ (x_1,y_1),(x_2,y_2),…,(x_N,y_N) \right \}$ ，其中，$x_i\in X=\mathbb{R}^n$ ，$y_i\in Y=\{-1,+1\}$ ，弱学习算法；</p><p>输出：最终分类器$G(x)$ 。</p><p>(1) 初始化训练数据的权值分布</p><script type="math/tex; mode=display">\begin{matrix}D_1=(w_{11},...,w_{1i},...,w_{1N}),& w_{1i}=\frac{1}{N}, & i=1,2...,N \end{matrix}</script><p>(2) 对$m=1,2,…,M$ </p><p>&emsp;&emsp;(a) 使用具有权值分布$D_m$ 的训练数据集学习，得到基本分类器</p><script type="math/tex; mode=display">G_m(x):X\rightarrow\{ -1,+1 \}</script><p>&emsp;&emsp;(b) 计算$G_m(x)$ 在训练数据集上的分类误差率</p><script type="math/tex; mode=display">e_m=P(G_m(x_i)\neq y_i)=\sum\limits_{i=1}^{N}w_{m_i}I(G_m(x_i)\neq y_i)</script><p>&emsp;&emsp;(c) 计算$G_m(x)$ 的系数</p><script type="math/tex; mode=display">\alpha_m=\frac{1}{2}\log\frac{1-e_m}{e_m}</script><p>这里的对数是自然对数。</p><p>&emsp;&emsp;(d) 更新训练数据集的权值分布</p><script type="math/tex; mode=display">D_{m+1}=(w_{m+1,1},...,w_{m+1,i},...,w_{m+1,N})</script><script type="math/tex; mode=display">\begin{matrix} w_{m+1,i}=\frac{w_{mi}}{Z_m}\exp(-\alpha_my_iG_m(x_i)) & i=1,2,...,N \end{matrix}</script><p>也可以写成</p><script type="math/tex; mode=display">w_{m+1,i}=\begin{cases} \frac{w_{mi}}{Z_m}e^{-\alpha_m},& G_m(x_i)=y_i \\ \frac{w_{mi}}{Z_m}e^{-\alpha_m},& G_m(x_i)\neq y_i \end{cases}</script><p>这里，$Z_m$ 是规范化因子</p><script type="math/tex; mode=display">Z_m=\sum\limits_{i=1}^{N}w_{mi}\exp(-\alpha_my_iG_m(x_i))</script><p>它使$D_{m+1}$ 成为一个概率分布。</p><p>(3) 构建基本分类器的线性组合</p><script type="math/tex; mode=display">f(x)=\sum\limits_{m=1}^{M}\alpha_mG_m(x)</script><p>得到最终分类器</p><script type="math/tex; mode=display">G(x)=sign(f(x))=sign\left( \sum\limits_{m=1}^{M}\alpha_mG_m(x) \right)</script><p>&emsp;&emsp;从以上算法可以看到：最开始步骤1，我们假设了样本具有均匀的权重分布，它将产生一个基本分类器$G_1(x)$ 。步骤2是一个m从1到M的循环过程，每一次循环都可以产生一个弱分类器。</p><ol><li>分类误差率实际上就是被误分类点的权值之和。</li><li>在计算当前弱分类器在线性组合中的系数时，当$e\geq0.5$ 时，$\alpha\geq0$，并且随着e的减小而增大，正好印证了需要使误差率小的弱分类器的权值更大这个事实。</li><li>每一个样本的权值$w$ ，都与它原来的标签<script type="math/tex">y_i</script> 以及预测的标签$G_m(x_i)$ 有关，当预测正确即它们同号时，exp指数是一个负值，这样就会减小原来样本点的权重；当预测不正确即它们异号时，exp指数是一个正值，它会增加当前样本点的权重。这正印证了我们需要使被误分类样本的权值更大这个事实。</li></ol><h3 id="AdaBoost算法的解释"><a href="#AdaBoost算法的解释" class="headerlink" title="AdaBoost算法的解释"></a>AdaBoost算法的解释</h3><p>&emsp;&emsp;AdaBoost算法还有另一个解释，即可以认为AdaBoost算法是模型为加法模型、损失函数为指数函数、学习算法为前向分步算法时的二类分类学习方法。</p><h4 id="前向分步算法"><a href="#前向分步算法" class="headerlink" title="前向分步算法"></a>前向分步算法</h4><p>&emsp;&emsp;考虑加法模型（additive model)</p><script type="math/tex; mode=display">f(x)=\sum\limits_{m=1}^{M}\beta_mb(x;\gamma_m)</script><p>&emsp;&emsp;其中，$b(x;\gamma_m)$ 为基函数，$\gamma_m$ 为基函数的参数，$\beta_m$ 为基函数的系数。显然，$f(x)=\sum\limits_{m=1}^{M}\beta_mb(x;\gamma_m)$ 是一个加法模型。</p><p>&emsp;&emsp;在给定训练数据及损失函数的条件下，学习加法模型$f(x)$ 成为经验风险极小化即损失函数极小化问题：</p><script type="math/tex; mode=display">\min\limits_{\beta_m,\gamma_m}\sum\limits_{i=1}^{N}L\left( y_i,\sum\limits_{m=1}^{M}\beta_mb(x_i;\gamma_m) \right)</script><p>&emsp;&emsp;通常这是一个复杂的优化问题。前向分步算法（forward stage wise algorithm)求解这一优化问题的想法是：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式，那么就可以简化优化的复杂度。具体地，每步只需优化如下损失函数：</p><script type="math/tex; mode=display">\min\limits_{\beta,\gamma}\sum\limits_{i=1}^{N}L\left( y_i,\beta b(x_i;\gamma) \right)</script><h4 id="算法-前向分步算法"><a href="#算法-前向分步算法" class="headerlink" title="算法  (前向分步算法)"></a>算法  (前向分步算法)</h4><p>输入：训练数据集$T=\{(x_{1},y_{1}),(x_2,y_2),…，(x_N,y_N)\}$ ，损失函数$L(y,f(x))$ 和基函数的集合$\{ b(x;\gamma) \}$ ；</p><p>输出：加法模型$f(x)$ .</p><p>(1) 初始化$f_0(x)=0$ </p><p>(2) 对$m=1,2,…,M$ </p><p>&emsp;&emsp;(a) 极小化损失函数</p><script type="math/tex; mode=display">(\beta_m,\gamma_m)=\arg \min\limits_{\beta,\gamma} \sum\limits_{i=1}^{N}L(y_i,f_{m-1}(x_i)+\beta b(x_i;\gamma))</script><p>得到参数$\beta_m,\gamma_m$ </p><p>&emsp;&emsp;(b) 更新</p><script type="math/tex; mode=display">f_m(x)=f_{m-1}(x)+\beta_mb(x;\gamma_m)</script><p>(3) 得到加法模型</p><script type="math/tex; mode=display">f(x)=f_M(x)=\sum\limits_{m=1}^{M} \beta_mb(x;\gamma_m)</script><p>&emsp;&emsp;这样，前向分步算法将同时求解从$m=1$ 到$M$ 所有参数$\beta_m,\gamma_m$ 的优化问题简化为逐次求解各个$\beta_m,\gamma_m$ 的优化问题。</p><h4 id="前向分步算法与AdaBoost"><a href="#前向分步算法与AdaBoost" class="headerlink" title="前向分步算法与AdaBoost"></a>前向分步算法与AdaBoost</h4><p>&emsp;&emsp;由前向分步算法可以推导出AdaBoost，AdaBoost算法是前向分歩加法算法的特例。这时，模型是由基本分类器组成的加法模型，损失函数是指数函数。</p><h3 id="提升树"><a href="#提升树" class="headerlink" title="提升树"></a>提升树</h3><p>&emsp;&emsp;提升树是以分类树或回归树为基本分类器的提升方法。提升树被认为是统计学习中性能最好的方法之一。</p><h4 id="提升树模型"><a href="#提升树模型" class="headerlink" title="提升树模型"></a>提升树模型</h4><p>&emsp;&emsp;提升方法实际采用加法模型（即基函数的线性组合）与前向分步算法。以决策树为基函数的提升方法称为提升树（boosting tree)。对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。在原著例题中看到的基本分类器，可以看作是由一个根结点直接连接两个叶结点的简单决策树，即所谓的决策树桩（decision stump)。提升树模型可以表示为决策树的加法模型：</p><script type="math/tex; mode=display">f_M(x)=\sum\limits_{m=1}^{M} T(x;\Theta_m)</script><p>其中，$T(x;\Theta_m)$ 表示决策树；$\Theta_m$ 为决策树的参数；$M$ 为树的个数。</p><h4 id="提升树算法"><a href="#提升树算法" class="headerlink" title="提升树算法"></a>提升树算法</h4><p>&emsp;&emsp;提升树算法采用前向分步算法。首先确定初始提升树$f_m(x)=0$ ,第$m$ 歩的模型是</p><script type="math/tex; mode=display">f_m(x)=f_{m-1}(x)+T(x;\Theta_m)</script><p>其中，$f_{m-1}(x)$ 为当前模型，通过经验风险极小化确定下一棵决策树的参数$\Theta_m$ </p><script type="math/tex; mode=display">\hat\Theta_m=\arg\min\limits_{\Theta_m} \sum\limits_{i=1}^{N}L(y_i,f_{m-1}(x_i)+T(x_i;\Theta_m))</script><p>&emsp;&emsp;由于树的线性组合可以很好地拟合训练数据，即使数据中的输入与输出之间的关系很复杂也是如此，所以提升树是一个髙功能的学习算法。</p><p>&emsp;&emsp;不同问题有大同小异的提升树学习算法，其主要区别在于使用的损失函数不同。包括用平方误差损失函数的回归问题，用指数损失函数的分类问题，以及用一般损失函数的一般决策问题。</p><p>&emsp;&emsp;对于二类分类问题，提升树算法只需将AdaBoost算法中的基本分类器限制为二类分类树即可，可以说这时的提升树算法是AdaBoost算法的特殊情况。</p><h4 id="算法-回归问题的提升树算法"><a href="#算法-回归问题的提升树算法" class="headerlink" title="算法  (回归问题的提升树算法)"></a>算法  (回归问题的提升树算法)</h4><p>输入：线性可分训练数据集$T = \left \{ (x_1,y_1),(x_2,y_2),…,(x_N,y_N) \right \}$ ，其中，$x_i\in X\subseteq\mathbb{R}^n$ ，$y_i\in Y\subseteq \mathbb{R}$ ；</p><p>输出：提升树$f_M(x)$ .</p><p>(1) 初始化$f_0(x)=0$ </p><p>(2) 对$m=1,2,…,M$ </p><p>&emsp;&emsp;(a) 计算残差</p><script type="math/tex; mode=display">\begin{matrix} r_{mi}=y_i-f_{m-1}(x_i) ,& i=1,2,...,N \end{matrix}</script><p>&emsp;&emsp;(b) 拟合残差$r_{mi}$ 学习一个回归树，得到$T(x;\Theta_m)$ </p><p>&emsp;&emsp;(c) 更新$f_m(x)=f_{m-1}(x)+T(x;\Theta_m)$ </p><p>(3) 得到回归问题提升树</p><script type="math/tex; mode=display">f_M(x)=\sum\limits_{m=1}^{M} T(x;\Theta_m)</script><h4 id="算法-梯度提升算法"><a href="#算法-梯度提升算法" class="headerlink" title="算法  (梯度提升算法)"></a>算法  (梯度提升算法)</h4><p>输入：线性可分训练数据集$T = \left \{ (x_1,y_1),(x_2,y_2),…,(x_N,y_N) \right \}$ ，其中，$x_i\in X\subseteq\mathbb{R}^n$ ，$y_i\in Y\subseteq \mathbb{R}$ ，损失函数$L(y,f(x))$ ；</p><p>输出：回归树$\hat f(x)$ .</p><p>(1) 初始化</p><script type="math/tex; mode=display">f_0(x)=\arg\min\limits_c\sum\limits_{i=1}^{N}L(y_i,c)</script><p>(2) 对$m=1,2,…,M$ </p><p>&emsp;&emsp;(a) 对$i=1,2,…,N$ ，计算</p><script type="math/tex; mode=display">r_{mi}=-\left[\frac{\partial L(y_i,f(x_i)) } {\partial f(x_i)}\right]_{f(x)=f_{m-1}(x)}</script><p>&emsp;&emsp;(b) 对$r_{mi}$ 拟合一个回归树，得到第$m$ 棵树的叶节点区域$R_{mj},j=1,2,…,J$ </p><p>&emsp;&emsp;(c) 对$j=1,2,…,J$ ，计算</p><script type="math/tex; mode=display">c_{mj}=\arg\min\limits_c\sum\limits_{x_i\in R_{mj}}L(y_i,f_{m-1}(x_i)+c)</script><p>&emsp;&emsp;(d) 更新$f_m(x)=f_{m-1}(x)+\sum\limits_{j=1}^{J}c_{mj}I(x\in R_{mj})$ </p><p>(3) 得到回归树</p><script type="math/tex; mode=display">\hat f(x)=f_M(x)=\sum\limits_{m=1}^M\sum\limits_{j=1}^Jc_{mj}I(x\in R_{mj})</script><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><ol><li><a href="http://www.hankcs.com/ml/adaboost.html" target="_blank" rel="noopener">提升方法</a></li><li><a href="https://www.cnblogs.com/itmorn/p/7751276.html" target="_blank" rel="noopener">《统计学习方法（李航）》讲义 第08章 提升方法</a></li><li><a href="http://blog.csdn.net/wy250229163/article/details/53510015" target="_blank" rel="noopener">提升方法及AdaBoost</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;p&gt;&amp;emsp;&amp;emsp;提升方法的思路是综合多个分
      
    
    </summary>
    
      <category term="note" scheme="http://quanfita.cn/categories/note/"/>
    
    
      <category term="Machine Learning" scheme="http://quanfita.cn/tags/Machine-Learning/"/>
    
      <category term="Boosting" scheme="http://quanfita.cn/tags/Boosting/"/>
    
  </entry>
  
  <entry>
    <title>机器学习入门之《统计学习方法》笔记整理——支持向量机</title>
    <link href="http://quanfita.cn/2018/05/14/SVM/"/>
    <id>http://quanfita.cn/2018/05/14/SVM/</id>
    <published>2018-05-14T12:27:13.877Z</published>
    <updated>2018-05-14T12:31:55.460Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><p>﻿## 支持向量机</p><p>&emsp;&emsp;支持向量机(support vector machines, SVM)是一种二类分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器；支持向量机还包括核技巧，这使它成为实质上的非线性分类器。支持向量机的学习策略就是<strong>间隔最大化</strong>，可形式化为一个求解凸二次规划(convex quadratic programming)的问题，也等价于正则化的合页损失函数的最小化问。支持向量机的学习算法是<strong>求解凸二次规划的最优化算法。</strong></p><p>&emsp;&emsp;支持向量机，其含义是通过<strong>支持向量</strong>运算的分类器。</p><p>&emsp;&emsp;<strong>支持向量：</strong>在求解的过程中，会发现只根据部分数据就可以确定分类器，这些数据称为支持向量。</p><h3 id="线性可分支持向量机"><a href="#线性可分支持向量机" class="headerlink" title="线性可分支持向量机"></a>线性可分支持向量机</h3><h4 id="线性可分支持向量机-1"><a href="#线性可分支持向量机-1" class="headerlink" title="线性可分支持向量机"></a>线性可分支持向量机</h4><p>&emsp;&emsp;支持向量机的输入空间和特征空间是不同的，输入空间为欧氏空间或离散集合，特征空间是欧氏空间或希尔伯特空间。希尔伯特空间其实就可以当做欧氏空间的扩展，其空间维度可以是任意维的，包括无穷维，并且具有欧氏空间不具备的完备性。</p><p>&emsp;&emsp;这时，我们需要先回忆一下<a href="http://blog.csdn.net/qq_30611601/article/details/79313609" target="_blank" rel="noopener">感知机</a> ，因为这两个的决策函数是类似的：</p><p>&emsp;&emsp;给定线性可分训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离超平面为： </p><script type="math/tex; mode=display">w^∗\cdot x+b^∗=0</script><p>&emsp;&emsp;以及相应的分类决策函数：</p><script type="math/tex; mode=display">f(x)=sign(w^∗\cdot x+b^∗)</script><p>称为线性可分支持向量机。<br><img src="http://img.blog.csdn.net/20180308194708686?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p>&emsp;&emsp;感知机通过训练一个超平面将平面或空间线性可分的点进行划分。 线性可分支持向量机也是如此，通过找寻分割平面来划分数据集。两者的区别，感知机的学习策略是误分类点到超平面距离和最小化，而线性可分支持向量机是基于硬间隔最大化的。</p><h4 id="函数间隔与几何间隔"><a href="#函数间隔与几何间隔" class="headerlink" title="函数间隔与几何间隔"></a>函数间隔与几何间隔</h4><p><strong>&emsp;&emsp;函数间隔：</strong>对于给定的训练数据集T和超平面$(w, b)$ ，定义超平面关于样本点$(x_i, y_i)$ 的函数间隔为</p><script type="math/tex; mode=display">\hat\gamma_i=y_i(w\cdot x_i+b)</script><p>&emsp;&emsp;定义超平面$(w,b)$ 关于训练数据集T的函数间隔为超平面$(w,b)$ 关于T中所有样本点$(x_i, y_i)$ 的函数间隔之最小值，即</p><script type="math/tex; mode=display">\hat\gamma=\min\limits_{i=1,2,...,N} \hat\gamma_i</script><p>&emsp;&emsp;函数间隔可以表示分类预测的正确性及确信度。但是成比例地改变$w$ 和$b$ ，例如将它们改为$2w$ 和$2b$ ，超平面并没有改变，但函数间隔却成为原来的2倍。</p><p><img src="http://img.blog.csdn.net/20180308194835385?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>&emsp;&emsp;对分离超平面的法向量、加某些约束，如规范化，$\left |w\right |=1$ ，使得间隔是确定的。这时函数间隔成为几何间隔。</p><p><strong>&emsp;&emsp;几何间隔：</strong>对于给定的训练数据集T和超平面$(w, b)$ ，定义超平面关于样本点$(x_i, y_i)$ 的函数间隔为</p><script type="math/tex; mode=display">\gamma_i=y_i\left( \frac{w}{\left\| w \right \|}\cdot x_i+\frac{b}{\left\| w \right \|} \right)</script><p>&emsp;&emsp;定义超平面$(w,b)$ 关于训练数据集T的函数间隔为超平面$(w,b)$ 关于T中所有样本点$(x_i, y_i)$ 的函数间隔之最小值，即</p><script type="math/tex; mode=display">\gamma=\min\limits_{i=1,2,...,N}\gamma_i</script><p>&emsp;&emsp;函数间隔和几何间隔的关系:</p><script type="math/tex; mode=display">\gamma_i=\frac{\hat\gamma_i}{\left\| w \right \|}</script><script type="math/tex; mode=display">\gamma=\frac{\hat\gamma}{\left\| w \right \|}</script><p>&emsp;&emsp;如果超平面参数$w$ 和$b$ 成比例地改变(超平面没有改变)，函数间隔也按此比例改变，而几何间隔不变。</p><h4 id="间隔最大化"><a href="#间隔最大化" class="headerlink" title="间隔最大化"></a>间隔最大化</h4><p>&emsp;&emsp;支持向量机学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。对线性可分的训练数据集而言，线性可分分离超平面有无穷多个(等价于感知机)，但是几何间隔最大的分离超平面是唯一的。这里的间隔最大化又称为硬间隔最大化。   </p><p>&emsp;&emsp;间隔最大化的直观解释是：对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据进行分类，也就是说，不仅将正负实例点分开，而且对最难分的实例点(离超平面最近的点)也有足够大的确信度将它们分开。</p><p>&emsp;&emsp;这个问题可以表示为下面的约束最优化问题：</p><script type="math/tex; mode=display">\begin{matrix} \max\limits_{w,b} & \gamma \\s.t. & y_i\left( \frac{w}{\left\| w \right\|}\cdot x_i+\frac{b}{\left\| w \right\|} \right) \geq \gamma, & i=1,2,...,N \end{matrix}</script><p>&emsp;&emsp;即：</p><script type="math/tex; mode=display">\begin{matrix} \max\limits_{w,b} & \frac{\hat\gamma}{\left\| w \right\|} \\s.t. & y_i\left( w\cdot x_i+b \right) \geq \hat\gamma, & i=1,2,...,N \end{matrix}</script><p>&emsp;&emsp;由于<script type="math/tex">\hat\gamma</script> 的取值并不影响最优化，所以这里我们为了计算方便取<script type="math/tex">\hat\gamma=1</script> .目标函数变为：</p><script type="math/tex; mode=display">\max\limits_{w,b} \frac{1}{\left\| w \right\|}</script><p>&emsp;&emsp;因为最大化$\frac{1}{\left| w \right|}$ 等价于最小化$\frac{1}{2}\left| w \right|^2$ (为什么？因为要将目标函数转换为一个凸二次规划问题，从而满足后面求对偶问题需要的KKT条件(什么是KKT条件？维基百科：<a href="http://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions" target="_blank" rel="noopener">KKT条件</a>)，而且使所求的解为全局最优解。系数加个1/2是为了求导的时候约去系数，计算方便。)，从而将问题改写成：</p><script type="math/tex; mode=display">\begin{matrix} \max\limits_{w,b} & \frac{1}{2}\left\|w\right\|^2 \\s.t. & y_i\left( w\cdot x_i+b \right)-1 \geq0, & i=1,2,...,N \end{matrix}</script><h4 id="算法-线性可分支持向量机学习算法——最大间隔法"><a href="#算法-线性可分支持向量机学习算法——最大间隔法" class="headerlink" title="算法  (线性可分支持向量机学习算法——最大间隔法)"></a>算法  (线性可分支持向量机学习算法——最大间隔法)</h4><p>输入：线性可分训练数据集$T = \left \{ (x_1,y_1),(x_2,y_2),…,(x_N,y_N) \right \}$ ，其中，$x_i\in X=\mathbb{R}^n$ ，$y_i\in Y=\{-1,+1\}$ ，$i=1,2,…,N$ ；</p><p>输出：最大间隔分离超平面和分类决策函数。</p><p>(1) 构造并求解约束最优化问题：</p><script type="math/tex; mode=display">\begin{matrix} \max\limits_{w,b} & \frac{1}{2}\left\|w\right\|^2 \\s.t. & y_i\left( w\cdot x_i+b \right)-1 \geq0, & i=1,2,...,N \end{matrix}</script><p>求得最优解$w^<em>,b^</em>$ .</p><p>(2) 由此得到分离超平面：</p><script type="math/tex; mode=display">w^*\cdot x+b^*=0</script><p>分类决策函数</p><script type="math/tex; mode=display">f(x)=sign(w^*\cdot x+b^*)</script><h4 id="学习的对偶算法"><a href="#学习的对偶算法" class="headerlink" title="学习的对偶算法"></a>学习的对偶算法</h4><p>&emsp;&emsp;构建拉格朗日函数(Lagrange function)，引进拉格朗日乘子(Lagrange multiplier)：</p><script type="math/tex; mode=display">L(w,b,\alpha)=\frac{1}{2}\left\|w\right\|^2-\sum\limits_{i=1}^{N} \alpha_iy_i(w\cdot x_i+b)+\sum\limits_{i=1}^{N}\alpha_i</script><p>&emsp;&emsp;根据拉格朗日对偶性，原始问题的对偶问题是拉格朗日函数的极大极小问题</p><script type="math/tex; mode=display">\max\limits_{\alpha}\min\limits_{w,b}L(w,b,\alpha)</script><p>&emsp;&emsp;设$a^<em>$是对偶最优化问题的解，则存在下标$j$ 使得$a_j^</em> &gt;0$ ，并可按下式求得原始最优化问题的解:</p><script type="math/tex; mode=display">w^*=\sum\limits_{i=1}^{N}\alpha_i^*y_ix_i</script><script type="math/tex; mode=display">b^*=y_i-\sum\limits_{i=1}^{N}\alpha_i^*y_i(x_i\cdot x_j)</script><h4 id="算法-线性可分支持向量机学习算法"><a href="#算法-线性可分支持向量机学习算法" class="headerlink" title="算法  (线性可分支持向量机学习算法)"></a>算法  (线性可分支持向量机学习算法)</h4><p>输入：线性可分训练数据集$T = \left \{ (x_1,y_1),(x_2,y_2),…,(x_N,y_N) \right \}$ ，其中，$x_i\in X=\mathbb{R}^n$ ，$y_i\in Y=\{-1,+1\}$ ，$i=1,2,…,N$ ；</p><p>输出：分离超平面和分类决策函数。</p><p>(1) 构造并求解约束最优化问题：</p><script type="math/tex; mode=display">\min\limits_{\alpha} \frac{1}{2}\sum\limits_{i=1}^{N}\sum\limits_{j=1}^{N} \alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum\limits_{i=1}^{N}\alpha_i</script><script type="math/tex; mode=display">\begin{matrix}s.t. & \sum\limits_{i=1}^{N}\alpha_iy_i=0\end{matrix}</script><script type="math/tex; mode=display">\alpha_i\geq0,i=1,2,...,N</script><p>求得最优解<script type="math/tex">\alpha^*=(\alpha_1^*,\alpha_2^*,...,\alpha_N^*)^T</script> .</p><p>(2) 计算</p><script type="math/tex; mode=display">w^*=\sum\limits_{i=1}^{N}\alpha_i^*y_ix_i</script><p>并选择$\alpha^<em>$ 的一个正分量$\alpha_j^</em>&gt;0$ ，计算</p><script type="math/tex; mode=display">b^*=y_i-\sum\limits_{i=1}^{N}\alpha_i^*y_i(x_i\cdot x_j)</script><p>(3) 求得分离超平面</p><script type="math/tex; mode=display">w^*\cdot x+b^*=0</script><p>分类决策函数：</p><script type="math/tex; mode=display">f(x)=sign(w^*\cdot x+b^*)</script><h3 id="线性支持向量机"><a href="#线性支持向量机" class="headerlink" title="线性支持向量机"></a>线性支持向量机</h3><p>&emsp;&emsp;上面所说的线性可分支持向量机是基于训练样本线性可分的理想状态，当训练样本中存在噪声或者特异点而造成线性不可分时，就需要用到线性支持向量机。 </p><p>&emsp;&emsp;在线性可分支持向量机中，我们假设函数间隔$\hat\gamma$ 为1，若存在噪声或特异点函数间隔处于 $(0,1)$ 中间，那么这些点就不满足问题的约束条件，也就线性不可分。为了解决这样的问题，引入了松弛变量$\xi_i\geq0$ ，使得函数间隔与松弛变量的和大于等于1，从而约束条件变为：</p><script type="math/tex; mode=display">y_i(w\cdot x_i+b)\geq1-\xi_i</script><p>&emsp;&emsp;同时因为约束条件引入了$\xi_i$ ，所以目标函数也要改变，改为： </p><script type="math/tex; mode=display">\frac{1}{2}\left\| w \right\|^2+C\sum\limits_{i=1}^N \xi_i</script><p>&emsp;&emsp;这里，$C&gt;0$ 称为惩罚参数，由问题决定。</p><p>&emsp;&emsp;依然构造拉格朗日函数，并转换为对偶问题： </p><script type="math/tex; mode=display">\begin{matrix}\min\limits_{\alpha} &  \frac{1}{2}\sum\limits_{i=1}^{N}\sum\limits_{j=1}^{N} \alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum\limits_{i=1}^{N}\alpha_i \end{matrix}</script><script type="math/tex; mode=display">\begin{matrix}s.t. & \sum\limits_{i=1}^{N}\alpha_iy_i=0\end{matrix}</script><script type="math/tex; mode=display">0\leq\alpha_i\leq C,i=1,2,...,N</script><p>&emsp;&emsp;其拉格朗日函数是</p><script type="math/tex; mode=display">L(w,b,\xi,\alpha,\mu)\equiv \frac{1}{2}\left\| w \right\|^2+C\sum\limits_{i=1}^N \xi_i -\sum\limits_{i=1}^{N} \alpha_i(y_i(w\cdot x_i+b)-1+\xi_i)-\sum\limits_{i=1}^{N}\mu_i\xi_i</script><p>其中，$\alpha_i\geq0,\mu_i\geq0$ .</p><h4 id="算法-线性支持向量机学习算法"><a href="#算法-线性支持向量机学习算法" class="headerlink" title="算法  (线性支持向量机学习算法)"></a>算法  (线性支持向量机学习算法)</h4><p>输入：线性可分训练数据集$T = \left \{ (x_1,y_1),(x_2,y_2),…,(x_N,y_N) \right \}$ ，其中，$x_i\in X=\mathbb{R}^n$ ，$y_i\in Y=\{-1,+1\}$ ，$i=1,2,…,N$ ；</p><p>输出：分离超平面和分类决策函数。</p><p>(1) 选择惩罚参数$C&gt;0$ ，构造并求解凸二次规划问题：</p><script type="math/tex; mode=display">\begin{matrix}\min\limits_{\alpha} &  \frac{1}{2}\sum\limits_{i=1}^{N}\sum\limits_{j=1}^{N} \alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum\limits_{i=1}^{N}\alpha_i \end{matrix}</script><script type="math/tex; mode=display">\begin{matrix}s.t. & \sum\limits_{i=1}^{N}\alpha_iy_i=0\end{matrix}</script><script type="math/tex; mode=display">0\leq\alpha_i\leq C,i=1,2,...,N</script><p>求得最优解$\alpha^<em>=(\alpha_1^</em>,\alpha_2^<em>,…,\alpha_N^</em>)^T$ .</p><p>(2) 计算</p><script type="math/tex; mode=display">w^*=\sum\limits_{i=1}^{N}\alpha_i^*y_ix_i</script><p>并选择$\alpha^<em>$ 的一个分量$\alpha_j^</em>$ 适合条件$0&lt;\alpha_j^*&lt;C$ ，计算</p><script type="math/tex; mode=display">b^*=y_i-\sum\limits_{i=1}^{N}y_i\alpha_i^*(x_i\cdot x_j)</script><p>(3) 求得分离超平面</p><script type="math/tex; mode=display">w^*\cdot x+b^*=0</script><p>分类决策函数：</p><script type="math/tex; mode=display">f(x)=sign(w^*\cdot x+b^*)</script><h4 id="支持向量"><a href="#支持向量" class="headerlink" title="支持向量"></a>支持向量</h4><p>&emsp;&emsp;支持向量有两种解释，一种是直观的解释，一种与对偶最优化问题的解$\alpha^*$ 联系起来。</p><h5 id="1-支持向量和间隔边界"><a href="#1-支持向量和间隔边界" class="headerlink" title="1. 支持向量和间隔边界"></a>1. 支持向量和间隔边界</h5><p>&emsp;&emsp;在线性可分情况下，训练数据集的样本点中与分离超平面跄离最近的样本点的实例称为<strong>支持向量( support vector )。</strong>支持向量是使约束条件式等号成立的点，即</p><script type="math/tex; mode=display">y_i(w\cdot x_i+b)-1=0</script><p>对$y_i=+1$ 的正例点，支持向量在超平面$H_1:w\cdot x+b=1$ </p><p>对$y_i=-1$ 的负例点，支持向量在超平面$H_2:w\cdot x+b=-1$ </p><p>&emsp;&emsp;$H_1$ 和$H_2$ 之间的距离称为间隔(margin)。间隔依赖于分离超平面的法向量$w$ ，等于$\frac{2}{\left|w\right|}$ 。$H_1$ 和$H_2$ 称为间隔边界。</p><p>&emsp;&emsp;在决定分离超平面时只有支持向量起作用，而其他实例点并不起作用。如果移动支持向量将改变所求的解；但是如果移动其他实例点，甚至去掉这些点，则解是不会改变的。由于支持向量在确定分离超平面中起决定性作用，所以将这种分类模型称为支持向量机。支持向量的个数一般很少，所以支持向量机由很少的“重要的”训练样本确定。</p><h5 id="2-支持向量和对偶最优化问题的解-α-∗"><a href="#2-支持向量和对偶最优化问题的解-α-∗" class="headerlink" title="2. 支持向量和对偶最优化问题的解$α^∗$"></a>2. 支持向量和对偶最优化问题的解$α^∗$</h5><p>&emsp;&emsp;在线性可分支持向量机中，$(w^∗,b^∗)$只依赖于训练数据中对应于$α^∗_i&gt;0$的样本点$(x_i,y_i)$ ，而其他样本点对$(w^∗,b^∗)$ 没有影响，将训练数据中对应于$α^∗_i&gt;0$ 的实例点$(x_i,y_i)$ 称为支持向量。 </p><p>&emsp;&emsp;支持向量一定在间隔边界上，由KKT互补条件可知: </p><script type="math/tex; mode=display">α^∗_i(y_i(w^∗⋅x_i+b^∗)−1)=0,i=1,2,⋯,N</script><p>&emsp;&emsp;对应于$α^∗_i&gt;0$ 的实例点$(x_i,y_i)$ ，则有： </p><script type="math/tex; mode=display">y_i(w^∗⋅x_i+b^∗)−1=0</script><p>&emsp;&emsp;即$(x_i,y_i)$ 一定在间隔边界上，和前面的的支持向量定义是一致的。 </p><p>&emsp;&emsp;同时可以得出，非支持向量对应的$α^∗_i=0$，因为$y_i(w^∗⋅x_i+b^∗)−1&gt;0$ ，故$α^∗_i=0$ 。</p><h4 id="合页损失函数"><a href="#合页损失函数" class="headerlink" title="合页损失函数"></a>合页损失函数</h4><p>&emsp;&emsp;线性支持向量机学习还有另外一种解释，就是最小化以下目标函数</p><script type="math/tex; mode=display">\sum\limits_{i=1}^{N}\left [1-y_i(w\cdot x_i+b) \right]_++\lambda\left\|w \right\|^2</script><p>&emsp;&emsp;目标函数的第1项是经验损失或经验风险，函数</p><script type="math/tex; mode=display">L(y(w\cdot x+b))=\left[1-y(w\cdot x+b) \right]_+</script><p>称为合页损失函数.下标“+”表示下取正值的函数</p><script type="math/tex; mode=display">\left [ z \right ]_+=\begin{cases}z, & \text{  } z>0 \\ 0 & \text{  } z\leq0\end{cases}</script><p>&emsp;&emsp;合页损失函数不仅要分类正确，而且确信度足够高时损失才是0。</p><h3 id="非线性支持向量机"><a href="#非线性支持向量机" class="headerlink" title="非线性支持向量机"></a>非线性支持向量机</h3><h4 id="核技巧"><a href="#核技巧" class="headerlink" title="核技巧"></a>核技巧</h4><p>非线性分类问题：如果能用$\mathbb{R}^n$ 中的一个超曲面将正负例正确分开，则称这个问题为非线性可分问题.</p><p>求解方法：进行非线性变换，将非线性问题变成线性问题。</p><p>&emsp;&emsp;学习是隐式地在特征空间进行的，不需要显式地定义特征空间和映射函数。这样的技巧称为<strong>核技巧</strong>。</p><p>&emsp;&emsp;核技巧应用到支持向量机，其基本想法就是通过一个非线性变换将输入空间(欧氏空间$\mathbb{R}^n$ 或离散集合)对应于一个特征空间(希尔伯特空间$H$ )，使得在输入空间$\mathbb{R}^n$ 中的超曲面模型对应于特征空间$H$ 中的超平面模型(支持向量机)。</p><p>&emsp;&emsp;设$X$ 是输入空间，$H$ 为特征空间，如果存在一个映射映射函数</p><script type="math/tex; mode=display">\phi(x):X\rightarrow H</script><p>&emsp;&emsp;使得对所有$x,z\in X$ ，函数$K(x,z)$ 满足条件</p><script type="math/tex; mode=display">K(x,z)=\phi(x)\cdot \phi(z)</script><p><strong>则称$K(x,z)$ 为核函数。</strong></p><p>&emsp;&emsp;核技巧的想法是，在学习与预测中只定义核函数$K(x,z)$ ，而不显式地定义映射函数。对于给定的核$K(x,z)$ ，特征空间x和映射函数的取法并不唯一，可以取不同的特征空间，即便是在同一特征空间里也可以取不同的映射。</p><p>&emsp;&emsp;在对偶问题的目标函数中的内积$(x_i\cdot x_j)$ 可以用核函数$K(x_i, x_j)$ 来代替：</p><script type="math/tex; mode=display">w(\alpha)= \frac{1}{2}\sum\limits_{i=1}^{N}\sum\limits_{j=1}^{N} \alpha_i\alpha_jy_iy_jK(x_i,x_j)-\sum\limits_{i=1}^{N}\alpha_i</script><p>&emsp;&emsp;分类决策函数也可用核函数代替，变为：</p><script type="math/tex; mode=display">f(x)=sign\left( \sum\limits_{i=1}^{N_s}a_i^*y_i\phi(x_i)\cdot\phi(x)+b^* \right)=sign\left( \sum\limits_{i=1}^{n_s}a_i^*y_iK(x_i,x)+b^* \right)</script><p>&emsp;&emsp;这等价于经过映射函数将原来的输入空间变换到一个新的特征空间，将输入空间中的内积$x_i\cdot x_j$ 变换为特征空间中的内积$\phi(x_i)\cdot \phi(x_j)$ .</p><p>&emsp;&emsp;在新的特征空间里从训练样本中学习线性支持向量机。当映射函数是非线性函数时，学习到的含有核函数的支持向量机是非线性分类模型。</p><p>&emsp;&emsp;在核函数给定的条件下，可以利用解线性分类问题的方法求解非线性分类问题的支持向量机。</p><p>&emsp;&emsp;这里给出判定正定核的充要条件：</p><p>&emsp;&emsp;设$Κ：X\times X→\mathbb{R}$ 是对称函数，则$Κ(x,z)$ 为正定核函数的充要条件是对任意$x_i∈X$ ,$i=1,2,…,m$,$Κ(x,z)$ 对应的Gram矩阵： </p><script type="math/tex; mode=display">K=\left [ K(x_i,x_j) \right]_{m\times n}</script><p>是半正定矩阵。</p><p>&emsp;&emsp;由充要条件可以给出判定正定核的等价定义： </p><p>&emsp;&emsp;设$X$ 为输入空间，$Κ(x,z)$ 是定义在$X\times X$ 对称函数，如果对任意$x_i∈X$ , $i=1,2,…,m$ , $Κ(x,z)$ 对应的Gram矩阵：</p><script type="math/tex; mode=display">K=\left [ K(x_i,x_j) \right]_{m\times n}</script><p>是半正定矩阵，则称$Κ(x,z)$ 是正定核。 符合这样条件的函数，我们称它为正定核函数。 </p><p><strong>注意</strong>：也有的核函数是非正定核，如多元二次核函数$K(x,z)=(\left| x-z \right|^2+c^2)^{\frac{1}{2}}$ </p><p>&emsp;&emsp;在实际应用中，还经常用到Mercer定理还确定核函数。由Mercer定理得到的核函数称为Mercer核，正定核比Mercer核更具有一般性，因为正定核要求函数为定义空间上的对称函数，而Mercer核要求函数为对称连续函数。</p><h4 id="常用核函数"><a href="#常用核函数" class="headerlink" title="常用核函数"></a>常用核函数</h4><h5 id="1-线性核函数"><a href="#1-线性核函数" class="headerlink" title="1. 线性核函数"></a>1. 线性核函数</h5><p>&emsp;&emsp;线性核函数是最简单的核函数，是径向基核函数的一个特例，公式为：</p><script type="math/tex; mode=display">K(x,z)=x^Ty+c</script><p>&emsp;&emsp;主要用于线性可分的情形，在原始空间中寻找最优线性分类器，具有参数少速度快的优势。 </p><h5 id="2-多项式核函数"><a href="#2-多项式核函数" class="headerlink" title="2. 多项式核函数"></a>2. 多项式核函数</h5><p>&emsp;&emsp;多项式核适合于正交归一化数据，公式为：</p><script type="math/tex; mode=display">K(x,z)=(x\cdot z+1)^p</script><p>&emsp;&emsp;多项式核函数属于全局核函数，允许相距很远的数据点对核函数的值有影响。参数$p$ 越大，映射的维度越高，计算量就会越大。当$p$ 过大时，学习复杂性也会过高，易出现过拟合。 </p><h5 id="3-径向基核函数"><a href="#3-径向基核函数" class="headerlink" title="3. 径向基核函数"></a>3. 径向基核函数</h5><p>&emsp;&emsp;径向基核函数属于局部核函数，当数据点距离中心点变远时，取值会变小，公式为：</p><script type="math/tex; mode=display">K(x,z)=\exp(-\gamma\left\| x-z \right\|^2)</script><h5 id="4-高斯核函数"><a href="#4-高斯核函数" class="headerlink" title="4. 高斯核函数"></a>4. 高斯核函数</h5><p>&emsp;&emsp;高斯核函数可以看作是径向基核函数的另一种形式：</p><script type="math/tex; mode=display">K(x,z)=\exp\left(-\frac{\left\| x-z \right\|^2}{2\sigma^2}\right)</script><p>&emsp;&emsp;高斯(径向基)核对数据中存在的噪声有着较好的抗干扰能力，由于其很强的局部性，其参数决定了函数作用范围，随着参数$\sigma$ 的增大而减弱。</p><h5 id="5-字符串核函数"><a href="#5-字符串核函数" class="headerlink" title="5. 字符串核函数"></a>5. 字符串核函数</h5><p>&emsp;&emsp;核函数不仅可以定义在欧氏空间上，还可以定义在离散数据的集合上。字符串核函数是定义在字符串集合上的核函数，可以直观地理解为度量一对字符串的相似度，在文本分类、信息检索等方面都有应用。</p><script type="math/tex; mode=display">k_n(s,t)=\sum\limits_{u\in\Sigma^n}[\phi_n(s)]_u[\phi_n(t)]_u=\sum\limits_{u\in\Sigma^n}\sum\limits_{(i,j):s(i)=t(j)=u}\lambda^{l(i)}\lambda^{l(j)}</script><p>&emsp;&emsp;字符串核函数$k_n(s,t)$ 给出了字符串$s$ 和$t$ 中长度等于$n$ 的所有子串组成的特征向量的余弦相似度。两个字符串相同的子串越多，他们就越相似，字符串核函数的值就越大。字符串核函数可以由动态规划快速的计算。</p><h5 id="6-Sigmoid核函数"><a href="#6-Sigmoid核函数" class="headerlink" title="6. Sigmoid核函数"></a>6. Sigmoid核函数</h5><p>&emsp;&emsp;Sigmoid核函数来源于神经网络，被广泛用于深度学习和机器学习中，公式为： </p><script type="math/tex; mode=display">K(x,z)=\tanh(x\cdot z+c)</script><p>&emsp;&emsp;采用Sigmoid函数作为核函数时，支持向量机实现的就是一种多层感知器神经网络。支持向量机的理论基础（凸二次规划）决定了它最终求得的为全局最优值而不是局部最优值，也保证了它对未知样本的良好泛化能力。 </p><h4 id="算法-非线性支持向量机学习算法"><a href="#算法-非线性支持向量机学习算法" class="headerlink" title="算法  (非线性支持向量机学习算法)"></a>算法  (非线性支持向量机学习算法)</h4><p>输入：线性可分训练数据集$T = \left \{ (x_1,y_1),(x_2,y_2),…,(x_N,y_N) \right \}$ ，其中，$x_i\in X=\mathbb{R}^n$ ，$y_i\in Y=\{-1,+1\}$ ，$i=1,2,…,N$ ；</p><p>输出：分类决策函数。</p><p>(1) 选择惩罚参数$C&gt;0$ ，构造并求解凸二次规划问题：</p><script type="math/tex; mode=display">\begin{matrix}\min\limits_{\alpha} &  \frac{1}{2}\sum\limits_{i=1}^{N}\sum\limits_{j=1}^{N} \alpha_i\alpha_jy_iy_jK(x_i,x_j)-\sum\limits_{i=1}^{N}\alpha_i \end{matrix}</script><script type="math/tex; mode=display">\begin{matrix}s.t. & \sum\limits_{i=1}^{N}\alpha_iy_i=0\end{matrix}</script><script type="math/tex; mode=display">0\leq\alpha_i\leq C,i=1,2,...,N</script><p>求得最优解$\alpha^<em>=(\alpha_1^</em>,\alpha_2^<em>,…,\alpha_N^</em>)^T$ .</p><p>(2) 选择$\alpha^<em>$ 的一个分量$\alpha_j^</em>$ 适合条件$0&lt;\alpha_j^*&lt;C$ ，计算</p><script type="math/tex; mode=display">b^*=y_i-\sum\limits_{i=1}^{N}y_i\alpha_i^*K(x_i, x_j)</script><p>(3) 分类决策函数：</p><script type="math/tex; mode=display">f(x)=sign\left( \sum\limits_{i=1}^{n_s}a_i^*y_iK(x_i,x)+b^* \right)</script><h3 id="SMO算法"><a href="#SMO算法" class="headerlink" title="SMO算法"></a>SMO算法</h3><p>&emsp;&emsp;SMO算法是一种启发式算法，其基本思路是：如果所有变量的解都满足此最优化问题的KKT条件(Karush-Kuhn-Tucker conditions)，那么这个最优化问题的解就得到了。因为KKT条件是该最优化问题的充分必要条件。否则，选择两个变量，固定其他变量，针对这两个变量构建一个二次规划问题。这个二次规划问题关于这两个变量的解应该更接近原始二次规划问题的解，因为这会使得原始二次规划问题的目标函数值变得更小。重要的是，这时子问题可以通过解析方法求解，这样就可以大大提高整个算法的计算速度。子问题有两个变量，一个是违反KKT条件最严重的那一个，另一个由约束条件自动确定。如此，SMO算法将原问题不断分解为子问题并对子问题求解，进而达到求解原问题的目的。</p><p>概要：SMO方法的中心思想是每次取一对$α_i$ 和$α_j$ ，调整这两个值。</p><p>参数：训练数据/分类数据/$C$ /$\xi$ /最大迭代数</p><p>过程：</p><blockquote><p>初始化$\alpha$为0；</p><p>在每次迭代中 （小于等于最大迭代数），</p><ul><li><p>找到第一个不满足KKT条件的训练数据，对应的$α_i$，</p></li><li><p>在其它不满足KKT条件的训练数据中，找到误差最大的x，对应的index的$α_j$ ，</p></li><li><p>$α_i$ 和$α_j$ 组成了一对，根据约束条件调整$α_i$ , $α_j$ 。</p></li></ul></blockquote><p>&emsp;&emsp;整个SMO算法包括两个部分:求解两个变量二次规划的解析方法和选择变量的启发式方法.</p><h4 id="坐标上升法"><a href="#坐标上升法" class="headerlink" title="坐标上升法"></a>坐标上升法</h4><p>&emsp;&emsp;假设有优化问题： </p><script type="math/tex; mode=display">\begin{matrix} \max\limits_{\alpha}&W(\alpha_1,\alpha_2,...,\alpha_m) \end{matrix}</script><p>&emsp;&emsp;W是α向量的函数。利用坐标上升法（求目标函数的最小时即为坐标下降法）求解问题最优的过程如下： </p><blockquote><p>//循环到函数收敛</p><p>Loop until convergence{</p><p>​    //依次选取一个变量，将其作为固定值</p><p>​    For i = 1,2,…,m{<br>​        <script type="math/tex">\alpha_i = \arg \max \limits_{\alpha_i} W(\alpha_1,\alpha_2,...,\alpha_i,...,\alpha_m)</script><br>​    }<br>}</p></blockquote><p>&emsp;&emsp;算法的思想为：每次只考虑一个变量进行优化，将其他变量固定。这时整个函数可以看作只关于该变量的函数，可以对其直接求导计算。然后继续求其他分变量的值，整个内循环下来就得到了$\alpha$ 的一组值，若该组值满足条件，即为我们求的值，否则继续迭代计算直至收敛。</p><h4 id="SMO算法-1"><a href="#SMO算法-1" class="headerlink" title="SMO算法"></a>SMO算法</h4><p>&emsp;&emsp;参考坐标上升法，我们选择向量$\alpha$ 的一个变量，将其他变量固定进行优化，该处优化问题包含了约束条件， 变量必须满足等式约束$\sum\limits_{i=1}^{N}\alpha_iy_i=0$ ，所以每次选择两个变量进行优化</p><p>&emsp;&emsp;不失一般性，将设选择的两个变量为$\alpha_1，\alpha_2$，其他变量$\alpha_i (i=3,4,…,N)$ 是固定的。 于是优化问题的子问题可以写作： </p><script type="math/tex; mode=display">\begin{matrix}\min\limits_{\alpha_1,\alpha_2} & W(\alpha_1,\alpha_2)=\frac{1}{2}K_{11}\alpha_1^2+\frac{1}{2}K_{22}\alpha_2^2+y_1y_2K_{12}\alpha_1\alpha_2-(\alpha_1+\alpha_2) \\&+y_1\alpha_1\sum\limits_{i=3}^N y_i\alpha_iK_{i1}+y_2\alpha_2\sum\limits_{i=3}^{N}y_i\alpha_iK_i2\\s.t. & \alpha_1y_1+\alpha_2y_2=-\sum\limits_{i=3}^{N}y_i\alpha_i=\varsigma \\ & 0\leq\alpha_i\leq C,i=1,2 \end{matrix}</script><p>其中，$K_{ij}=K(x_i,x_j),i,j=1,2,…,N$ ，$\varsigma$ 是常数。</p><p>&emsp;&emsp;现在的问题就是如何选择两个变量构造最优子问题。SMO采用启发式选择方法选择变量。所谓启发式，即每次选择拉格朗日乘子时，优先选择前面样本系数中满足条件$0&lt;α_i &lt; C$ 的 $α_i$ 作优化，不考虑约束条件中相等的情况是因为在界上的样例对应的系数$α_i$  一般都不会改变。 </p><p>&emsp;&emsp;通过启发式搜索找到第一个变量，因为要考虑算法的收敛性，第二个变量显然不是随便选的。实际上，只要选择的两个变量中有一个违背KKT条件，那么目标函数在一步迭代后值就会减小,并且我们希望找到的$α_2$ 在更新后能够有足够大的变化。 </p><h4 id="算法-SMO算法"><a href="#算法-SMO算法" class="headerlink" title="算法  (SMO算法)"></a>算法  (SMO算法)</h4><p>输入：线性可分训练数据集$T = \left \{ (x_1,y_1),(x_2,y_2),…,(x_N,y_N) \right \}$ ，其中，$x_i\in X=\mathbb{R}^n$ ，$y_i\in Y=\{-1,+1\}$ ，$i=1,2,…,N$ ，精度$\varepsilon$ ；</p><p>输出：近似解$\hat\alpha$。</p><p>(1) 取初值$\alpha^{(0)}=0$，令$k$ ；</p><p>(2) 选取优化变量$\alpha_1^{(k)},\alpha_2^{(k)}$ ，解析求解两个变量的最优化问题，求得最优解$\alpha_1^{(k+1)},\alpha_2^{(k+1)}$ ，更新$\alpha$ 为$\alpha^{(k+1)}$ ；</p><p>(3) 若精度$\varepsilon$ 范围内满足停机条件</p><script type="math/tex; mode=display">\sum\limits_{i=1}^{N}\alpha_iy_i=0</script><script type="math/tex; mode=display">0\leq\alpha_i\leq C,i=1,2,...,N</script><script type="math/tex; mode=display">y_i\cdot g(x_i)=\begin{cases} \geq1,& \{x_i|\alpha_i=0\} \\=1,& \{x_i|0<\alpha_i<C\} \\  \leq1,& \{x_i|\alpha_i=C\}  \end{cases}</script><p>其中，</p><script type="math/tex; mode=display">g(x_i)=\sum\limits_{j=1}^N \alpha_jy_jK(x_j,x_i)+b</script><p>则转(4) ；否则令$k=k+1$ ，转(2) ；</p><p>(4) 取$\hat\alpha=\alpha^{(k+1)}$ </p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>&emsp;&emsp;力推一篇文章，写的太好了，我写的只能算是笔记，这才是真正详细地讲述原理——<a href="http://blog.csdn.net/macyang/article/details/38782399" target="_blank" rel="noopener">支持向量机通俗导论（理解SVM的三层境界）</a></p><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><ol><li><a href="http://blog.csdn.net/shijing_0214/article/details/50982602" target="_blank" rel="noopener">理解支持向量机</a></li><li><a href="http://blog.csdn.net/shijing_0214/article/details/51000845" target="_blank" rel="noopener">理解支持向量机（二）核函数</a></li><li><a href="https://www.cnblogs.com/YongSun/p/4767130.html" target="_blank" rel="noopener">统计学习方法 李航—-第7章 支持向量机</a></li><li><a href="http://blog.csdn.net/shijing_0214/article/details/51052208" target="_blank" rel="noopener">理解数学空间，从距离到希尔伯特空间</a></li><li><a href="https://www.cnblogs.com/steven-yang/p/5658362.html" target="_blank" rel="noopener">机器学习实战 - 读书笔记(06) – SVM支持向量机</a></li><li><a href="https://www.zhihu.com/question/21094489" target="_blank" rel="noopener">支持向量机(SVM)是什么意思？</a></li><li><a href="http://blog.csdn.net/macyang/article/details/38782399" target="_blank" rel="noopener">支持向量机通俗导论（理解SVM的三层境界）</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;p&gt;﻿## 支持向量机&lt;/p&gt;
&lt;p&gt;&amp;emsp;&amp;e
      
    
    </summary>
    
      <category term="note" scheme="http://quanfita.cn/categories/note/"/>
    
    
      <category term="Machine Learning" scheme="http://quanfita.cn/tags/Machine-Learning/"/>
    
      <category term="SVM" scheme="http://quanfita.cn/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>机器学习入门实战——感知机算法实战Iris数据集</title>
    <link href="http://quanfita.cn/2018/05/14/preceptron_Iris/"/>
    <id>http://quanfita.cn/2018/05/14/preceptron_Iris/</id>
    <published>2018-05-14T12:09:38.067Z</published>
    <updated>2018-05-14T12:18:43.295Z</updated>
    
    <content type="html"><![CDATA[<p>关于感知机的相关理论知识请查看：<a href="http://quanfita.cn/2018/02/11/perceptron/">感知机</a></p><h3 id="关于Iris数据集"><a href="#关于Iris数据集" class="headerlink" title="关于Iris数据集"></a>关于Iris数据集</h3><blockquote><p>Iris也称鸢尾花卉数据集，是一类多重变量分析的数据集。数据集包含150个数据集，分为3类，每类50个数据，每个数据包含4个属性。可通过花萼长度，花萼宽度，花瓣长度，花瓣宽度4个属性预测鸢尾花卉属于（Setosa，Versicolour，Virginica）三个种类中的哪一类。</p><p>Iris以鸢尾花的特征作为数据来源，常用在分类操作中。该数据集由3种不同类型的鸢尾花的50个样本数据构成。其中的一个种类与另外两个种类是线性可分离的，后两个种类是非线性可分离的。</p><p>该数据集包含了5个属性：</p><p>&amp; Sepal.Length（花萼长度），单位是cm;</p><p>&amp; Sepal.Width（花萼宽度），单位是cm;</p><p>&amp; Petal.Length（花瓣长度），单位是cm;</p><p>&amp; Petal.Width（花瓣宽度），单位是cm;</p><p>&amp; 种类：Iris Setosa（山鸢尾）、Iris Versicolour（杂色鸢尾），以及Iris Virginica（维吉尼亚鸢尾）。</p></blockquote><h3 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a>代码实战</h3><p>先介绍一下如何搭建一个感知机，我们需要用到numpy库</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Perception</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    eta:学习率</span></span><br><span class="line"><span class="string">    n_iter:权重向量的训练次数</span></span><br><span class="line"><span class="string">    w_:神经分叉权重向量</span></span><br><span class="line"><span class="string">    error_:用于记录神经元判断出错次数</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, eta = <span class="number">0.01</span>, n_iter = <span class="number">10</span>)</span>:</span></span><br><span class="line">        self.eta = eta</span><br><span class="line">        self.n_iter = n_iter</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        输入训练数据，培训神经元，x输入样本向量，y对应样本分类</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        x:shape[n_samples, n_features]</span></span><br><span class="line"><span class="string">        x:[[1, 2, 3], [4, 5, 6]]</span></span><br><span class="line"><span class="string">        n_samples:2</span></span><br><span class="line"><span class="string">        n_features:3</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        y:[1, -1]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化权重向量为0</span></span><br><span class="line"><span class="string">        加一是因为前面算法提到的w0，也就是步调函数的阈值</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.w_ = np.zeros(<span class="number">1</span> + x.shape[<span class="number">1</span>])</span><br><span class="line">        self.errors_ = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(self.n_iter):</span><br><span class="line">            errors = <span class="number">0</span></span><br><span class="line">            <span class="string">"""</span></span><br><span class="line"><span class="string">            x:[[1, 2, 3], [4, 5, 6]]</span></span><br><span class="line"><span class="string">            y:[1, -1]</span></span><br><span class="line"><span class="string">            zip(x,y) = [[1, 2, 3, 1], [4, 5, 6, -1]]</span></span><br><span class="line"><span class="string">            """</span></span><br><span class="line">            <span class="keyword">for</span> xi, target <span class="keyword">in</span> zip(x, y):</span><br><span class="line">                <span class="string">"""</span></span><br><span class="line"><span class="string">                update = η * (y - y')</span></span><br><span class="line"><span class="string">                """</span></span><br><span class="line">                update = self.eta * (target - self.predict(xi))</span><br><span class="line">                <span class="string">"""</span></span><br><span class="line"><span class="string">                xi是一个向量</span></span><br><span class="line"><span class="string">                update * xi 等价：</span></span><br><span class="line"><span class="string">                [▽w[1]=x[1]*update,▽w[2]=x[2]*update,▽w[3]=x[3]*update]</span></span><br><span class="line"><span class="string">                """</span></span><br><span class="line">                self.w_[<span class="number">1</span>:] += update * xi</span><br><span class="line">                self.w_[<span class="number">0</span>] += update</span><br><span class="line">                </span><br><span class="line">                errors += int(update != <span class="number">0.0</span>)</span><br><span class="line">                self.errors_.append(errors)</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">net_input</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> np.dot(x, self.w_[<span class="number">1</span>:]) + self.w_[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> np.where(self.net_input(x) &gt;= <span class="number">0.0</span>, <span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>上面我们完成了一个最基本的感知机的搭建，下面我们就要开始处理数据了</p><p>首先，我们需要使用pandas库来读取数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">file = <span class="string">'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'</span></span><br><span class="line">df= pd.read_csv(file,header=<span class="keyword">None</span>)</span><br><span class="line">df.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>我们查看一下前十行数据</p><p><img src="http://img.blog.csdn.net/20180225155956807?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p>通过这些数字我们并不能明显看出什么关系，所以接下来，我们用matplotlib库画出其中两种花的两个变量的关系，我这里选取的是花瓣长度和花茎的长度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">y = df.loc[<span class="number">0</span>:<span class="number">100</span>, <span class="number">4</span>].values</span><br><span class="line">y = np.where(y == <span class="string">'Iris-setosa'</span>,<span class="number">-1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">x = df.loc[<span class="number">0</span>:<span class="number">100</span>,[<span class="number">0</span>,<span class="number">2</span>]].values</span><br><span class="line"></span><br><span class="line">plt.scatter(x[:<span class="number">50</span>,<span class="number">0</span>],x[:<span class="number">50</span>,<span class="number">1</span>],color=<span class="string">'red'</span>,marker=<span class="string">'o'</span>,label=<span class="string">'setosa'</span>)</span><br><span class="line">plt.scatter(x[<span class="number">50</span>:<span class="number">100</span>,<span class="number">0</span>],x[<span class="number">50</span>:<span class="number">100</span>,<span class="number">1</span>],color=<span class="string">'blue'</span>,marker=<span class="string">'x'</span>,label=<span class="string">'versicolor'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'花瓣的长度'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'花茎的长度'</span>)</span><br><span class="line">plt.legend(loc = <span class="string">'upper left'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>通过图像我们可以很明显地看出，这两种花具有的特点</p><p><img src="http://img.blog.csdn.net/20180225160017212?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p>下面我们调用之前定义好的感知机模型，让它学习这些数据，然后我们同样画出感知机学习过程中的错误次数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ppn = Perception(eta=<span class="number">0.1</span>, n_iter=<span class="number">10</span>)</span><br><span class="line">ppn.fit(x, y)</span><br><span class="line">plt.plot(range(<span class="number">1</span>,len(ppn.errors_) + <span class="number">1</span>),ppn.errors_, marker=<span class="string">'o'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epoch'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'错误分类次数'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>通过图中可以看出，在刚开始学习时，分类错误比较多，到后面就基本没有错误了</p><p><img src="http://img.blog.csdn.net/2018022516003744?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p>如果我们想要查看一下感知机学习出来的分离超平面可以定义如下一个绘图函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_decision_regions</span><span class="params">(x, y, classifier, resolution = <span class="number">0.02</span>)</span>:</span></span><br><span class="line">    marker = (<span class="string">'s'</span>, <span class="string">'x'</span>, <span class="string">'o'</span>, <span class="string">'v'</span>)</span><br><span class="line">    colors = (<span class="string">'red'</span>, <span class="string">'blue'</span>, <span class="string">'lightgreen'</span>, <span class="string">'gray'</span>, <span class="string">'cyan'</span>)</span><br><span class="line">    cmap = ListedColormap(colors[:len(np.unique(y))])</span><br><span class="line">    </span><br><span class="line">    x1_min, x1_max = x[:,<span class="number">0</span>].min() - <span class="number">1</span>, x[:,<span class="number">0</span>].max()</span><br><span class="line">    x2_min, x2_max = x[:,<span class="number">1</span>].min() - <span class="number">1</span>, x[:,<span class="number">1</span>].max()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#print(x1_min, x1_max)</span></span><br><span class="line">    <span class="comment">#print(x2_min, x2_max)</span></span><br><span class="line">    </span><br><span class="line">    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),</span><br><span class="line">                          np.arange(x2_min, x2_max, resolution))</span><br><span class="line">    </span><br><span class="line">    z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#print(xx1.ravel())</span></span><br><span class="line">    <span class="comment">#print(xx2.ravel())</span></span><br><span class="line">    <span class="comment">#print(z)</span></span><br><span class="line">    </span><br><span class="line">    z = z.reshape(xx1.shape)</span><br><span class="line">    plt.contourf(xx1, xx2, z, alpha=<span class="number">0.4</span>, cmap=cmap)</span><br><span class="line">    plt.xlim(xx1.min(), xx1.max())</span><br><span class="line">    plt.ylim(xx2.min(), xx2.max())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> idx, cl <span class="keyword">in</span> enumerate(np.unique(y)):</span><br><span class="line">        plt.scatter(x=x[y==cl, <span class="number">0</span>],y=x[y==cl, <span class="number">1</span>], alpha=<span class="number">0.8</span>, c=cmap(idx), </span><br><span class="line">                    marker=marker[idx], label=cl)</span><br></pre></td></tr></table></figure><p>我们通过调用这个函数可以画出我们学习到的超平面</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plot_decision_regions(x, y, ppn, resolution=<span class="number">0.02</span>)</span><br><span class="line">plt.xlabel(<span class="string">'花瓣的长度'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'花茎的长度'</span>)</span><br><span class="line">plt.legend(loc = <span class="string">'upper left'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="http://img.blog.csdn.net/20180225160054504?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p>(PS: 在画图的时候忘记了中文的问题了。。。)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;关于感知机的相关理论知识请查看：&lt;a href=&quot;http://quanfita.cn/2018/02/11/perceptron/&quot;&gt;感知机&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;关于Iris数据集&quot;&gt;&lt;a href=&quot;#关于Iris数据集&quot; class=&quot;headerlink
      
    
    </summary>
    
      <category term="note" scheme="http://quanfita.cn/categories/note/"/>
    
    
      <category term="Machine Learning" scheme="http://quanfita.cn/tags/Machine-Learning/"/>
    
      <category term="Iris" scheme="http://quanfita.cn/tags/Iris/"/>
    
      <category term="perceptron" scheme="http://quanfita.cn/tags/perceptron/"/>
    
  </entry>
  
  <entry>
    <title>机器学习入门实战——KNN实战Iris数据集</title>
    <link href="http://quanfita.cn/2018/05/14/KNN_Iris/"/>
    <id>http://quanfita.cn/2018/05/14/KNN_Iris/</id>
    <published>2018-05-14T12:09:21.473Z</published>
    <updated>2018-05-14T12:20:26.527Z</updated>
    
    <content type="html"><![CDATA[<p>关于KNN的相关理论知识请查看：<a href="http://quanfita.cn/2018/02/12/k_NN/">KNN</a></p><p>关于Iris数据集的相关信息可查看我的上一篇博客：<a href="http://blog.csdn.net/qq_30611601/article/details/79369437" target="_blank" rel="noopener">感知机算法实战Iris数据集</a></p><p>接下来的实战我们将使用sklearn库</p><h3 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a>代码实战</h3><p>首先，我们还是先导入数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line">iris = load_iris()</span><br><span class="line">iris.data.shape</span><br></pre></td></tr></table></figure><blockquote><p>(150, 4)</p></blockquote><p>sklearn的datasets都有详细的数据集信息，我们可以把这些信息打印出来查看</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(iris.DESCR)</span><br></pre></td></tr></table></figure><blockquote><h1 id="Iris-Plants-Database"><a href="#Iris-Plants-Database" class="headerlink" title="Iris Plants Database"></a>Iris Plants Database</h1><h2 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h2><p>Data Set Characteristics:<br>    :Number of Instances: 150 (50 in each of three classes)<br>    :Number of Attributes: 4 numeric, predictive attributes and the class<br>    :Attribute Information:</p><pre><code>    - sepal length in cm    - sepal width in cm    - petal length in cm    - petal width in cm    - class:            - Iris-Setosa            - Iris-Versicolour            - Iris-Virginica:Summary Statistics:============== ==== ==== ======= ===== ====================                Min  Max   Mean    SD   Class Correlation============== ==== ==== ======= ===== ====================sepal length:   4.3  7.9   5.84   0.83    0.7826sepal width:    2.0  4.4   3.05   0.43   -0.4194petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)============== ==== ==== ======= ===== ====================:Missing Attribute Values: None:Class Distribution: 33.3% for each of 3 classes.:Creator: R.A. Fisher:Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov):Date: July, 1988</code></pre><p>This is a copy of UCI ML iris datasets.<br><a href="http://archive.ics.uci.edu/ml/datasets/Iris" target="_blank" rel="noopener">http://archive.ics.uci.edu/ml/datasets/Iris</a></p><p>The famous Iris database, first used by Sir R.A Fisher</p><p>This is perhaps the best known database to be found in the<br>pattern recognition literature.  Fisher’s paper is a classic in the field and<br>is referenced frequently to this day.  (See Duda &amp; Hart, for example.)  The<br>data set contains 3 classes of 50 instances each, where each class refers to a<br>type of iris plant.  One class is linearly separable from the other 2; the<br>latter are NOT linearly separable from each other.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li>Fisher,R.A. “The use of multiple measurements in taxonomic problems”<br>Annual Eugenics, 7, Part II, 179-188 (1936); also in “Contributions to<br>Mathematical Statistics” (John Wiley, NY, 1950).</li><li>Duda,R.O., &amp; Hart,P.E. (1973) Pattern Classification and Scene Analysis.<br>(Q327.D83) John Wiley &amp; Sons.  ISBN 0-471-22361-1.  See page 218.</li><li>Dasarathy, B.V. (1980) “Nosing Around the Neighborhood: A New System<br>Structure and Classification Rule for Recognition in Partially Exposed<br>Environments”.  IEEE Transactions on Pattern Analysis and Machine<br>Intelligence, Vol. PAMI-2, No. 1, 67-71.</li><li>Gates, G.W. (1972) “The Reduced Nearest Neighbor Rule”.  IEEE Transactions<br>on Information Theory, May 1972, 431-433.</li><li>See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al”s AUTOCLASS II<br>conceptual clustering system finds 3 classes in the data.</li><li>Many, many more …</li></ul></blockquote><p>按照一般机器学习的套路，我们将整个数据分成训练集和测试集，75%的训练集和25%的测试集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train,X_test,Y_train,Y_test = train_test_split(iris.data,iris.target,test_size=<span class="number">0.25</span>,random_state=<span class="number">33</span>)</span><br></pre></td></tr></table></figure><p>下一步，我们将数据进行标准化处理，然后导入KNN模型，进行训练（这些都是套路）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line">ss = StandardScaler()</span><br><span class="line">X_train = ss.fit_transform(X_train)</span><br><span class="line">X_test = ss.fit_transform(X_test)</span><br><span class="line"></span><br><span class="line">knc = KNeighborsClassifier()</span><br><span class="line">knc.fit(X_train,Y_train)</span><br><span class="line">y_predict = knc.predict(X_test)</span><br></pre></td></tr></table></figure><p>最后，我们来检验一下模型的好坏</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'The accuracy of K-Nearest Neighbor Classifier is'</span>,knc.score(X_test,Y_test))</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line">print(classification_report(Y_test,y_predict,target_names=iris.target_names))</span><br></pre></td></tr></table></figure><p><img src="http://img.blog.csdn.net/20180225162450466?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p>代码参考：《Python机器学习及实践：从零开始通往Kaggle竞赛之路》</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;关于KNN的相关理论知识请查看：&lt;a href=&quot;http://quanfita.cn/2018/02/12/k_NN/&quot;&gt;KNN&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;关于Iris数据集的相关信息可查看我的上一篇博客：&lt;a href=&quot;http://blog.csdn.net/qq_30
      
    
    </summary>
    
      <category term="note" scheme="http://quanfita.cn/categories/note/"/>
    
    
      <category term="Machine Learning" scheme="http://quanfita.cn/tags/Machine-Learning/"/>
    
      <category term="KNN" scheme="http://quanfita.cn/tags/KNN/"/>
    
      <category term="Iris" scheme="http://quanfita.cn/tags/Iris/"/>
    
  </entry>
  
  <entry>
    <title>机器学习入门实战——朴素贝叶斯实战新闻组数据集</title>
    <link href="http://quanfita.cn/2018/05/14/naive_bayes_news/"/>
    <id>http://quanfita.cn/2018/05/14/naive_bayes_news/</id>
    <published>2018-05-14T12:09:03.616Z</published>
    <updated>2018-05-14T12:24:50.731Z</updated>
    
    <content type="html"><![CDATA[<p>﻿## 朴素贝叶斯实战新闻组数据集</p><p>关于朴素贝叶斯的相关理论知识可查看：<a href="http://quanfita.cn/2018/02/21/naive_Bayes/">朴素贝叶斯法</a></p><h3 id="关于新闻组数据集"><a href="#关于新闻组数据集" class="headerlink" title="关于新闻组数据集"></a>关于新闻组数据集</h3><p>20newsgroups数据集是用于文本分类、文本挖据和信息检索研究的国际标准数据集之一。一些新闻组的主题特别相似(e.g. comp.sys.ibm.pc.hardware/comp.sys.mac.hardware)，还有一些却完全不相关 (e.g misc.forsale /soc.religion.christian)。</p><p>20个新闻组数据集包含大约18000个新闻组，其中20个主题分成两个子集:一个用于训练(或开发)，另一个用于测试(或用于性能评估)。训练集和测试集之间的分割是基于特定日期之前和之后发布的消息。</p><h3 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a>代码实战</h3><p>首先，还是导入数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_20newsgroups</span><br><span class="line"></span><br><span class="line">news = fetch_20newsgroups(subset=<span class="string">'all'</span>)</span><br><span class="line">print(len(news.data))</span><br><span class="line">print(news.data[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>我们这里打印出来一个新闻例子，如下</p><blockquote><p>18846<br>From: Mamatha Devineni Ratnam <a href="&#x6d;&#97;&#105;&#108;&#x74;&#x6f;&#58;&#109;&#114;&#52;&#x37;&#x2b;&#x40;&#97;&#110;&#100;&#114;&#101;&#119;&#x2e;&#x63;&#x6d;&#x75;&#46;&#x65;&#x64;&#117;">&#109;&#114;&#52;&#x37;&#x2b;&#x40;&#97;&#110;&#100;&#114;&#101;&#119;&#x2e;&#x63;&#x6d;&#x75;&#46;&#x65;&#x64;&#117;</a><br>Subject: Pens fans reactions<br>Organization: Post Office, Carnegie Mellon, Pittsburgh, PA<br>Lines: 12<br>NNTP-Posting-Host: po4.andrew.cmu.edu</p><p>I am sure some bashers of Pens fans are pretty confused about the lack<br>of any kind of posts about the recent Pens massacre of the Devils. Actually,<br>I am  bit puzzled too and a bit relieved. However, I am going to put an end<br>to non-PIttsburghers’ relief with a bit of praise for the Pens. Man, they<br>are killing those Devils worse than I thought. Jagr just showed you why<br>he is much better than his regular season stats. He is also a lot<br>fo fun to watch in the playoffs. Bowman should let JAgr have a lot of<br>fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final<br>regular season game.          PENS RULE!!!</p></blockquote><p>接下来，划分数据集，还是75%训练集，25%测试集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train,X_test,Y_train,Y_test = train_test_split(news.data,news.target,test_size=<span class="number">0.25</span>,random_state=<span class="number">33</span>)</span><br></pre></td></tr></table></figure><p>我们需要对文本特征进行提取，我们这里使用CountVectorizer来提取特征。CountVectorizer能够将文本词块化，通过计算词汇的数量来将文本转化成向量（更多文本特征提取内容可查看<a href="https://www.cnblogs.com/Haichao-Zhang/p/5220974.html）。然后我们导入模型来学习数据。" target="_blank" rel="noopener">https://www.cnblogs.com/Haichao-Zhang/p/5220974.html）。然后我们导入模型来学习数据。</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line">vec = CountVectorizer()</span><br><span class="line">X_train = vec.fit_transform(X_train)</span><br><span class="line">X_test = vec.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line">mnb = MultinomialNB()</span><br><span class="line">mnb.fit(X_train,Y_train)</span><br><span class="line">y_predict = mnb.predict(X_test)</span><br></pre></td></tr></table></figure><p>最后，我们还是一样，检验一下模型的准确度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line">print(<span class="string">'The Accuracy of Navie Bayes Classifier is'</span>,mnb.score(X_test,Y_test))</span><br><span class="line">print(classification_report(Y_test,y_predict,target_names = news.target_names))</span><br></pre></td></tr></table></figure><p><img src="http://img.blog.csdn.net/20180225212238191?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>代码参考：《Python机器学习及实践：从零开始通往Kaggle竞赛之路》</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;﻿## 朴素贝叶斯实战新闻组数据集&lt;/p&gt;
&lt;p&gt;关于朴素贝叶斯的相关理论知识可查看：&lt;a href=&quot;http://quanfita.cn/2018/02/21/naive_Bayes/&quot;&gt;朴素贝叶斯法&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;关于新闻组数据集&quot;&gt;&lt;a href=
      
    
    </summary>
    
      <category term="note" scheme="http://quanfita.cn/categories/note/"/>
    
    
      <category term="Machine Learning" scheme="http://quanfita.cn/tags/Machine-Learning/"/>
    
      <category term="naive bayes" scheme="http://quanfita.cn/tags/naive-bayes/"/>
    
      <category term="20newsgroups" scheme="http://quanfita.cn/tags/20newsgroups/"/>
    
  </entry>
  
  <entry>
    <title>机器学习入门实战——决策树算法实战Titanic数据集</title>
    <link href="http://quanfita.cn/2018/05/14/decision_tree_titanic/"/>
    <id>http://quanfita.cn/2018/05/14/decision_tree_titanic/</id>
    <published>2018-05-14T12:08:37.023Z</published>
    <updated>2018-05-14T12:22:05.083Z</updated>
    
    <content type="html"><![CDATA[<p>关于决策树的理论知识可以查看：<a href="http://quanfita.cn/2018/03/03/decision_tree/">决策树</a></p><h2 id="Titanic数据集概述"><a href="#Titanic数据集概述" class="headerlink" title="Titanic数据集概述"></a>Titanic数据集概述</h2><p>&emsp;&emsp;RMS泰坦尼克号的沉没是历史上最臭名昭着的沉船之一。 1912年4月15日，在首航期间，泰坦尼克号撞上一座冰山后沉没，2224名乘客和机组人员中有1502人遇难。这一耸人听闻的悲剧震撼了国际社会，导致了更好的船舶安全条例。预测是否有乘客幸存下来的泰坦尼克号。</p><h2 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a>代码实战</h2><p>首先，一如既往导入数据集，并查看一下部分数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">titanic = pd.read_csv(<span class="string">'http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt'</span>)</span><br><span class="line">titanic.head()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdn.net/2018032220364373?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>info可以查看一下数据集的基本信息，我们可以看到数据集中有部分缺失值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">titanic.info()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt; &lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;</span><br><span class="line">&gt; RangeIndex: 1313 entries, 0 to 1312</span><br><span class="line">&gt; Data columns (total 11 columns):</span><br><span class="line">&gt; row.names    1313 non-null int64</span><br><span class="line">&gt; pclass       1313 non-null object</span><br><span class="line">&gt; survived     1313 non-null int64</span><br><span class="line">&gt; name         1313 non-null object</span><br><span class="line">&gt; age          633 non-null float64</span><br><span class="line">&gt; embarked     821 non-null object</span><br><span class="line">&gt; home.dest    754 non-null object</span><br><span class="line">&gt; room         77 non-null object</span><br><span class="line">&gt; ticket       69 non-null object</span><br><span class="line">&gt; boat         347 non-null object</span><br><span class="line">&gt; sex          1313 non-null object</span><br><span class="line">&gt; dtypes: float64(1), int64(2), object(8)</span><br><span class="line">&gt; memory usage: 112.9+ KB</span><br></pre></td></tr></table></figure><p>我们从中选取三个特征来对数据进行预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = titanic[[<span class="string">'pclass'</span>,<span class="string">'age'</span>,<span class="string">'sex'</span>]]</span><br><span class="line">y = titanic[<span class="string">'survived'</span>]</span><br><span class="line"></span><br><span class="line">X.info()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt; &lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;</span><br><span class="line">&gt; RangeIndex: 1313 entries, 0 to 1312</span><br><span class="line">&gt; Data columns (total 3 columns):</span><br><span class="line">&gt; pclass    1313 non-null object</span><br><span class="line">&gt; age       633 non-null float64</span><br><span class="line">&gt; sex       1313 non-null object</span><br><span class="line">&gt; dtypes: float64(1), object(2)</span><br><span class="line">&gt; memory usage: 30.9+ KB</span><br></pre></td></tr></table></figure><p>age具有缺失值，所以我们要对缺失值进行处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X[<span class="string">'age'</span>].fillna(X[<span class="string">'age'</span>].mean(),inplace=<span class="keyword">True</span>)</span><br><span class="line">X.info()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt; &lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;</span><br><span class="line">&gt; RangeIndex: 1313 entries, 0 to 1312</span><br><span class="line">&gt; Data columns (total 3 columns):</span><br><span class="line">&gt; pclass    1313 non-null object</span><br><span class="line">&gt; age       1313 non-null float64</span><br><span class="line">&gt; sex       1313 non-null object</span><br><span class="line">&gt; dtypes: float64(1), object(2)</span><br><span class="line">&gt; memory usage: 30.9+ KB</span><br></pre></td></tr></table></figure><p>之后就是将数据集划分成训练集和测试集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train,X_test,Y_train,Y_test = train_test_split(X,y,test_size=<span class="number">0.25</span>,random_state=<span class="number">33</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line">vec = DictVectorizer(sparse=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">X_train = vec.fit_transform(X_train.to_dict(orient=<span class="string">'record'</span>))</span><br><span class="line">print(vec.feature_names_)</span><br><span class="line">X_test = vec.fit_transform(X_test.to_dict(orient=<span class="string">'record'</span>))</span><br></pre></td></tr></table></figure><blockquote><p>[‘age’, ‘pclass=1st’, ‘pclass=2nd’, ‘pclass=3rd’, ‘sex=female’, ‘sex=male’]</p></blockquote><p>导入决策树模型，并进行预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line">dtc = DecisionTreeClassifier()</span><br><span class="line">dtc.fit(X_train,Y_train)</span><br><span class="line">y_predict = dtc.predict(X_test)</span><br></pre></td></tr></table></figure><p>最后，检查一下模型的效果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line">print(dtc.score(X_test,Y_test))</span><br><span class="line">print(classification_report(y_predict,Y_test,target_names=[<span class="string">'died'</span>,<span class="string">'survived'</span>]))</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdn.net/2018032220361216?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>代码参考：《Python机器学习及实践：从零开始通往Kaggle竞赛之路》</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;关于决策树的理论知识可以查看：&lt;a href=&quot;http://quanfita.cn/2018/03/03/decision_tree/&quot;&gt;决策树&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Titanic数据集概述&quot;&gt;&lt;a href=&quot;#Titanic数据集概述&quot; class=&quot;he
      
    
    </summary>
    
      <category term="note" scheme="http://quanfita.cn/categories/note/"/>
    
    
      <category term="Machine Learning" scheme="http://quanfita.cn/tags/Machine-Learning/"/>
    
      <category term="Titanic" scheme="http://quanfita.cn/tags/Titanic/"/>
    
      <category term="decision tree" scheme="http://quanfita.cn/tags/decision-tree/"/>
    
  </entry>
  
  <entry>
    <title>机器学习入门实战——逻辑斯谛回归实战breast cancer数据集</title>
    <link href="http://quanfita.cn/2018/05/14/logistic_breast_cancer/"/>
    <id>http://quanfita.cn/2018/05/14/logistic_breast_cancer/</id>
    <published>2018-05-14T12:08:18.050Z</published>
    <updated>2018-05-14T12:23:17.767Z</updated>
    
    <content type="html"><![CDATA[<p>更多有关逻辑斯谛回归的理论知识查看：<a href="http://quanfita.cn/2018/03/03/logistic_regression/">逻辑斯谛回归</a></p><h2 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a>代码实战</h2><p>首先，我们还是先将需要用到的库导入，应为此数据集缺少名称，所以，使用pandas导入数据时，我们需要手动添加名称</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">column_names = [<span class="string">'Sample code number'</span>,<span class="string">'Clump Thickness'</span>,<span class="string">'Uniformity of Cell Size'</span>,</span><br><span class="line">                <span class="string">'Uniformity of Cell Shape'</span>,<span class="string">'Marginal Adhesion'</span>,<span class="string">'Single Epithelial Cell Size'</span>,</span><br><span class="line">               <span class="string">'Bare Nuclei'</span>,<span class="string">'Bland Chromatin'</span>,<span class="string">'Normal Nucleoli'</span>,<span class="string">'Mitoses'</span>,<span class="string">'Class'</span>]</span><br><span class="line">data = pd.read_csv(<span class="string">'breast-cancer-train.csv'</span>,names=column_names)</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure><p>为了更好的了解数据集的情况，我们查看一下数据信息</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data.info()</span><br><span class="line">data = data.replace(to_replace=<span class="string">'?'</span>,value=np.nan)</span><br><span class="line">data = data.dropna(how=<span class="string">'any'</span>)</span><br><span class="line">data.shape</span><br></pre></td></tr></table></figure><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&gt; &lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;</span><br><span class="line">&gt; Int64Index: 683 entries, 0 to 698</span><br><span class="line">&gt; Data columns (total 11 columns):</span><br><span class="line">&gt; Sample code number             683 non-null int64</span><br><span class="line">&gt; Clump Thickness                683 non-null int64</span><br><span class="line">&gt; Uniformity of Cell Size        683 non-null int64</span><br><span class="line">&gt; Uniformity of Cell Shape       683 non-null int64</span><br><span class="line">&gt; Marginal Adhesion              683 non-null int64</span><br><span class="line">&gt; Single Epithelial Cell Size    683 non-null int64</span><br><span class="line">&gt; Bare Nuclei                    683 non-null object</span><br><span class="line">&gt; Bland Chromatin                683 non-null int64</span><br><span class="line">&gt; Normal Nucleoli                683 non-null int64</span><br><span class="line">&gt; Mitoses                        683 non-null int64</span><br><span class="line">&gt; Class                          683 non-null int64</span><br><span class="line">&gt; dtypes: int64(10), object(1)</span><br><span class="line">&gt; memory usage: 84.0+ KB</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><p>接下来，我们将数据集划分为训练集和测试集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line">x_train,x_test,y_train,y_test = train_test_split(data[column_names[<span class="number">1</span>:<span class="number">10</span>]],data[column_names[<span class="number">10</span>]],test_size=<span class="number">0.25</span>,random_state=<span class="number">33</span>)</span><br></pre></td></tr></table></figure><p>将数据标准化，导入逻辑斯谛回归模型，然后就可以进行预测了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line">ss = StandardScaler()</span><br><span class="line">x_train = ss.fit_transform(x_train)</span><br><span class="line">x_test = ss.transform(x_test)</span><br><span class="line"></span><br><span class="line">print(X_train.shape,Y_train.shape)</span><br><span class="line"></span><br><span class="line">lr = LogisticRegression()</span><br><span class="line">lr.fit(x_train,y_train)</span><br><span class="line">y_predict = lr.predict(x_test)</span><br></pre></td></tr></table></figure><p>最后，我们查看一下模型的效果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line">print(lr.score(x_test,y_test))</span><br></pre></td></tr></table></figure><blockquote><p>0.988304093567</p></blockquote><p>代码参考：《Python机器学习及实践：从零开始通往Kaggle竞赛之路》</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;更多有关逻辑斯谛回归的理论知识查看：&lt;a href=&quot;http://quanfita.cn/2018/03/03/logistic_regression/&quot;&gt;逻辑斯谛回归&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;代码实战&quot;&gt;&lt;a href=&quot;#代码实战&quot; class=&quot;header
      
    
    </summary>
    
      <category term="note" scheme="http://quanfita.cn/categories/note/"/>
    
    
      <category term="Machine Learning" scheme="http://quanfita.cn/tags/Machine-Learning/"/>
    
      <category term="logistic" scheme="http://quanfita.cn/tags/logistic/"/>
    
      <category term="breast cancer" scheme="http://quanfita.cn/tags/breast-cancer/"/>
    
  </entry>
  
  <entry>
    <title>机器学习入门实战——线性支持向量机实战digits数据集</title>
    <link href="http://quanfita.cn/2018/05/14/SVM_digits/"/>
    <id>http://quanfita.cn/2018/05/14/SVM_digits/</id>
    <published>2018-05-14T12:07:46.898Z</published>
    <updated>2018-05-14T12:25:39.880Z</updated>
    
    <content type="html"><![CDATA[<p>关于支持向量机的理论知识查看：<a href="">支持向量机</a></p><h2 id="digits数据集概述"><a href="#digits数据集概述" class="headerlink" title="digits数据集概述"></a>digits数据集概述</h2><p>digits.data：手写数字特征向量数据集，每一个元素都是一个64维的特征向量。</p><p>digits.target：特征向量对应的标记，每一个元素都是自然是0-9的数字。</p><p>digits.images：对应着data中的数据，每一个元素都是8*8的二维数组，其元素代表的是灰度值，转化为以为是便是特征向量。</p><h2 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a>代码实战</h2><p>先导入数据，我们直接使用sklearn为我们准备好的数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br><span class="line"></span><br><span class="line">digits = load_digits()</span><br><span class="line">digits.data.shape</span><br></pre></td></tr></table></figure><blockquote><p>(1797, 64)</p></blockquote><p>将数据集进行划分</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train,X_test,Y_train,Y_test = train_test_split(digits.data,digits.target,test_size=<span class="number">0.25</span>,random_state=<span class="number">33</span>)</span><br></pre></td></tr></table></figure><p>数据标准化，导入线性支持向量机并训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br><span class="line"></span><br><span class="line">ss = StandardScaler()</span><br><span class="line">X_train = ss.fit_transform(X_train)</span><br><span class="line">X_test = ss.fit_transform(X_test)</span><br><span class="line"></span><br><span class="line">lsvc = LinearSVC()</span><br><span class="line">lsvc.fit(X_train,Y_train)</span><br><span class="line">Y_predict = lsvc.predict(X_test)</span><br></pre></td></tr></table></figure><p>然后，我们对模型进行评估</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'The Accuracy of Linear SVC is'</span>,lsvc.score(X_test,Y_test))</span><br></pre></td></tr></table></figure><blockquote><p>The Accuracy of Linear SVC is 0.948888888889</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line">print(classification_report(Y_test,Y_predict,target_names=digits.target_names.astype(str)))</span><br></pre></td></tr></table></figure><p>代码参考：《Python机器学习及实践：从零开始通往Kaggle竞赛之路》 </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;关于支持向量机的理论知识查看：&lt;a href=&quot;&quot;&gt;支持向量机&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;digits数据集概述&quot;&gt;&lt;a href=&quot;#digits数据集概述&quot; class=&quot;headerlink&quot; title=&quot;digits数据集概述&quot;&gt;&lt;/a&gt;digits数据集概
      
    
    </summary>
    
      <category term="note" scheme="http://quanfita.cn/categories/note/"/>
    
    
      <category term="Machine Learning" scheme="http://quanfita.cn/tags/Machine-Learning/"/>
    
      <category term="SVM" scheme="http://quanfita.cn/tags/SVM/"/>
    
      <category term="digits" scheme="http://quanfita.cn/tags/digits/"/>
    
  </entry>
  
  <entry>
    <title>机器学习入门实战——支持向量机实战Boston房价数据集</title>
    <link href="http://quanfita.cn/2018/05/14/SVM_boston/"/>
    <id>http://quanfita.cn/2018/05/14/SVM_boston/</id>
    <published>2018-05-14T12:07:02.097Z</published>
    <updated>2018-05-14T12:26:13.689Z</updated>
    
    <content type="html"><![CDATA[<p>更多支持向量机的理论知识查看：<a href="">支持向量机</a></p><h2 id=""><a href="#" class="headerlink" title=" "></a> </h2><p>该数据集来源于1978年美国某经济学杂志上。该数据集包含若干波士顿房屋的价格及其各项数据，每个数据项包含14个数据，分别是房屋均价及周边犯罪率、是否在河边等相关信息，其中最后一个数据是房屋均价。</p><h2 id="代码实战"><a href="#代码实战" class="headerlink" title="代码实战"></a>代码实战</h2><p>这里我们说简单点儿，我们从sklearn中的datasets中导入数据集，导入需要的库，将数据集进行划分，再标准化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line">boston = load_boston()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">X = boston.data</span><br><span class="line">y = boston.target</span><br><span class="line"></span><br><span class="line">X_train,X_test,Y_train,Y_test = train_test_split(X,y,test_size=<span class="number">0.25</span>,random_state=<span class="number">33</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line">ss_X = StandardScaler()</span><br><span class="line">ss_Y = StandardScaler()</span><br><span class="line"></span><br><span class="line">X_train = ss_X.fit_transform(X_train)</span><br><span class="line">X_test = ss_X.fit_transform(X_test)</span><br><span class="line">Y_train = ss_Y.fit_transform(Y_train)</span><br><span class="line">Y_test = ss_Y.fit_transform(Y_test)</span><br></pre></td></tr></table></figure><p>由于支持向量机的核函数我们可以自己选择，所以我们选择三种核函数进行对比</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVR</span><br><span class="line">linear_svr = SVR(kernel=<span class="string">'linear'</span>)</span><br><span class="line">linear_svr.fit(X_train,Y_train)</span><br><span class="line">linear_svr_y_predict = linear_svr.predict(X_test)</span><br><span class="line"></span><br><span class="line">poly_svr = SVR(kernel=<span class="string">'poly'</span>)</span><br><span class="line">poly_svr.fit(X_train,Y_train)</span><br><span class="line">poly_svr_y_predict = poly_svr.predict(X_test)</span><br><span class="line"></span><br><span class="line">rbf_svr = SVR(kernel=<span class="string">'rbf'</span>)</span><br><span class="line">rbf_svr.fit(X_train,Y_train)</span><br><span class="line">rbf_svr_y_predict = rbf_svr.predict(X_test)</span><br></pre></td></tr></table></figure><p>我们来分别检测一下模型的性能</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score,mean_squared_error,mean_absolute_error</span><br><span class="line">print(<span class="string">"The value of R-squared of linear SVR is"</span>,linear_svr.score(X_test,Y_test))</span><br><span class="line">print(<span class="string">"The mean squared error of linear SVR is"</span>,mean_squared_error(ss_Y.inverse_transform(Y_test),ss_Y.inverse_transform(linear_svr_y_predict)))</span><br><span class="line">print(<span class="string">"The mean absolute error of linear SVR is"</span>,mean_absolute_error(ss_Y.inverse_transform(Y_test),ss_Y.inverse_transform(linear_svr_y_predict)))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"The value of R-squared of poly SVR is"</span>,poly_svr.score(X_test,Y_test))</span><br><span class="line">print(<span class="string">"The mean squared error of poly SVR is"</span>,mean_squared_error(ss_Y.inverse_transform(Y_test),ss_Y.inverse_transform(poly_svr_y_predict)))</span><br><span class="line">print(<span class="string">"The mean absolute error of poly SVR is"</span>,mean_absolute_error(ss_Y.inverse_transform(Y_test),ss_Y.inverse_transform(poly_svr_y_predict)))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"The value of R-squared of RBF SVR is"</span>,rbf_svr.score(X_test,Y_test))</span><br><span class="line">print(<span class="string">"The mean squared error of RBF SVR is"</span>,mean_squared_error(ss_Y.inverse_transform(Y_test),ss_Y.inverse_transform(rbf_svr_y_predict)))</span><br><span class="line">print(<span class="string">"The mean absolute error of RBF SVR is"</span>,mean_absolute_error(ss_Y.inverse_transform(Y_test),ss_Y.inverse_transform(rbf_svr_y_predict)))</span><br></pre></td></tr></table></figure><blockquote><p>The value of R-squared of linear SVR is 0.654497663771<br>The mean squared error of linear SVR is 26.7906984256<br>The mean absolute error of linear SVR is 3.41002068375<br>The value of R-squared of poly SVR is 0.23496198912<br>The mean squared error of poly SVR is 59.3220377532<br>The mean absolute error of poly SVR is 4.19595019294<br>The value of R-squared of RBF SVR is 0.71072756206<br>The mean squared error of RBF SVR is 22.4305593192<br>The mean absolute error of RBF SVR is 2.81406224321</p></blockquote><p>代码参考：《Python机器学习及实践：从零开始通往Kaggle竞赛之路》 </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;更多支持向量机的理论知识查看：&lt;a href=&quot;&quot;&gt;支持向量机&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;&quot;&gt;&lt;a href=&quot;#&quot; class=&quot;headerlink&quot; title=&quot; &quot;&gt;&lt;/a&gt; &lt;/h2&gt;&lt;p&gt;该数据集来源于1978年美国某经济学杂志上。该数据集包含若干波士顿
      
    
    </summary>
    
      <category term="note" scheme="http://quanfita.cn/categories/note/"/>
    
    
      <category term="Machine Learning" scheme="http://quanfita.cn/tags/Machine-Learning/"/>
    
      <category term="SVM" scheme="http://quanfita.cn/tags/SVM/"/>
    
      <category term="Boston" scheme="http://quanfita.cn/tags/Boston/"/>
    
  </entry>
  
  <entry>
    <title>Unreal Circle（北京站）（多图）</title>
    <link href="http://quanfita.cn/2018/04/01/Unreal_Circle/"/>
    <id>http://quanfita.cn/2018/04/01/Unreal_Circle/</id>
    <published>2018-04-01T10:28:14.019Z</published>
    <updated>2018-04-01T10:35:58.662Z</updated>
    
    <content type="html"><![CDATA[<p>﻿有幸参加了Unreal Circle北京站的活动，晒一下活动时的照片<br><img src="https://img-blog.csdn.net/20180401175939474?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><img src="https://img-blog.csdn.net/20180401175949411?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><img src="https://img-blog.csdn.net/20180401180025938?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><img src="https://img-blog.csdn.net/20180401180050487?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><img src="https://img-blog.csdn.net/20180401180059818?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><img src="https://img-blog.csdn.net/20180401180109721?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><img src="https://img-blog.csdn.net/20180401180123329?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><img src="https://img-blog.csdn.net/20180401180205737?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><img src="https://img-blog.csdn.net/20180401180216828?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><img src="https://img-blog.csdn.net/20180401180227706?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><img src="https://img-blog.csdn.net/20180401180239350?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><img src="https://img-blog.csdn.net/20180401180256503?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><img src="https://img-blog.csdn.net/2018040118030614?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><img src="https://img-blog.csdn.net/2018040118032118?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><img src="https://img-blog.csdn.net/20180401180331194?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><img src="https://img-blog.csdn.net/20180401180340355?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><img src="https://img-blog.csdn.net/20180401180359175?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><img src="https://img-blog.csdn.net/20180401180410605?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><img src="https://img-blog.csdn.net/20180401180421832?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><img src="https://img-blog.csdn.net/20180401180432167?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><img src="https://img-blog.csdn.net/20180401180443399?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><img src="https://img-blog.csdn.net/20180401180456306?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><img src="https://img-blog.csdn.net/20180401180506678?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><img src="https://img-blog.csdn.net/20180401180518550?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><img src="https://img-blog.csdn.net/20180401180530573?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br><img src="https://img-blog.csdn.net/20180401180540555?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMwNjExNjAx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;﻿有幸参加了Unreal Circle北京站的活动，晒一下活动时的照片&lt;br&gt;&lt;img src=&quot;https://img-blog.csdn.net/20180401175939474?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV
      
    
    </summary>
    
      <category term="life" scheme="http://quanfita.cn/categories/life/"/>
    
    
      <category term="Unreal Circle" scheme="http://quanfita.cn/tags/Unreal-Circle/"/>
    
  </entry>
  
  <entry>
    <title>蓝桥杯日常刷题——历届试题1434：回文数字</title>
    <link href="http://quanfita.cn/2018/03/17/PalindromeNum/"/>
    <id>http://quanfita.cn/2018/03/17/PalindromeNum/</id>
    <published>2018-03-17T05:10:07.241Z</published>
    <updated>2018-03-17T05:11:57.522Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><p><strong>题目描述</strong></p><p>观察数字：12321，123321  都有一个共同的特征，无论从左到右读还是从右向左读，都是相同的。这样的数字叫做：回文数字。<br>本题要求你找到一些5位或6位的十进制数字。满足如下要求：<br>该数字的各个数位之和等于输入的整数。 </p><p><strong>输入</strong></p><p>一个正整数  n  (10&lt; n&lt; 100),  表示要求满足的数位和。</p><p><strong>输出</strong></p><p>若干行，每行包含一个满足要求的5位或6位整数。<br>数字按从小到大的顺序排列。<br>如果没有满足条件的，输出：-1 </p><p><strong>样例输入</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">44</span><br></pre></td></tr></table></figure><p><strong>样例输出</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">99899</span><br><span class="line">499994</span><br><span class="line">589985</span><br><span class="line">598895</span><br><span class="line">679976</span><br><span class="line">688886</span><br><span class="line">697796</span><br><span class="line">769967</span><br><span class="line">778877</span><br><span class="line">787787</span><br><span class="line">796697</span><br><span class="line">859958</span><br><span class="line">868868</span><br><span class="line">877778</span><br><span class="line">886688</span><br><span class="line">895598</span><br><span class="line">949949</span><br><span class="line">958859</span><br><span class="line">967769</span><br><span class="line">976679</span><br><span class="line">985589</span><br><span class="line">994499</span><br></pre></td></tr></table></figure><p><strong>题目分析</strong></p><p>解法一：</p><p>暴力破解法</p><p>既然题目中说了是五位和六位的数字，那么我们可以直接遍历从10000到999999的所有数字，从中进行筛选</p><p><strong>题目代码</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">int</span> n;</span><br><span class="line"><span class="keyword">bool</span> flag = <span class="literal">false</span>;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">huiwen</span><span class="params">(<span class="keyword">long</span> a)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">long</span> temp = a;</span><br><span class="line">    <span class="keyword">long</span> b = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(temp)&#123;</span><br><span class="line">        b = b*<span class="number">10</span>;</span><br><span class="line">        b += temp % <span class="number">10</span>;</span><br><span class="line">        temp /= <span class="number">10</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> a == b;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">xiangjia</span><span class="params">(<span class="keyword">long</span> a)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">long</span> sum = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(a)&#123;</span><br><span class="line">        sum += a%<span class="number">10</span>;</span><br><span class="line">        a /= <span class="number">10</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(sum == n)&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; n;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">long</span> i = <span class="number">10000</span>; i &lt; <span class="number">1000000</span>; i++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(huiwen(i))&#123;</span><br><span class="line">            <span class="keyword">if</span>(xiangjia(i))</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="built_in">cout</span> &lt;&lt; i &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">                flag = <span class="literal">true</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(!flag)</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"-1"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>解法二：</p><p>暴力破解法虽然简单但是遍历10000到999999之间的所有数浪费了很多时间，所以为了降低运行时间，我们将每一位单独讨论首尾相等我们用一个变量保存，这样下来，我们只需遍历9^3次。</p><p><strong>题目代码</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">bool</span> flag = <span class="literal">false</span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">solve</span><span class="params">(<span class="keyword">int</span> x,<span class="keyword">int</span> y,<span class="keyword">int</span> z,<span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> a[<span class="number">3</span>] = &#123;<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>&#125;;</span><br><span class="line">    <span class="keyword">for</span>(a[<span class="number">0</span>] = <span class="number">1</span> ;a[<span class="number">0</span>]&lt;=<span class="number">9</span>;a[<span class="number">0</span>]++)</span><br><span class="line">        <span class="keyword">for</span>(a[<span class="number">1</span>] = <span class="number">0</span> ;a[<span class="number">1</span>]&lt;=<span class="number">9</span>;a[<span class="number">1</span>]++)</span><br><span class="line">            <span class="keyword">for</span>(a[<span class="number">2</span>] = <span class="number">0</span> ;a[<span class="number">2</span>]&lt;=<span class="number">9</span>;a[<span class="number">2</span>]++)</span><br><span class="line">                <span class="keyword">if</span>(z == <span class="number">1</span>)&#123;</span><br><span class="line">                    <span class="keyword">if</span>(x*a[<span class="number">0</span>]+y*a[<span class="number">1</span>]+z*a[<span class="number">2</span>] == n)&#123;</span><br><span class="line">                        <span class="built_in">cout</span>&lt;&lt;a[<span class="number">0</span>]&lt;&lt;a[<span class="number">1</span>]&lt;&lt;a[<span class="number">2</span>]&lt;&lt;a[<span class="number">1</span>]&lt;&lt;a[<span class="number">0</span>]&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">                        flag = <span class="literal">true</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span>&#123;</span><br><span class="line">                    <span class="keyword">if</span>(x*a[<span class="number">0</span>]+y*a[<span class="number">1</span>]+z*a[<span class="number">2</span>] == n)&#123;</span><br><span class="line">                        <span class="built_in">cout</span>&lt;&lt;a[<span class="number">0</span>]&lt;&lt;a[<span class="number">1</span>]&lt;&lt;a[<span class="number">2</span>]&lt;&lt;a[<span class="number">2</span>]&lt;&lt;a[<span class="number">1</span>]&lt;&lt;a[<span class="number">0</span>]&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">                        flag = <span class="literal">true</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n = <span class="number">0</span>;</span><br><span class="line">    <span class="built_in">cin</span>&gt;&gt;n;</span><br><span class="line">    solve(<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>,n);</span><br><span class="line">    solve(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>,n);</span><br><span class="line">    <span class="keyword">if</span>(flag == <span class="literal">false</span>) <span class="built_in">cout</span>&lt;&lt;<span class="string">"-1"</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>原题链接：<a href="http://www.dotcpp.com/oj/problem1434.html" target="_blank" rel="noopener">C语言网</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;题目描述&lt;/strong&gt;&lt;/p&gt;
      
    
    </summary>
    
      <category term="算法练习" scheme="http://quanfita.cn/categories/%E7%AE%97%E6%B3%95%E7%BB%83%E4%B9%A0/"/>
    
    
      <category term="蓝桥杯" scheme="http://quanfita.cn/tags/%E8%93%9D%E6%A1%A5%E6%9D%AF/"/>
    
  </entry>
  
  <entry>
    <title>蓝桥杯日常刷题——练习1097：蛇行矩阵</title>
    <link href="http://quanfita.cn/2018/03/17/SnakeMat/"/>
    <id>http://quanfita.cn/2018/03/17/SnakeMat/</id>
    <published>2018-03-17T02:06:01.429Z</published>
    <updated>2018-03-17T02:08:00.863Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><p><strong>题目描述</strong></p><p>蛇形矩阵是由1开始的自然数依次排列成的一个矩阵上三角形。</p><p><strong>输入</strong></p><p>本题有多组数据，每组数据由一个正整数N组成。（N不大于100）</p><p><strong>输出</strong></p><p>对于每一组数据，输出一个N行的蛇形矩阵。两组输出之间不要额外的空行。矩阵三角中同一行的数字用一个空格分开。行尾不要多余的空格。</p><p><strong>样例输入</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">5</span><br></pre></td></tr></table></figure><p><strong>样例输出</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1 3 6 10 15</span><br><span class="line">2 5 9 14</span><br><span class="line">4 8 13</span><br><span class="line">7 12</span><br><span class="line">11</span><br></pre></td></tr></table></figure><p><strong>题目分析</strong></p><p>解法一：</p><p>如果，我们只从数学规律角度去分析这道题，应该把这道问题分成行和列来分别计算</p><p>列的规律</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">1+1</span><br><span class="line">1+1+2</span><br><span class="line">1+1+2+3</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>我们可以写成公式<script type="math/tex">1+\sum\limits_{i = 1}^{n}(i-1)</script> </p><p>而行的规律</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1 1+2 1+2+3 1+2+3+4 ...</span><br><span class="line">2 2+3 2+3+4</span><br></pre></td></tr></table></figure><p>设行首数据为<script type="math/tex">a_0</script> 写成公式<script type="math/tex">\sum\limits_{i=1}^{n}(a_0+i-1)</script> </p><p><strong>题目代码</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> b;</span><br><span class="line">    <span class="keyword">while</span>(~<span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;b))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> a = <span class="number">1</span>,m=<span class="number">1</span>,n=<span class="number">2</span>,s=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>;j&lt;b;j++)&#123;</span><br><span class="line">            m+=j;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"%d "</span>,m);</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> k = n ;k&lt;=b;k++)&#123;</span><br><span class="line">                s=s+k;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">"%d"</span>,m+s);</span><br><span class="line">                <span class="keyword">if</span>(k!=b) <span class="built_in">printf</span>(<span class="string">" "</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            s=<span class="number">0</span>;</span><br><span class="line">            n++;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>解法二：</p><p>其实也不难发现，我们如果将第一行的数据存储一下，或许还可以使算法更高效</p><p>我们发现以下规律</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a[0]a[1]a[2]...</span><br><span class="line">a[1]-1a[2]-1a[3]-1...</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p><strong>题目代码</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> Max_N = <span class="number">100</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> a[Max_N],n;</span><br><span class="line">    <span class="keyword">while</span>(~<span class="built_in">scanf</span>(<span class="string">"%d"</span>,&amp;n))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i&lt;n;i++)</span><br><span class="line">        &#123;</span><br><span class="line">            a[i] = (i+<span class="number">1</span>)*(i+<span class="number">2</span>)/<span class="number">2</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>;j&lt;n;j++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> k = j;k&lt;n;k++)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">"%d"</span>,a[k]);</span><br><span class="line">                <span class="keyword">if</span>(k!=n<span class="number">-1</span>) <span class="built_in">printf</span>(<span class="string">" "</span>);</span><br><span class="line">                <span class="keyword">else</span> <span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line">                a[k] = a[k]<span class="number">-1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>原题链接：<a href="http://www.dotcpp.com/oj/problem1097.html" target="_blank" rel="noopener">C语言网</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;题目描述&lt;/strong&gt;&lt;/p&gt;
      
    
    </summary>
    
      <category term="算法练习" scheme="http://quanfita.cn/categories/%E7%AE%97%E6%B3%95%E7%BB%83%E4%B9%A0/"/>
    
    
      <category term="蓝桥杯" scheme="http://quanfita.cn/tags/%E8%93%9D%E6%A1%A5%E6%9D%AF/"/>
    
  </entry>
  
  <entry>
    <title>蓝桥杯日常刷题——练习1118：Tom数</title>
    <link href="http://quanfita.cn/2018/03/16/TomNum/"/>
    <id>http://quanfita.cn/2018/03/16/TomNum/</id>
    <published>2018-03-16T13:44:20.725Z</published>
    <updated>2018-03-16T13:47:58.145Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><p><strong>题目描述</strong></p><p>​    正整数的各位数字之和被Tom称为Tom数。求输入数（&lt;2^32）的Tom数!</p><p><strong>输入</strong></p><p>​    每行一个整数(&lt;2^32).</p><p><strong>输出</strong></p><p>​    每行一个输出,对应该数的各位数之和.</p><p><strong>样例输入</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">12345</span><br><span class="line">56123</span><br><span class="line">82</span><br></pre></td></tr></table></figure><p><strong>样例输出</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">15</span><br><span class="line">17</span><br><span class="line">10</span><br></pre></td></tr></table></figure><p><strong>题目分析</strong></p><p>这个题目考的是基本的数学知识，没有啥可分析的，直接看代码吧</p><p><strong>题目代码</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Tom</span><span class="params">(<span class="keyword">long</span> a)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> sum=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(a)</span><br><span class="line">    &#123;</span><br><span class="line">        sum+=a%<span class="number">10</span>;</span><br><span class="line">        a/=<span class="number">10</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> sum;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">long</span> n;</span><br><span class="line">    <span class="keyword">while</span>(~<span class="built_in">scanf</span>(<span class="string">"%ld"</span>,&amp;n))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>,Tom(n));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;题目描述&lt;/strong&gt;&lt;/p&gt;
      
    
    </summary>
    
      <category term="算法练习" scheme="http://quanfita.cn/categories/%E7%AE%97%E6%B3%95%E7%BB%83%E4%B9%A0/"/>
    
    
      <category term="蓝桥杯" scheme="http://quanfita.cn/tags/%E8%93%9D%E6%A1%A5%E6%9D%AF/"/>
    
  </entry>
  
  <entry>
    <title>数据结构详解——线性表（C++实现）</title>
    <link href="http://quanfita.cn/2018/03/11/list/"/>
    <id>http://quanfita.cn/2018/03/11/list/</id>
    <published>2018-03-11T08:09:44.272Z</published>
    <updated>2018-03-11T08:19:14.854Z</updated>
    
    <content type="html"><![CDATA[<h1 id="线性表"><a href="#线性表" class="headerlink" title="线性表"></a>线性表</h1><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><p>&emsp;&emsp;线性表是最常用且是最简单的一种数据结构。形如：A1、A2、A3….An这样含有有限的数据序列，我们就称之为线性表。</p><h2 id="一、线性表的定义"><a href="#一、线性表的定义" class="headerlink" title="一、线性表的定义"></a>一、线性表的定义</h2><p><strong>线性表：零个或多个数据元素的有限序列。</strong></p><p>线性表、包括顺序表和链表<br>顺序表（其实就是数组）里面元素的地址是连续的，<br>链表里面节点的地址不是连续的，是通过指针连起来的。</p><h2 id="二、线性表的抽象数据类型"><a href="#二、线性表的抽象数据类型" class="headerlink" title="二、线性表的抽象数据类型"></a>二、线性表的抽象数据类型</h2><p>&emsp;&emsp;线性表的抽象数据类型定义如下：</p><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&gt; ADT 线性表(List)</span><br><span class="line">&gt; Data</span><br><span class="line">&gt;     线性表的数据对象集合为&#123;a1,a2,....,an&#125;,每个元素的类型均为DataType。其中，除了第一个元素a1外，每一个元素有且只有一个直接前驱元素，除最后一个元素an外，每一个元素有且只有一个直接后继元素。数据元素之间的关系是一对一的关系。</span><br><span class="line">&gt;</span><br><span class="line">&gt; Operation</span><br><span class="line">&gt;     InitList(*L):初始化操作，建立一个空的线性表。</span><br><span class="line">&gt;     ListEmpty(L):若线性表为空，返回true，否则返回false。</span><br><span class="line">&gt;     ClearList(*L):线性表清空。</span><br><span class="line">&gt;     GetElem(L,i,*e):将线性表L中第i个位置元素返回给e。</span><br><span class="line">&gt;     LocateElem(L,e):在线性表L中查找与给定值e相等的元素，如果查找成功,返回该元素在表中的序列号；否则，返回0表示失败。</span><br><span class="line">&gt;     ListInsert(*L,i,e):在线性表的第i个位置插入元素e。</span><br><span class="line">&gt;     ListDelete(*L,i,*e):删除线性表L中的第i个元素，并用e返回其值</span><br><span class="line">&gt;     ListLength(L):返回线性表L的元素个数。</span><br><span class="line">&gt;     PrintList(L):打印线性表</span><br><span class="line">&gt;     </span><br><span class="line">&gt; 对于不同的应用，线性表的基本操作是不同的，上述操作是最基本的。</span><br><span class="line">&gt; 对于实际问题中涉及的关于线性表的更复杂操作，完全可以用这些基本操作的组合来实现。</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><h2 id="三、线性表的顺序存储"><a href="#三、线性表的顺序存储" class="headerlink" title="三、线性表的顺序存储"></a>三、线性表的顺序存储</h2><h3 id="1-顺序存储定义"><a href="#1-顺序存储定义" class="headerlink" title="1. 顺序存储定义"></a>1. 顺序存储定义</h3><p>&emsp;&emsp;顺序表，一般使用数组实现，事实上就是在内存中找个初始地址，然后通过占位的形式，把一定连续的内存空间给占了，然后把相同数据类型的数据元素依次放在这块空地中，数组大小有两种方式指定，一是静态分配，二是动态扩展。</p><p>&emsp;&emsp;顺序表相关的操作跟数组有关，一般都是移动数组元素。</p><p><img src="http://img.blog.csdn.net/20180311152120741?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><h3 id="2-顺序存储的实现方式"><a href="#2-顺序存储的实现方式" class="headerlink" title="2. 顺序存储的实现方式"></a>2. 顺序存储的实现方式</h3><p><strong>结构</strong></p><p>&emsp;&emsp;我们直接来看顺序表的模板类的代码：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> MaxSize = <span class="number">100</span>;</span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">SeqList</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    SeqList()&#123;length=<span class="number">0</span>;&#125;            <span class="comment">//无参数构造方法</span></span><br><span class="line">    SeqList(DataType a[],<span class="keyword">int</span> n);    <span class="comment">//有参数构造方法</span></span><br><span class="line">    ~SeqList()&#123;&#125;                    <span class="comment">//析构函数</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">Length</span><span class="params">()</span></span>&#123;<span class="keyword">return</span> length;&#125;    <span class="comment">//线性表长度</span></span><br><span class="line">    <span class="function">DataType <span class="title">Get</span><span class="params">(<span class="keyword">int</span> i)</span></span>;            <span class="comment">//按位查找</span></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">Locate</span><span class="params">(DataType x)</span></span>;         <span class="comment">//按值查找</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Insert</span><span class="params">(<span class="keyword">int</span> i,DataType x)</span></span>;  <span class="comment">//插入</span></span><br><span class="line">    <span class="function">DataType <span class="title">Delete</span><span class="params">(<span class="keyword">int</span> i)</span></span>;         <span class="comment">//删除</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">PrintList</span><span class="params">()</span></span>;               <span class="comment">//遍历</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    DataType data[MaxSize];         <span class="comment">//顺序表使用数组实现</span></span><br><span class="line">    <span class="keyword">int</span> length;                     <span class="comment">//存储顺序表的长度</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>顺序表的封装需要三个属性：</p><ol><li>存储空间的起始位置。数组data的存储位置就是线性表存储空间的存储位置</li><li>线性表的最大存储容量。数组长度MAXSIZE</li><li>线性表的当前长度。length</li></ol><p><strong>注意</strong>：数组的长度与线性表的当前长度是不一样的。数组的长度是存放线性表的存储空间的总长度，一般初始化后不变。而线性表的当前长度是线性表中元素的个数，是会改变的。</p><p>&emsp;&emsp;下面我们将实现顺序表的各个功能：</p><p><strong>有参数构造</strong>：</p><p>&emsp;&emsp;创建一个长度为n的顺序表，需要将给定的数组元素作为线性表的数据元素传入顺序表中，并将传入的元素个数作为顺序表的长度</p><p><img src="http://img.blog.csdn.net/20180311152302345?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">SeqList</span>&lt;DataType&gt;:</span>:SeqList(DataType a[],<span class="keyword">int</span> n)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span>(n&gt;MaxSize) <span class="keyword">throw</span> <span class="string">"wrong parameter"</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++)</span><br><span class="line">        data[i]=a[i];</span><br><span class="line">    length=n;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>按位查找</strong></p><p>&emsp;&emsp;按位查找的时间复杂度为<script type="math/tex">O(1)</script> 。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">DataType</span> <span class="title">SeqList</span>&lt;DataType&gt;:</span>:Get(<span class="keyword">int</span> i)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span>(i&lt;<span class="number">1</span> &amp;&amp; i&gt;length) <span class="keyword">throw</span> <span class="string">"wrong Location"</span>;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> data[i<span class="number">-1</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>按值查找</strong></p><p>&emsp;&emsp;按值查找，需要对顺序表中的元素依次进行比较。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">int</span> <span class="title">SeqList</span>&lt;DataType&gt;:</span>:Locate(DataType x)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;length;i++)</span><br><span class="line">        <span class="keyword">if</span>(data[i]==x) <span class="keyword">return</span> i+<span class="number">1</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>插入</strong></p><p>&emsp;&emsp;插入的过程中需要注意元素移动的方向，必须从最后一个元素开始移动，如果表满了，则引发上溢；如果插入位置不合理，则引发位置异常。</p><p><img src="http://img.blog.csdn.net/20180311152401708?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">void</span> <span class="title">SeqList</span>&lt;DataType&gt;:</span>:Insert(<span class="keyword">int</span> i,DataType x)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span>(length&gt;=MaxSize) <span class="keyword">throw</span> <span class="string">"Overflow"</span>;</span><br><span class="line">    <span class="keyword">if</span>(i&lt;<span class="number">1</span> || i&gt;length+<span class="number">1</span>) <span class="keyword">throw</span> <span class="string">"Location"</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j=length;j&gt;=i;j--)</span><br><span class="line">        data[j]=data[j<span class="number">-1</span>];</span><br><span class="line">    data[i<span class="number">-1</span>]=x;</span><br><span class="line">    length++;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>删除</strong></p><p>&emsp;&emsp;注意算法中元素移动方向，移动元素之前必须取出被删的元素，如果表为空则发生下溢，如果删除位置不合理，则引发删除位置异常。</p><p><img src="http://img.blog.csdn.net/20180311152453294?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">DataType</span> <span class="title">SeqList</span>&lt;DataType&gt;:</span>:Delete(<span class="keyword">int</span> i)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">int</span> x;</span><br><span class="line">    <span class="keyword">if</span>(length==<span class="number">0</span>) <span class="keyword">throw</span> <span class="string">"Underflow"</span>;</span><br><span class="line">    <span class="keyword">if</span>(i&lt;<span class="number">1</span> || i&gt;length) <span class="keyword">throw</span> <span class="string">"Location"</span>;</span><br><span class="line">    x = data[i<span class="number">-1</span>];</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j=i;j&lt;length;j++)</span><br><span class="line">        data[j<span class="number">-1</span>] = data[j];</span><br><span class="line">    length--;</span><br><span class="line">    <span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>遍历</strong></p><p>&emsp;&emsp;按下标依次输出各元素</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">void</span> <span class="title">SeqList</span>&lt;DataType&gt;:</span>:PrintList()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;length;i++)</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;data[i]&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>&emsp;&emsp;完整代码示例(更多数据结构完整示例可见<a href="https://github.com/Quanfita/Data_Stuctures" target="_blank" rel="noopener">GitHub</a>)：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> MaxSize = <span class="number">100</span>;</span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">SeqList</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    SeqList()&#123;length=<span class="number">0</span>;&#125;            </span><br><span class="line">    SeqList(DataType a[],<span class="keyword">int</span> n);    </span><br><span class="line">    ~SeqList()&#123;&#125;                    </span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">Length</span><span class="params">()</span></span>&#123;<span class="keyword">return</span> length;&#125;    </span><br><span class="line">    <span class="function">DataType <span class="title">Get</span><span class="params">(<span class="keyword">int</span> i)</span></span>;            </span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">Locate</span><span class="params">(DataType x)</span></span>;         </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">Insert</span><span class="params">(<span class="keyword">int</span> i,DataType x)</span></span>;  </span><br><span class="line">    <span class="function">DataType <span class="title">Delete</span><span class="params">(<span class="keyword">int</span> i)</span></span>;         </span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">PrintList</span><span class="params">()</span></span>;               </span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    DataType data[MaxSize];         </span><br><span class="line">    <span class="keyword">int</span> length;                     </span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">SeqList</span>&lt;DataType&gt;:</span>:SeqList(DataType a[],<span class="keyword">int</span> n)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span>(n&gt;MaxSize) <span class="keyword">throw</span> <span class="string">"wrong parameter"</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++)</span><br><span class="line">        data[i]=a[i];</span><br><span class="line">    length=n;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">DataType</span> <span class="title">SeqList</span>&lt;DataType&gt;:</span>:Get(<span class="keyword">int</span> i)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span>(i&lt;<span class="number">1</span> &amp;&amp; i&gt;length) <span class="keyword">throw</span> <span class="string">"wrong Location"</span>;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> data[i<span class="number">-1</span>];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">int</span> <span class="title">SeqList</span>&lt;DataType&gt;:</span>:Locate(DataType x)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;length;i++)</span><br><span class="line">        <span class="keyword">if</span>(data[i]==x) <span class="keyword">return</span> i+<span class="number">1</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">void</span> <span class="title">SeqList</span>&lt;DataType&gt;:</span>:Insert(<span class="keyword">int</span> i,DataType x)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span>(length&gt;=MaxSize) <span class="keyword">throw</span> <span class="string">"Overflow"</span>;</span><br><span class="line">    <span class="keyword">if</span>(i&lt;<span class="number">1</span> || i&gt;length+<span class="number">1</span>) <span class="keyword">throw</span> <span class="string">"Location"</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j=length;j&gt;=i;j--)</span><br><span class="line">        data[j]=data[j<span class="number">-1</span>];</span><br><span class="line">    data[i<span class="number">-1</span>]=x;</span><br><span class="line">    length++;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">DataType</span> <span class="title">SeqList</span>&lt;DataType&gt;:</span>:Delete(<span class="keyword">int</span> i)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">int</span> x;</span><br><span class="line">    <span class="keyword">if</span>(length==<span class="number">0</span>) <span class="keyword">throw</span> <span class="string">"Underflow"</span>;</span><br><span class="line">    <span class="keyword">if</span>(i&lt;<span class="number">1</span> || i&gt;length) <span class="keyword">throw</span> <span class="string">"Location"</span>;</span><br><span class="line">    x = data[i<span class="number">-1</span>];</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j=i;j&lt;length;j++)</span><br><span class="line">        data[j<span class="number">-1</span>] = data[j];</span><br><span class="line">    length--;</span><br><span class="line">    <span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">void</span> <span class="title">SeqList</span>&lt;DataType&gt;:</span>:PrintList()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;length;i++)</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;data[i]&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    SeqList&lt;<span class="keyword">int</span>&gt; p;</span><br><span class="line">    p.Insert(<span class="number">1</span>,<span class="number">5</span>);</span><br><span class="line">    p.Insert(<span class="number">2</span>,<span class="number">9</span>);</span><br><span class="line">    p.PrintList();</span><br><span class="line">    p.Insert(<span class="number">2</span>,<span class="number">3</span>);</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;p.Length()&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    p.PrintList();</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;p.Get(<span class="number">3</span>)&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    p.Delete(<span class="number">2</span>);</span><br><span class="line">    p.PrintList();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-顺序存储的优缺点"><a href="#3-顺序存储的优缺点" class="headerlink" title="3. 顺序存储的优缺点"></a>3. 顺序存储的优缺点</h3><p><strong>优点：</strong></p><ul><li>随机访问特性，查找O(1)时间，存储密度高；</li><li>逻辑上相邻的元素，物理上也相邻；</li><li>无须为表中元素之间的逻辑关系而增加额外的存储空间；</li></ul><p><strong>缺点：</strong></p><ul><li>插入和删除需移动大量元素；</li><li>当线性表长度变化较大时，难以确定存储空间的容量；</li><li>造成存储空间的“碎片”</li></ul><h2 id="四、线性表的链式存储"><a href="#四、线性表的链式存储" class="headerlink" title="四、线性表的链式存储"></a>四、线性表的链式存储</h2><h3 id="1-链式存储定义"><a href="#1-链式存储定义" class="headerlink" title="1. 链式存储定义"></a>1. 链式存储定义</h3><p>&emsp;&emsp;线性表的链式存储结构的特点是用一组任意的存储单元存储线性表的数据元素，这组存储单元可以是连续的，也可以是不连续的。这就意味着，这些元素可以存在内存未被占用的任意位置。</p><p><img src="http://img.blog.csdn.net/20180311152556473?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p>&emsp;&emsp;链表的定义是递归的，它或者为空null，或者指向另一个节点node的引用，这个节点含有下一个节点或链表的引用，线性链表的最后一个结点指针为“空”（通常用NULL或“^”符号表示）。</p><p><img src="http://img.blog.csdn.net/20180311152641656?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><h3 id="2-链式存储的实现方式"><a href="#2-链式存储的实现方式" class="headerlink" title="2. 链式存储的实现方式"></a>2. 链式存储的实现方式</h3><p><strong>存储方法</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">struct</span> <span class="title">Node</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">DataType data;<span class="comment">//存储数据</span></span><br><span class="line">Node&lt;DataType&gt; *next;<span class="comment">//存储下一个结点的地址</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>&emsp;&emsp;结点由存放数据元素的数据域和存放后继结点地址的指针域组成。 </p><p><img src="http://img.blog.csdn.net/20180311152733881?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p><strong>结构</strong></p><p>&emsp;&emsp;单链表的模板类的代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">LinkList</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">LinkList();                     </span><br><span class="line">LinkList(DataType a[], <span class="keyword">int</span> n);  </span><br><span class="line">~LinkList();                    </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Length</span><span class="params">()</span></span>;                   </span><br><span class="line"><span class="function">DataType <span class="title">Get</span><span class="params">(<span class="keyword">int</span> i)</span></span>;            </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Locate</span><span class="params">(DataType x)</span></span>;         </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Insert</span><span class="params">(<span class="keyword">int</span> i, DataType x)</span></span>; </span><br><span class="line"><span class="function">DataType <span class="title">Delete</span><span class="params">(<span class="keyword">int</span> i)</span></span>;         </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">PrintList</span><span class="params">()</span></span>;               </span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">Node&lt;DataType&gt; *first;          </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p><strong>特点</strong>：</p><ul><li>用一组任意的存储单元存储线性表的数据元素， 这组存储单元可以存在内存中未被占用的任意位置</li><li>顺序存储结构每个数据元素只需要存储一个位置就可以了，而链式存储结构中，除了要存储数据信息外，还要存储它的后继元素的存储地址</li></ul><p><strong>无参数构造</strong></p><p>&emsp;&emsp;生成只有头结点的空链表</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">LinkList</span>&lt;DataType&gt;:</span>:LinkList()</span><br><span class="line">&#123;</span><br><span class="line">first = <span class="keyword">new</span> Node&lt;DataType&gt;;</span><br><span class="line">first-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>头插法构造单链表</strong></p><p>&emsp;&emsp;头插法是每次将新申请的结点插在头结点后面</p><p><img src="http://img.blog.csdn.net/20180311152822301?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">LinkList</span>&lt;DataType&gt;:</span>:LinkList(DataType a[], <span class="keyword">int</span> n)</span><br><span class="line">&#123;</span><br><span class="line">first = <span class="keyword">new</span> Node&lt;DataType&gt;;</span><br><span class="line">first-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">&#123;</span><br><span class="line">Node&lt;DataType&gt; *s = <span class="keyword">new</span> Node&lt;DataType&gt;;</span><br><span class="line">s-&gt;data = a[i];</span><br><span class="line">s-&gt;next = first-&gt;next;</span><br><span class="line">first-&gt;next = s;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>尾插法构造单链表</strong></p><p>&emsp;&emsp;尾插法就是每次将新申请的结点插在终端节点的后面</p><p><img src="http://img.blog.csdn.net/20180311152858388?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">LinkList</span>&lt;DataType&gt;:</span>:LinkList(DataType a[], <span class="keyword">int</span> n)</span><br><span class="line">&#123;</span><br><span class="line">first = <span class="keyword">new</span> Node&lt;DataType&gt;;</span><br><span class="line">Node&lt;DataType&gt; *r = first;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">&#123;</span><br><span class="line">Node&lt;DataType&gt; *s = <span class="keyword">new</span> Node&lt;DataType&gt;;</span><br><span class="line">s-&gt;data = a[i];</span><br><span class="line">r-&gt;next = s;</span><br><span class="line">r = s;</span><br><span class="line">&#125;</span><br><span class="line">    r-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>析构函数</strong></p><p>&emsp;&emsp;单链表类中的结点是用new申请的，在释放的时候无法自动释放，所以，析构函数要将单链表中的结点空间释放</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">LinkList</span>&lt;DataType&gt;:</span>:~LinkList()</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">while</span> (first != <span class="literal">NULL</span>)</span><br><span class="line">&#123;</span><br><span class="line">Node&lt;DataType&gt;* q = first;</span><br><span class="line">first = first-&gt;next;</span><br><span class="line"><span class="keyword">delete</span> q;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>计算长度</strong></p><p>&emsp;&emsp;单链表中不能直接求出长度，所以我们只能将单链表扫描一遍，所以时间复杂度为<script type="math/tex">O(n)</script> </p><p><img src="http://img.blog.csdn.net/20180311152953343?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">int</span> <span class="title">LinkList</span>&lt;DataType&gt;:</span>:Length()</span><br><span class="line">&#123;</span><br><span class="line">Node&lt;DataType&gt;* p = first-&gt;next;</span><br><span class="line"><span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span> (p != <span class="literal">NULL</span>)</span><br><span class="line">&#123;</span><br><span class="line">p = p-&gt;next;</span><br><span class="line">count++;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> count;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>按位查找</strong></p><p>&emsp;&emsp;单链表中即使知道节点位置也不能直接访问，需要从头指针开始逐个节点向下搜索，平均时间性能为<script type="math/tex">O(n)</script> ，单链表是<strong>顺序存取</strong>结构</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">DataType</span> <span class="title">LinkList</span>&lt;DataType&gt;:</span>:Get(<span class="keyword">int</span> i)</span><br><span class="line">&#123;</span><br><span class="line">Node&lt;DataType&gt;* p = first-&gt;next;</span><br><span class="line"><span class="keyword">int</span> count = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">while</span> (p != <span class="literal">NULL</span> &amp;&amp; count&lt;i)</span><br><span class="line">&#123;</span><br><span class="line">p = p-&gt;next;</span><br><span class="line">count++;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (p == <span class="literal">NULL</span>) <span class="keyword">throw</span> <span class="string">"Location"</span>;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">return</span> p-&gt;data;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>按值查找</strong></p><p>&emsp;&emsp;单链表中按值查找与顺序表中的实现方法类似，对链表中的元素依次进行比较，平均时间性能为<script type="math/tex">O(n)</script> .</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">int</span> <span class="title">LinkList</span>&lt;DataType&gt;:</span>:Locate(DataType x)</span><br><span class="line">&#123;</span><br><span class="line">Node&lt;DataType&gt; *p = first-&gt;next;</span><br><span class="line"><span class="keyword">int</span> count = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">while</span> (p != <span class="literal">NULL</span>)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">if</span> (p-&gt;data == x) <span class="keyword">return</span> count;</span><br><span class="line">p = p-&gt;next;</span><br><span class="line">count++;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>插入</strong></p><p>&emsp;&emsp;单链表在插入过程中需要注意分析在表头、表中间、表尾的三种情况，由于单链表带头结点，这三种情况的操作语句一致，不用特殊处理，时间复杂度为<script type="math/tex">O(n)</script> </p><p><img src="http://img.blog.csdn.net/20180311153151640?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">void</span> <span class="title">LinkList</span>&lt;DataType&gt;:</span>:Insert(<span class="keyword">int</span> i, DataType x)</span><br><span class="line">&#123;</span><br><span class="line">Node&lt;DataType&gt; *p = first;</span><br><span class="line"><span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span> (p != <span class="literal">NULL</span> &amp;&amp; count&lt;i - <span class="number">1</span>)</span><br><span class="line">&#123;</span><br><span class="line">p = p-&gt;next;</span><br><span class="line">count++;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (p == <span class="literal">NULL</span>) <span class="keyword">throw</span> <span class="string">"Location"</span>;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">Node&lt;DataType&gt; *s = <span class="keyword">new</span> Node&lt;DataType&gt;;</span><br><span class="line">s-&gt;data = x;</span><br><span class="line">s-&gt;next = p-&gt;next;</span><br><span class="line">p-&gt;next = s;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>删除</strong></p><p>&emsp;&emsp;删除操作时需要注意表尾的特殊情况，此时虽然被删结点不存在，但其前驱结点却存在。因此仅当被删结点的前驱结点存在且不是终端节点时，才能确定被删节点存在，时间复杂度为<script type="math/tex">O(n)</script> .</p><p><img src="http://img.blog.csdn.net/20180311153330100?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">DataType</span> <span class="title">LinkList</span>&lt;DataType&gt;:</span>:Delete(<span class="keyword">int</span> i)</span><br><span class="line">&#123;</span><br><span class="line">Node&lt;DataType&gt; *p = first;</span><br><span class="line"><span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span> (p != <span class="literal">NULL</span> &amp;&amp; count&lt;i - <span class="number">1</span>)</span><br><span class="line">&#123;</span><br><span class="line">p = p-&gt;next;</span><br><span class="line">count++;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (p == <span class="literal">NULL</span> || p-&gt;next == <span class="literal">NULL</span>) <span class="keyword">throw</span> <span class="string">"Location"</span>;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">Node&lt;DataType&gt; *q = p-&gt;next;</span><br><span class="line"><span class="keyword">int</span> x = q-&gt;data;</span><br><span class="line">p-&gt;next = q-&gt;next;</span><br><span class="line"><span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>遍历</strong></p><p>&emsp;&emsp;遍历单链表时间复杂度为<script type="math/tex">O(n)</script> .</p><p><img src="http://img.blog.csdn.net/20180311153433777?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">void</span> <span class="title">LinkList</span>&lt;DataType&gt;:</span>:PrintList()</span><br><span class="line">&#123;</span><br><span class="line">Node&lt;DataType&gt; *p = first-&gt;next;</span><br><span class="line"><span class="keyword">while</span> (p != <span class="literal">NULL</span>)</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; p-&gt;data &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">p = p-&gt;next;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>&emsp;&emsp;完整代码示例(更多数据结构完整示例可见<a href="https://github.com/Quanfita/Data_Stuctures" target="_blank" rel="noopener">GitHub</a>)：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">struct</span> <span class="title">Node</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">DataType data;</span><br><span class="line">Node&lt;DataType&gt; *next;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">LinkList</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">LinkList();                     </span><br><span class="line">LinkList(DataType a[], <span class="keyword">int</span> n);  </span><br><span class="line">~LinkList();                    </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Length</span><span class="params">()</span></span>;                   </span><br><span class="line"><span class="function">DataType <span class="title">Get</span><span class="params">(<span class="keyword">int</span> i)</span></span>;            </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">Locate</span><span class="params">(DataType x)</span></span>;         </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Insert</span><span class="params">(<span class="keyword">int</span> i, DataType x)</span></span>; </span><br><span class="line"><span class="function">DataType <span class="title">Delete</span><span class="params">(<span class="keyword">int</span> i)</span></span>;         </span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">PrintList</span><span class="params">()</span></span>;               </span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">Node&lt;DataType&gt; *first;          </span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">LinkList</span>&lt;DataType&gt;:</span>:LinkList()</span><br><span class="line">&#123;</span><br><span class="line">first = <span class="keyword">new</span> Node&lt;DataType&gt;;</span><br><span class="line">first-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">LinkList</span>&lt;DataType&gt;:</span>:LinkList(DataType a[], <span class="keyword">int</span> n)</span><br><span class="line">&#123;</span><br><span class="line">first = <span class="keyword">new</span> Node&lt;DataType&gt;;</span><br><span class="line">first-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">&#123;</span><br><span class="line">Node&lt;DataType&gt; *s = <span class="keyword">new</span> Node&lt;DataType&gt;;</span><br><span class="line">s-&gt;data = a[i];</span><br><span class="line">s-&gt;next = first-&gt;next;</span><br><span class="line">first-&gt;next = s;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">LinkList</span>&lt;DataType&gt;:</span>:~LinkList()</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">while</span> (first != <span class="literal">NULL</span>)</span><br><span class="line">&#123;</span><br><span class="line">Node&lt;DataType&gt;* q = first;</span><br><span class="line">first = first-&gt;next;</span><br><span class="line"><span class="keyword">delete</span> q;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">int</span> <span class="title">LinkList</span>&lt;DataType&gt;:</span>:Length()</span><br><span class="line">&#123;</span><br><span class="line">Node&lt;DataType&gt;* p = first-&gt;next;</span><br><span class="line"><span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span> (p != <span class="literal">NULL</span>)</span><br><span class="line">&#123;</span><br><span class="line">p = p-&gt;next;</span><br><span class="line">count++;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> count;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">DataType</span> <span class="title">LinkList</span>&lt;DataType&gt;:</span>:Get(<span class="keyword">int</span> i)</span><br><span class="line">&#123;</span><br><span class="line">Node&lt;DataType&gt;* p = first-&gt;next;</span><br><span class="line"><span class="keyword">int</span> count = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">while</span> (p != <span class="literal">NULL</span> &amp;&amp; count&lt;i)</span><br><span class="line">&#123;</span><br><span class="line">p = p-&gt;next;</span><br><span class="line">count++;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (p == <span class="literal">NULL</span>) <span class="keyword">throw</span> <span class="string">"Location"</span>;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">return</span> p-&gt;data;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">int</span> <span class="title">LinkList</span>&lt;DataType&gt;:</span>:Locate(DataType x)</span><br><span class="line">&#123;</span><br><span class="line">Node&lt;DataType&gt; *p = first-&gt;next;</span><br><span class="line"><span class="keyword">int</span> count = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">while</span> (p != <span class="literal">NULL</span>)</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">if</span> (p-&gt;data == x) <span class="keyword">return</span> count;</span><br><span class="line">p = p-&gt;next;</span><br><span class="line">count++;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">void</span> <span class="title">LinkList</span>&lt;DataType&gt;:</span>:Insert(<span class="keyword">int</span> i, DataType x)</span><br><span class="line">&#123;</span><br><span class="line">Node&lt;DataType&gt; *p = first;</span><br><span class="line"><span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span> (p != <span class="literal">NULL</span> &amp;&amp; count&lt;i - <span class="number">1</span>)</span><br><span class="line">&#123;</span><br><span class="line">p = p-&gt;next;</span><br><span class="line">count++;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (p == <span class="literal">NULL</span>) <span class="keyword">throw</span> <span class="string">"Location"</span>;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">Node&lt;DataType&gt; *s = <span class="keyword">new</span> Node&lt;DataType&gt;;</span><br><span class="line">s-&gt;data = x;</span><br><span class="line">s-&gt;next = p-&gt;next;</span><br><span class="line">p-&gt;next = s;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">DataType</span> <span class="title">LinkList</span>&lt;DataType&gt;:</span>:Delete(<span class="keyword">int</span> i)</span><br><span class="line">&#123;</span><br><span class="line">Node&lt;DataType&gt; *p = first;</span><br><span class="line"><span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span> (p != <span class="literal">NULL</span> &amp;&amp; count&lt;i - <span class="number">1</span>)</span><br><span class="line">&#123;</span><br><span class="line">p = p-&gt;next;</span><br><span class="line">count++;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (p == <span class="literal">NULL</span> || p-&gt;next == <span class="literal">NULL</span>) <span class="keyword">throw</span> <span class="string">"Location"</span>;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">Node&lt;DataType&gt; *q = p-&gt;next;</span><br><span class="line"><span class="keyword">int</span> x = q-&gt;data;</span><br><span class="line">p-&gt;next = q-&gt;next;</span><br><span class="line"><span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">void</span> <span class="title">LinkList</span>&lt;DataType&gt;:</span>:PrintList()</span><br><span class="line">&#123;</span><br><span class="line">Node&lt;DataType&gt; *p = first-&gt;next;</span><br><span class="line"><span class="keyword">while</span> (p != <span class="literal">NULL</span>)</span><br><span class="line">&#123;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; p-&gt;data &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">p = p-&gt;next;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">LinkList&lt;<span class="keyword">int</span>&gt; p;</span><br><span class="line">p.Insert(<span class="number">1</span>, <span class="number">6</span>);</span><br><span class="line">p.Insert(<span class="number">2</span>, <span class="number">9</span>);</span><br><span class="line">p.PrintList();</span><br><span class="line">p.Insert(<span class="number">2</span>, <span class="number">3</span>);</span><br><span class="line">p.PrintList();</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; p.Get(<span class="number">2</span>) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; p.Locate(<span class="number">9</span>) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; p.Length() &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">p.Delete(<span class="number">1</span>);</span><br><span class="line">p.PrintList();</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="链式存储的优缺点"><a href="#链式存储的优缺点" class="headerlink" title="链式存储的优缺点"></a>链式存储的优缺点</h4><p><strong>优点</strong>：</p><ol><li>插入、删除不需移动其他元素，只需改变指针.</li><li>链表各个节点在内存中空间不要求连续，空间利用率高 </li></ol><p><strong>缺点</strong>：</p><ol><li>查找需要遍历操作，比较麻烦</li></ol><h2 id="五、其他线性表"><a href="#五、其他线性表" class="headerlink" title="五、其他线性表"></a>五、其他线性表</h2><h3 id="循环链表"><a href="#循环链表" class="headerlink" title="循环链表"></a>循环链表</h3><p>&emsp;&emsp;循环链表是另一种形式的链式存储结构。它的特点是表中最后一个结点的指针域指向头结点，整个链表形成一个环。（通常为了使空表和非空表的处理一致，通常也附加一个头结点）</p><p><img src="http://img.blog.csdn.net/20180311153541327?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p>&emsp;&emsp;在很多实际问题中，一般都使用<strong>尾指针</strong>来指示循环链表，因为使用尾指针查找开始结点和终端结点都很方便。</p><p><img src="http://img.blog.csdn.net/20180311153630144?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p>&emsp;&emsp;循环链表没有增加任何存储量，仅对链接方式稍作改变，循环链表仅在循环条件与单链表不同。从循环链表的任一结点出发可扫描到其他结点，增加了灵活性。但是，由于循环链表没有明显的尾端，所以链表操作有进入死循环的危险。通常以判断指针是否等于某一指定指针来判定是否扫描了整个循环链表。</p><h3 id="双链表"><a href="#双链表" class="headerlink" title="双链表"></a>双链表</h3><p>&emsp;&emsp;循环链表虽然可以从任意结点出发扫描其他结点，但是如果要查找其前驱结点，则需遍历整个循环链表。为了快速确定任意结点的前驱结点，可以再每个节点中再设置一个指向前驱结点的指针域，这样就形成了<strong>双链表</strong>。</p><p><img src="http://img.blog.csdn.net/20180311153714830?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p><strong>存储方法</strong></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">struct</span> <span class="title">Node</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">DataType data;</span><br><span class="line">Node&lt;DataType&gt; *prior,*next;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>&emsp;&emsp;结点p的地址既存储在其前驱结点的后继指针域内，又存储在它后继结点的前驱指针域中</p><p>需要注意：</p><ol><li>循环双链表中求表长、按位查找、按值查找、遍历等操作的实现与单链表基本相同。</li><li>插入操作需要修改4个指针，并且要注意修改的相对顺序。</li></ol><h3 id="静态链表"><a href="#静态链表" class="headerlink" title="静态链表"></a>静态链表</h3><p>&emsp;&emsp;静态链表是用数组来表示单链表，用数组元素的下标来模拟单链表的指针。</p><p><strong>静态链表的存储结构</strong>：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> MaxSize = <span class="number">100</span>;</span><br><span class="line"><span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">DataType</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">struct</span> <span class="title">Node</span>&#123;</span></span><br><span class="line">DataType data;</span><br><span class="line">    <span class="keyword">int</span> next;</span><br><span class="line">&#125;SList[MaxSize];</span><br></pre></td></tr></table></figure><p><strong>静态链表存储示意图</strong>：</p><p><img src="http://img.blog.csdn.net/20180311153823776?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p><strong>静态链表插入操作示意图</strong>：</p><p><img src="http://img.blog.csdn.net/20180311153857775?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p><strong>静态链表删除操作示意图</strong>：</p><p><img src="http://img.blog.csdn.net/20180311153938316?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p>&emsp;&emsp;静态链表虽然是用数组来存储线性表的元素，但在插入和删除操作时，只需要修改游标，不需要移动表中的元素，从而改进了在顺序表中插入和删除操作需要移动大量元素的缺点，但是它并没有解决连续存储分配带来的表长难以确定的问题。</p><h3 id="间接寻址"><a href="#间接寻址" class="headerlink" title="间接寻址"></a>间接寻址</h3><p>&emsp;&emsp;间接寻址是将数组和指针结合起来的一种方法，它将数组中存储的单元改为存储指向该元素的指针。</p><p><img src="http://img.blog.csdn.net/20180311154017416?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p>&emsp;&emsp;该算法的时间复杂度仍为<script type="math/tex">O(n)</script> ，但当每个元素占用较大空间时，比顺序表的插入快的多。线性表的间接寻址保持了顺序表随机存取的优点，同时改进了插入和删除操作的时间性能，但是它也没有解决连续存储分配带来的表长难以确定的问题。</p><p>&emsp;&emsp;具体代码实现均可在<a href="https://github.com/Quanfita/Data_Stuctures" target="_blank" rel="noopener">GitHub</a>中找到。如有错误，请在评论区指正。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li>数据结构（C++版）王红梅等编著</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;线性表&quot;&gt;&lt;a href=&quot;#线性表&quot; class=&quot;headerlink&quot; title=&quot;线性表&quot;&gt;&lt;/a&gt;线性表&lt;/h1&gt;&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/la
      
    
    </summary>
    
      <category term="Data Structures" scheme="http://quanfita.cn/categories/Data-Structures/"/>
    
    
      <category term="Linear List" scheme="http://quanfita.cn/tags/Linear-List/"/>
    
  </entry>
  
  <entry>
    <title>计算机网络笔记整理——绪论</title>
    <link href="http://quanfita.cn/2018/03/11/Internet/"/>
    <id>http://quanfita.cn/2018/03/11/Internet/</id>
    <published>2018-03-11T08:06:09.495Z</published>
    <updated>2018-03-11T08:08:34.874Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h2 id="因特网"><a href="#因特网" class="headerlink" title="因特网"></a>因特网</h2><p>所有接入因特网的设备都称为<strong>主机 </strong>或<strong>端系统</strong>。</p><p>主机通过<strong>通信链路</strong>和<strong>分组交换机</strong>连接，<strong>路由器</strong>和<strong>链路层交换机</strong>都是分组交换机。</p><p>一个分组所经历的一系列通信链路和分组交换机称为通过该网络的<strong>路径</strong>。</p><p><strong>ISP</strong>是因特网服务供应商，有不同层级，每个ISP是一个由多个分组交换机和多段通信链路组成的网络。</p><p><strong>协议</strong>控制着因特网中信息的接收和发送。因特网的主要协议统称为<strong>TCP/IP</strong>。</p><ul><li>TCP 可靠 面向连接  确保传递与流量控制</li><li>IP UDP 不可靠 非面向连接</li></ul><p><img src="http://img.blog.csdn.net/20180311155659592?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p><strong>因特网的两种描述方法</strong>：一种是根据它的硬件和软件组成来描述，另一种是根据基础设施向分布式应用程序提供的服务来描述。</p><p><strong>通信链路</strong>有不同的物理媒体组成，不同媒体具有不同速率，包括同轴电缆、铜线、光缆、无线电</p><p><strong>分组</strong>： 当一台端系统有数据要向另一台端系统发送时，端系统将数据分段并在每段加上首部字节，由此形成的信息包称为<strong>分组</strong>。</p><p>一个<strong>协议</strong>定义了在两个或多个通信实体之间交换的报文格式和次序，以及在报文或其他事件方面所采取的动作、传输和/或接收。</p><p><strong>端系统 = 主机</strong>：和因特网相连的计算机等设备（如TV，Web服务器，手提电脑）。</p><p>主机有时候有进一步分为两类：<strong>客户机（client）</strong>和<strong>服务器（server）</strong>。</p><p>在网络软件的上下文中，客户机和服务器有另一种定义，<strong>客户机程序（client program）</strong>是运行在一个端系统上的程序，它发出请求，并从运行在另一个端系统上的<strong>服务器程序（server program）</strong>接收服务。</p><p>客户机-服务器因特网应用程序是<strong>分布式应用程序（distributed application）</strong>。</p><p>还有的应用程序是P2P对等应用程序，其中的端系统互相作用并运行执行客户机和服务器功能的程序。</p><h2 id="网络边缘"><a href="#网络边缘" class="headerlink" title="网络边缘"></a>网络边缘</h2><p>互联网的边缘部分也叫<strong>资源子网</strong>。</p><p><strong>接入网</strong>：将端系统连接到<strong>边缘路由器（edge router）</strong>的物理链路。</p><p><strong>边缘路由器</strong>：端系统到任何其他远程端系统的路径上的第一台路由器。</p><p><img src="http://img.blog.csdn.net/20180311155759387?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><h3 id="网络接入类型"><a href="#网络接入类型" class="headerlink" title="网络接入类型"></a>网络接入类型</h3><p>网络接入的三种类型：（分类并不严格）</p><ol><li><p>住宅接入（residential  access），将<strong>家庭端系统（PC或家庭网络）</strong>和网络相连。</p><p>​    1. 通过普通模拟电话线用<strong>拨号调制解调器（dial-up modem）</strong>与住宅ISP相连。家用调制解调器将PC输出的数字信号转化为模拟形式，以便在模拟电话线上传输。模拟电话线由<strong>双绞铜线</strong>构成，就是用于打普通电话的电话线。允许56kbps接入（下载上传速度慢），用户上网就不能打电话了。</p><p>​    2. <strong>数字用户线（digital subscriber line，DSL）</strong>:一种新型的调制解调器技术，类似于<strong>拨号调制解调器</strong>，也运行在现有的双绞线电话线上，通过限制用户和ISP调制解调器之间的距离，DSL能够以高得多的速率传输和接受数据。（使用频分多路复用技术），分为上行信道和下行信道，两个信道速率不一样。</p><p>​    3. <strong>混合光纤同轴电缆（hybrid fiber-coaxial cable， HFC）</strong>：使用了光缆和同轴电缆相结合的技术。扩展了当前用于广播电缆电视的电缆网络，需要<strong>电缆调制解调器（cable modem），</strong>分为上行信道和下行信道，共享广播媒体（HFC特有），信道都是共享的，需要一个分布式多路访问协议，以协调传输和避免碰撞。</p></li><li><p>公司接入（company access），将<strong>商业或教育机构中的端系统</strong>和网络相连</p><p>​    1. 局域网（LAN）</p><p>​    2. 以太网</p><p>​        1. 共享以太网</p><p>​        2. 交换以太网</p></li><li><p>无线接入（wireless access），将<strong>移动端系统</strong>与网络相连。分为两类</p><p>​    1. <strong>无线局域网（wireless LAN）：</strong>无线用户与位于几十米半径内的基站（无线接入点）之间传输/接收分组。这些基站和有线的因特网相连接，因而为无线用户提供连接到有线网络的服务。</p><ol><li><strong>广域无线接入网（wide-area wireless access network）</strong>：分组经用于蜂窝电话的相同无线基础设施进行发送，基站由电信提供商管理，为数万米半径内的用户提供无线接入服务。</li></ol></li></ol><p>基于IEEE 802.11技术的无线局域网也被称为无线以太网和WiFi。</p><p>HFC使用了光缆和同轴电缆相结合的技术，拨号56kbps调制解调器和ASDL使用了双绞铜线；移动接入网络使用了无线电频谱。</p><h3 id="物理媒体"><a href="#物理媒体" class="headerlink" title="物理媒体"></a>物理媒体</h3><p>物理媒体分为两类：</p><p>​    1. 导引型媒体（guided media）：电波沿着<strong>固体媒体</strong>（光缆，双绞铜线或同轴电缆）被导引。</p><ol><li>非导引型媒体（unguided media）：电波在<strong>空气或外层空间</strong>（在无线局域网或数字卫星频道）中传播；</li></ol><p>导引型媒体（guided media）</p><p>​    1. <strong>双绞铜线</strong>：最便宜，使用最普遍，两根线被绞合起来，以减少对邻近双绞线的电气干扰。一根电缆由许多双绞线捆扎在一起，并在外面覆盖上保护性防护层，一堆电线构成一个通信链路。<strong>非屏蔽双绞线UTP</strong>常用于建筑物内的计算机网络中，即用于局域网（LAN）中。双绞线最终已经成为高速LAN联网的主要方式。</p><p>​   2. <strong>同轴电缆</strong>：能作为导引式共享媒体，具有高比特速率。</p><ol><li><strong>光缆</strong>：不受电磁干扰，长达100km的光缆信号衰减极低，并且很难接头。</li></ol><p>非导引型媒体（unguided media）：电波在<strong>空气或外层空间</strong>（在无线局域网或数字卫星频道）中传播；</p><p>​    1. <strong>陆地无线电信道</strong>：具有穿透墙壁，提供与移动用户的连接以及长距离承载信号的能力。</p><ol><li><strong>卫星无线电信道</strong></li></ol><p>   一颗通信卫星连接两个或多个位于地球的微波发射方/接收方，它们被称为地面站。卫星无线电信道分为<strong>同步卫星</strong>和<strong>低地球轨道卫星</strong>。</p><p>   同步卫星永久的停留在地球上方相同的点，卫星链路常用于电话网或因特网的主干。卫星链路在那些无法使用DSL或基于电缆的因特网接入区域，也越来越多地用作高速住宅因特网接入。</p><p>   低地球轨道卫星 围绕地球旋转，彼此通信，未来低地球轨道卫星技术也许能用于因特网接入。</p><h2 id="网络核心"><a href="#网络核心" class="headerlink" title="网络核心"></a>网络核心</h2><p>网络核心，即互联了<strong>因特网端系统的分组交换机</strong>和<strong>链路的网状网络</strong>，也叫<strong>通信子网</strong>。</p><p><img src="http://img.blog.csdn.net/20180311155906494?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p>通过网络链路和交换机移动数据有两种基本方法：<strong>电路交换（circuit switching）</strong>和<strong>分组交换（packet swiitching）</strong>。</p><p><strong>电路交换网络</strong>中，沿着端系统通信路径，为端系统之间通信所提供的资源在通信会话期间会被预留。例子有电话网络。</p><p><strong>分组交换网络</strong>中，这些资源不被预留。例子有因特网网络。</p><ol><li><p>电路交换：创建专用的端到端连接，独占带宽 有独立的建立、连接过程；</p></li><li><p>电路交换网络中的多路复用</p><p>​    1. 频分多路复用（Frequency-Division Multiplexing，FDM）</p><p>​    2. 时分多路复用（Time-Division Multiplexing，TDM）</p></li><li><p>分组交换：共享带宽</p><p>​    存储转发传输机制：在交换机能够开始向输出链路传输该分组的第一个比特之前，必须接收到整个分组。</p></li><li><p>分组交换和电路交换对比：统计多路复用</p></li></ol><h3 id="分组交换"><a href="#分组交换" class="headerlink" title="分组交换"></a>分组交换</h3><p>​    分组交换是以分组为单位进行传输和交换的，它是一种存储——转发交换方式，即将到达交换机的分组先送到存储器暂时存储和处理，等到相应的输出电路有空闲时再送出。</p><p><img src="http://img.blog.csdn.net/20180311155922915?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p>​    <strong>优点</strong>：</p><p>​    （1）分组交换不需要为通信双反预先建立一条专用的通信线路，不存在连接建立时延，用户可随时发送分组。</p><p>​    （2）由于采用存储转发方式，加之交换节点具有路径选择，当某条传输线路故障时可选择其他传输线路，提高了传输的可靠性。</p><p>​    （3）通信双反不是固定的战友一条通信线路，而是在不同的时间一段一段地部分占有这条物理通路，因而大大提高了通信线路的利用率。</p><p>​    （4）加速了数据在网络中的传输。因而分组是逐个传输，可以使后一个分组的存储操作与前一个分组的转发操作并行，这种流水线式传输方式减少了传输时间。</p><p>​    （5）分组长度固定，相应的缓冲区的大小也固定，所以简化了交换节点中存储器的管理。</p><p>​    （6）分组较短，出错几率减少，每次重发的数据量也减少，不仅提高了可靠性，也减少了时延。</p><p><img src="http://img.blog.csdn.net/20180311160209121?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p>​    <strong>缺点：</strong></p><p>​    （1）由于数据进入交换节点后要经历存储转发这一过程，从而引起的转发时延（包括接受分组、检验正确性、排队、发送时间等），而且网络的通信量越大，造成的时延就越大，实时性较差。</p><p>​    （2）分组交换只适用于数字信号。</p><p>​    （3）分组交换可能出现失序，丢失或重复分组，分组到达目的节点时，对分组按编号进行排序等工作，增加了麻烦。</p><p>​    <strong>综上</strong>，若传输的数据量很大，而且传送时间远大于呼叫时间，则采用电路交换较为合适；当端到端的通路有很多段链路组成是，采用分组交换较为合适。从提高整个网络的信道利用率上看，分组交换优于电路交换。</p><h3 id="电路交换"><a href="#电路交换" class="headerlink" title="电路交换"></a>电路交换</h3><p>​    电路交换是以电路连接为目的的交换方式，通信之前要在通信双方之间建立一条被双方独占的物理通道。</p><p>​    <strong>电路交换的三个阶段</strong>：</p><p>​    （1）建立连接    （2）通信    （3）释放连接</p><p>​    <strong>优点</strong>：</p><p>​    （1）由于通信线路为通信双方用户专用，数据直达，所以传输数据的时延非常小。</p><p>​    （2）通信双方之间的屋里通路一旦建立，双方可以随时通信，实时性强。</p><p>​    （3）双方通信时按发送顺序传送数据，不存在失序问题。</p><p>​    （4）电路交换既适用于传输模拟信号，也适用于传输数字信号。</p><p>​    （5）电路交换的交换设备及控制均比较简单。</p><p>​    <strong>缺点</strong>：</p><p>​    （1）电路交换平均连接建立时间对计算机通信来说较长。</p><p>​    （2）电路交换家里连接后，物理通路被通信双方独占，即使通信线路空闲，也不能供其他用户使用，因而信道利用率低。</p><p>​    （3）电路交换时，数据直达，不同类型，不同规格，不同速率的终端很难相互进行通信，也难以在通信过程中进行差错控制。</p><h2 id="时延、丢包和吞吐量"><a href="#时延、丢包和吞吐量" class="headerlink" title="时延、丢包和吞吐量"></a>时延、丢包和吞吐量</h2><p>时延分为<strong>节点处理时延（nodal processing delay），排队时延（queuing delay），传输时延（transmission delay）</strong>和<strong>传播时延（propagation delay）</strong>，这些加起来就是<strong>节点总时延（total nodal delay），即</strong></p><p><strong>节点总时延 = 节点处理时延 + 排队时延 + 传输时延 + 传播时延</strong></p><h3 id="时延类型"><a href="#时延类型" class="headerlink" title="时延类型"></a>时延类型</h3><ol><li><p><strong>处理时延</strong></p><ol><li>检查分组首部和决定将分组导向哪一个队列；</li><li>其他：检查比特级差错所需要的时间。</li></ol></li><li><p><strong>排队时延</strong></p><p>在队列中，当分组在链路上等待传输时所需的时间，取决于先期到达的，正在排队等待想链路传输分组的数量。</p></li><li><p><strong>传输时延</strong></p><ol><li>将所有分组的比特推向链路所需要的时间。</li><li>用L比特表示分组的长度，用R bps表示从路由器A到路由器B的链路传输速率。（对于一条10Mbps的以太网链路，速率R = 10Mbps）,<strong>传输时延（</strong>又称为<strong>存储转发时延）</strong>是<strong>L/R</strong>。</li></ol></li><li><p><strong>传播时延</strong></p><p>​    1. 从该链路的起点到路由器B传播所需要的时间是<strong>传播时延</strong>。该比特以该链路的传播速率传播。</p><p>​    2. 传播时延 = 两台路由器的距离d / 传播速率s。</p><ol><li>传播速率取决于该链路的物理媒体（即光纤，双绞铜线等），速率范围是<script type="math/tex">2\times10^8\sim3\times10^8 m/s</script></li></ol></li></ol><h3 id="端到端时延"><a href="#端到端时延" class="headerlink" title="端到端时延"></a>端到端时延</h3><p>假定在源主机和目的主机之间有<script type="math/tex">N-1</script>台路由器，并且该网络是无拥塞的（因此排队时延是微不足道的），处理时延为<script type="math/tex">d_{proc}</script> ，每台路由器和源主机的输出速率是 R bps，每条链路的传播时延是<script type="math/tex">d_{proc}</script> ，节点时延累加起来得到端到端时延</p><script type="math/tex; mode=display">d_{end-end} = N（d_{proc} + d_{trans} + d_{prop}）</script><script type="math/tex; mode=display">d_{trans} = L /R</script><ol><li><p><strong>Traceroute程序</strong></p><p>能够在任何因特网主机上运行。当用户指定一个目的主机名字时，元主机中的改程序朝着该目的地发送多个特殊分组之一时，它向源回送一个短报文，该报文包括该路由器的名字和地址。RFC1393描述了Traceroute。</p></li><li><p><strong>端系统、应用程序和其他时延</strong></p></li></ol><p>​    除了处理时延，传输时延，传播时延外，端系统中还有一些其他重要的时延：</p><p>​    1. 拨号调制解调器引入的<strong>调制/编码时延</strong>，量级在几十毫秒，对于以太网，电缆调制解调器和DSL等接入技术，这种时延是不太多的；</p><p>​    2. 向共享媒体传输分组的端系统可以将有意地延迟传输作为其协议的一部分，以便与其他端系统共享媒体。（第五章探讨）</p><p>​    3. <strong>媒体分组化时延</strong>，在IP话音（VoIP）应用中。在VoIP中，发送方在向因特网传递分组之前必须首先用编码的数字化语音填充分组，这种填充分组的时间就是<strong>分组化时延</strong>。（可能比较大）</p><h3 id="计算机网络中的吞吐量"><a href="#计算机网络中的吞吐量" class="headerlink" title="计算机网络中的吞吐量"></a>计算机网络中的吞吐量</h3><p>​    除了时延和丢包外，计网中另个一个必不可少的性能测度就是<strong>端到端吞吐量</strong>。</p><p>​    吞吐量分为<strong>瞬时吞吐量（instancous throughput）</strong>和<strong>平均吞吐量（average throughput）</strong>，我们可以把他们类比为以前物理学过的瞬时速度和平均速度。</p><p>​    瞬时吞吐量是主机B接受到该文件的一个速率，平均吞吐量是所有比特F/T秒，即F/T bps。</p><p>​    对于某些应用程序（譬如因特网电话），希望他们具有低时延，并保持高于某一阈值的一致的瞬时吞吐量，对于其他应用程序（譬如文件传输等等），时延不是很重要，但是希望能具有尽可能高的吞吐量。</p><p>​    吞吐量：单位时间内通过某个网络（或信道、接口）的数据量。</p><h2 id="协议层次"><a href="#协议层次" class="headerlink" title="协议层次"></a>协议层次</h2><p>因特网协议栈：</p><p>应用层、运输层、网络层、数据链路层、物理层。</p><p>OIS的七层协议：</p><p>应用层、表示层、会话层、运输层、网络层、数据链路层、物理层。</p><p>TCP/IP四层协议：</p><p>应用层、运输层、网际层、网络接口层</p><p><img src="http://img.blog.csdn.net/20180311160251696?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p><strong>物理层</strong>：最基础的一层，建立在传输媒介基础上，起到<strong>建立、维护和取消物理连接的作用</strong>，实现设备之间的物理接口。物理层只接收和发送一串比特流，不考虑信息的意义和信息结构。包括对连接到网络上的设备描述其各种机械的、电气的、功能的规定。<strong>典型设备有：光纤、同轴电缆、双绞线、中继器和集线器</strong>。</p><p><strong>数据链路层</strong>：在物理层提供比特流服务的基础上，将比特信息封装成数据帧Frame，起到在物理层上建立、撤销、标识逻辑链接和链路复用以及差错校验等功能。通过使用接收系统的硬件地址或物理地址来寻址。建立相邻结点之间的数据链接，通过差错控制提供数据帧（Frame）在信道上无差错的传输，同时为其上面的网络层提供有效的服务。数据链路层在不可靠的物理介质上提供可靠的传输。该层的作用包括：<strong>物理地址寻址、数据的成帧、流量控制、数据检错、重发等</strong>。<strong>典型设备有：二层交换机、网桥、网卡</strong>。<strong>差错控制</strong>：在数据传输过程中如何发现并更在错误；流量控制：通信双方速度存在差异，需要协调匹配通信正常。</p><p><strong>网络层</strong>：或通信子网层，是高层协议之间的界面层，用于控制通信子网的操作，是通信子网与资源子网的接口。在网络间进行通信的计算机之间可能会通过多个数据链路，也可能还要经过很多通信子网。网络层的任务就是选择合适的网间路由和交换结点，确保数据及时传送。网络层将解封装数据链路层收到的帧，提取数据包，包中封装有网络层包头，其中含有逻辑地址信息，包括源站点和目的站点地址的网络地址。<strong>典型设备是路由器</strong>。<br>网络层主要功能为<strong>管理数据通信</strong>，<strong>实现端到端的数据传送服务</strong>；<strong>主体协议是IP协议</strong></p><p><strong>运输层</strong>：建立在网络层和会话层之间，实质上它是网络体系结构中低层与高层之间衔接的一个接口层。用一个寻址机制来标识一个特殊的应用程序（端口号）。传输层不仅是一个独立的结构层，它还是整个分层体系结构的核心。传输层的数据单元是由数据组织成的数据段（Segment）这个层负责获取全部信息，因此它必须跟踪数据单元碎片、乱序到达的数据包和其他在传输过程中可能发生的危险。<br>主要功能为负责总体的<strong>数据传输</strong>和<strong>数据控制</strong>，<strong>主要包括两个协议：TCP：传输控制协议；UDP：用户报文协议</strong></p><p><strong>会话层</strong>：也称会晤层或对话层，在会话层及以上的高层次中，数据传达的单位不再另外命名，统称为报文。会话层不参与具体传送，它提供包括访问校验和会话管理在内的建立和维护应用之间通信的机制。会话层提供的服务可使应用建立和维持会话，并使会话同步，会话层使用校验点可以使通信会话在通信失效时从校验点继续恢复通信，这对传送大型文件极为重要。<br>主要功能是<strong>为通信进程建立连接</strong>。</p><p><strong>表示层</strong>：对上服务应用层，对下接收会话层的服务，是为应用过程之中传送的信息提供表示方法的服务，它关心的只是发出的信息的语义和语法。表示层要完成某些特定的功能，主要有不同的数据编码格式的转换，提供数据压缩、解压缩服务，对数据进行加密、解密。如图像格式的显示就是由位于表示层的协议来支持的。表示层提供的服务包括：<strong>语法选择、语法转换</strong>等，语法选择是提供一种初始语法和以后修改这种选择的手段。语法转换涉及代码转换和字符集的转换、数据格式的修改以及对数据结构操作的适配。主要功能是进行<strong>加密和压缩</strong>。</p><p><strong>应用层</strong>：是通信用户之间的窗口，为用户提供网络管理、文件传输、事务处理等服务。其中包含了若干独立的用户通用的服务协议模块。网络应用层是OSI的最高层，<strong>为网络用户之间的通信提供专用的程序</strong>。主要功能是<strong>为通信进程建立连接</strong>。</p><h2 id="攻击威胁下的网络"><a href="#攻击威胁下的网络" class="headerlink" title="攻击威胁下的网络"></a>攻击威胁下的网络</h2><ol><li>坏家伙能够经因特网将恶意软件放入你的计算机</li><li>坏家伙能够攻击服务器和网络基础设施</li><li>能够嗅探分组</li><li>能够伪装成信任的人：IP哄骗（IP spoofing）可用端点鉴别（end-point authentication）机制</li><li>修改或删除报文：中间人攻击</li></ol><p>恶意软件（malware）：删除文件、收集隐私信息</p><p>僵尸网络（botnet）：对目标主机展开垃圾邮件分发或分布式拒绝服务攻击。</p><p>病毒（virus）：恶意可执行代码并自我复制</p><p>蠕虫（worm）：不需明显交互即可感染</p><p>特洛伊木马（Trojan horse）：为进行非法目的的计算机病毒</p><h2 id="计算机网络发展史"><a href="#计算机网络发展史" class="headerlink" title="计算机网络发展史"></a>计算机网络发展史</h2><ol><li><p>分组交换 1961-1972</p><p>第一个分组交换网ARPANET</p></li><li><p>专用网络和网络互联 1972-1980</p><p>开始产生TCP、UDP等协议</p></li><li><p>网络的激增 1980-1990</p><p>建立CSFNET</p></li><li><p>因特网爆炸 20世纪90年代<br>万维网应用出现</p></li></ol><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a href="https://www.jianshu.com/p/21f3af4653bb" target="_blank" rel="noopener">计算机网络－笔记</a></p><p><a href="http://blog.csdn.net/liuqiyao_01/article/details/39001067" target="_blank" rel="noopener">电路交换与分组交换的区别</a></p><p><a href="http://www.cnblogs.com/ArtemisZ/p/7555079.html" target="_blank" rel="noopener">《计算机网络自顶向下方法》读书笔记</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;h2 id=&quot;因特网&quot;&gt;&lt;a href=&quot;#因特网&quot; 
      
    
    </summary>
    
      <category term="note" scheme="http://quanfita.cn/categories/note/"/>
    
    
      <category term="计算机网络" scheme="http://quanfita.cn/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>2017~2018年寒假总结</title>
    <link href="http://quanfita.cn/2018/03/06/2017_2018_winter_holiday/"/>
    <id>http://quanfita.cn/2018/03/06/2017_2018_winter_holiday/</id>
    <published>2018-03-06T12:01:44.704Z</published>
    <updated>2018-03-06T12:03:54.115Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;&emsp;又一个寒假过去了，逢年过节胖三斤。。。上称一称，唉~胖了五斤 o(≧口≦)o </p><p>&emsp;&emsp;假期里除了吃饭睡觉串亲戚，还有算法博客拜年祭。</p><p>&emsp;&emsp;开学刚回学校就闹胃病 (╯﹏╰) ，现在我要顶着胃部的不适，完成这篇假期总结</p><h2 id="假期总结"><a href="#假期总结" class="headerlink" title="假期总结"></a>假期总结</h2><ol><li>学习李航的《统计学习方法》并做好笔记整理（学是学完了，笔记整理可累死我了…到现在还没整完）</li><li>用TensorFlow搭建一个完整的图像分类器</li><li>利用TensorFlow神经网络参加Kaggle竞赛两次（过年这几天都是晚上回家白天出去，所以没时间搭模型，只参加了Titanic生还者预测和Digit手写字体识别）</li><li>大年三十准时观看bilibili拜年祭（因为春晚太无聊了。。。所以我估计以后的除夕夜我都是看拜年祭的）</li><li>学习Andrew Ng的Coursera课程（只看到第四部分卷积神经网络）</li><li>搭建个人博客（嗯，你们如果看到这篇文章就一定也知道我的博客长什么样了）</li><li>给一个初一的小妹妹补数学（滑稽脸）</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;emsp;&amp;emsp;又一个寒假过去了，逢年过节胖三斤。。。上称一称，唉~胖了五斤 o(≧口≦)o &lt;/p&gt;
&lt;p&gt;&amp;emsp;&amp;emsp;假期里除了吃饭睡觉串亲戚，还有算法博客拜年祭。&lt;/p&gt;
&lt;p&gt;&amp;emsp;&amp;emsp;开学刚回学校就闹胃病 (╯﹏╰) ，现在我要顶
      
    
    </summary>
    
      <category term="life" scheme="http://quanfita.cn/categories/life/"/>
    
    
      <category term="假期总结" scheme="http://quanfita.cn/tags/%E5%81%87%E6%9C%9F%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>机器学习入门之《统计学习方法》笔记整理——决策树</title>
    <link href="http://quanfita.cn/2018/03/03/decision_tree/"/>
    <id>http://quanfita.cn/2018/03/03/decision_tree/</id>
    <published>2018-03-03T15:12:43.049Z</published>
    <updated>2018-03-04T00:19:05.035Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><p>&emsp;&emsp;决策树是一种基本的分类和回归算法。<br>&emsp;&emsp;决策树模型呈树形结构，可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。<br>&emsp;&emsp;决策树模型由结点和有向边组成，结点分为内部结点和叶结点，内部结点表示特征，叶结点表示类，有向边表示某一特征的取值。</p><h2 id="决策树模型与学习"><a href="#决策树模型与学习" class="headerlink" title="决策树模型与学习"></a>决策树模型与学习</h2><h3 id="决策树模型"><a href="#决策树模型" class="headerlink" title="决策树模型"></a>决策树模型</h3><p>&emsp;&emsp;分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点（node）和有向边（directed edge）组成。结点有两种类型：内结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或者属性，叶结点表示一个类。</p><h3 id="决策树与if-then规则"><a href="#决策树与if-then规则" class="headerlink" title="决策树与if-then规则"></a>决策树与if-then规则</h3><p>&emsp;&emsp;可以将决策树看成是一个if-then规则的集合。将决策树转化成if-then规则的过程是这样的：由决策树的根结点到叶结点的每一条路径构建一条规则;路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论。</p><h3 id="决策树与条件概率分布"><a href="#决策树与条件概率分布" class="headerlink" title="决策树与条件概率分布"></a>决策树与条件概率分布</h3><p>&emsp;&emsp;决策树还表示给定特征条件下的类的条件概率分布。这一条件概率分布定义在特征空间的一个划分（partition）上。将特征空间划分为互不相交的单元（cell）或者区域（region），并在每个单元定义一个类的概率分布就构成了一个条件概率分布。 </p><h3 id="决策树学习"><a href="#决策树学习" class="headerlink" title="决策树学习"></a>决策树学习</h3><p>&emsp;&emsp;决策树学习，假定给定训练数据集</p><script type="math/tex; mode=display">D = \left \{ (x_1,y_1),(x_2,y_2),...,(x_n,y_n) \right \}$$ ,&emsp;&emsp;其中$$x_i = (x_i^{(1)},x_i^{(2)},...,x_i^{(n)} )^T$$ ，$$n$$ 为特征个数，$$y_i\in \left \{ 1,2,...,K \right \}$$ ，为类的标记，$$i=1,2,...,N$$ ，$$N$$ 为样本容量。学习的目标是根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。 ![这里写图片描述](http://img.blog.csdn.net/20180303170018174?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)&emsp;&emsp;决策树学习本质上是从训练数据集中归纳出一组分类规则。我们需要的是一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。另一个角度看，决策树学习是由训练数据集估计条件概率模型。我们选择的条件概率模型应该不仅对训练数据有很好的拟合，而且对未知数据有很好的预测。 &emsp;&emsp;决策树学习用损失函数表示这一目标。如下所述，决策树学习的损失函数通常是正则化的极大似然函数。决策树学习的策略是以损失函数为目标函数的最小化。 &emsp;&emsp;当损失函数确定以后，学习问题就变为在损失函数意义下选择最优决策树的问题。因为从所有可能的决策树中选取最优决策树是NP完全问题（NP的英文全称是Non-deterministic Polynomial的问题，即多项式复杂程度的非确定性问题），所以现实中决策树学习算法通常采用启发式方法，近似求解这一最优化问题。这样得到的决策树是次最优(sub-optimal)的。 &emsp;&emsp;决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。学习模型：根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确分类。该模型不仅对训练数据有很好的拟合，而且对未知数据有很好的越策学习策略：通常选择正则化的极大似然函数作为损失函数，损失函数最小化学习算法：采用启发式算法，近似求解上述最优化问题。递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类。过拟合：以上方法生成的决策树可能对训练数据有很好的分类能力，但对未知的数据却未必，即可能发生过拟合。剪枝：对生成的树自下而上进行剪枝，将树变得更简单，从而使它具有更好的泛化能力。特征选择：如果特征数量很多，也可以在学习开始的时候，对特征进行选择。## 特征选择### 特征选择问题&emsp;&emsp;特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。经验上扔掉这样的特征对决策树学习的精度影响不大。通常特征选择的准则是信息增益或信息增益比。### 信息增益####熵&emsp;&emsp;在信息论与概率统计中，熵（entropy）是表示随机变量不确定性的度量。设$$X$$ 是一个取有限个值的离散随机变量，其概率分布为$$P(X=x_i)=p_i, i=1,2,...,n</script><p>&emsp;&emsp;则随机变量<script type="math/tex">X</script> 的<strong>熵</strong>定义为</p><script type="math/tex; mode=display">H(X)=-\sum \limits_{i=1}^n p_i \log p_i</script><p>&emsp;&emsp;通常上式中的对数以2为底或者以自然对数e为底，这时熵的单位分别称作比特（bit）或纳特（nat）。由定义可知，熵只依赖于<script type="math/tex">X</script> 分布，而与<script type="math/tex">X</script> 的取值无关，所以也可以将<script type="math/tex">X</script> 的熵记作<script type="math/tex">H(p)</script> ，即</p><script type="math/tex; mode=display">H(p)=-\sum \limits_{i=1}^n p_i \log p_i</script><p>&emsp;&emsp;熵越大，随机变量的不确定性就越大。从定义可以验证 </p><script type="math/tex; mode=display">0\leq H(p)\leq \log n</script><p>&emsp;&emsp;当随机变量只取2个值时，当取值概率为0/1时，熵为0，此时完全没有不确定性。</p><h4 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h4><p>&emsp;&emsp;已知随机变量<script type="math/tex">(X,Y)</script> 的联合概率分布为： </p><script type="math/tex; mode=display">P(X=x_i,Y=y_i)=p_{ij}, i=1,2,...,n;j=1,2,...,m</script><p>&emsp;&emsp;条件熵<script type="math/tex">H(Y|X)</script> 表示已知<script type="math/tex">X</script> 情况下，<script type="math/tex">Y</script> 的分布的不确定性。计算如下： </p><script type="math/tex; mode=display">H(Y|X)=\sum \limits_{i=1}^n p_iH(Y|X=x_i)</script><p>&emsp;&emsp;这里，<script type="math/tex">p_i=P(X=x_i),i=1,2,...,n</script> </p><h4 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h4><p>&emsp;&emsp;信息增益表示得知了特征<script type="math/tex">X</script> 的信息，使得类<script type="math/tex">Y</script> 的信息不确定性减小的程度。也叫作互信息（mutual information），决策树中的信息增益等价于训练集中的类与特征的互信息。</p><p>&emsp;&emsp;特征<script type="math/tex">A</script> 对训练数据集<script type="math/tex">D</script> 的信息增益<script type="math/tex">g(D,A)</script> ，定义为集合<script type="math/tex">D</script> 的经验熵<script type="math/tex">H(D)</script> 与特征<script type="math/tex">A</script> 给定条件下<script type="math/tex">D</script> 的经验条件熵<script type="math/tex">H(D|A)</script> 之差，即 </p><script type="math/tex; mode=display">g(D,A)=H(D)−H(D|A)</script><p>&emsp;&emsp;决策树学习应用信息增益准则选择特征。信息增益大的特征具有更强的分类能力。<br>&emsp;&emsp;根据信息增益准则的特征选择方法是：对训练数据集（或子集）<script type="math/tex">D</script> ,计算其每个特征的信息增益，并比较他们的大小，选择信息增益最大的特征。</p><h4 id="算法-信息增益的算法"><a href="#算法-信息增益的算法" class="headerlink" title="算法  (信息增益的算法)"></a>算法  (信息增益的算法)</h4><p>输入：训练数据集<script type="math/tex">D</script> 和特征<script type="math/tex">A</script> ；</p><p>输出：特征<script type="math/tex">A</script> 对训练数据集<script type="math/tex">D</script> 的信息增益<script type="math/tex">g(D,A)</script> 。</p><p>(1) 计算数据集<script type="math/tex">D</script> 的经验熵<script type="math/tex">H(D)</script> </p><script type="math/tex; mode=display">H(D)=-\sum \limits_{k=1}^K \frac{\left | C_k \right |}{\left | D \right |}\log_2 \frac{\left | C_k \right |}{\left | D \right |}</script><p>(2) 计算特征<script type="math/tex">A</script> 对数据集<script type="math/tex">D</script> 的经验条件熵<script type="math/tex">H(D|A)</script> </p><script type="math/tex; mode=display">H(D|A)=\sum \limits_{i=1}^n \frac{\left | D_i \right |}{\left | D \right |}H(D_i)=-\sum \limits_{i=1}^n \frac{\left | D_i \right |}{\left | D \right |} \sum \limits_{k=1}^K \frac{\left | D_{ik}\right |}{\left | D_i \right |} \log_2 \frac{\left | D_{ik} \right |}{\left | D_i \right |}</script><p>(3) 计算信息增益</p><script type="math/tex; mode=display">g(D,A)=H(D)-H(D|A)</script><p>&emsp;&emsp;实现信息增益的python代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_ent</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        calculate shanno ent of x</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    x_value_list = set([x[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">0</span>])])</span><br><span class="line">    ent = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> x_value <span class="keyword">in</span> x_value_list:</span><br><span class="line">        p = float(x[x == x_value].shape[<span class="number">0</span>]) / x.shape[<span class="number">0</span>]</span><br><span class="line">        logp = np.log2(p)</span><br><span class="line">        ent -= p * logp</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ent</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_condition_ent</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        calculate ent H(y|x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># calc ent(y|x)</span></span><br><span class="line">    x_value_list = set([x[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">0</span>])])</span><br><span class="line">    ent = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> x_value <span class="keyword">in</span> x_value_list:</span><br><span class="line">        sub_y = y[x == x_value]</span><br><span class="line">        temp_ent = calc_ent(sub_y)</span><br><span class="line">        ent += (float(sub_y.shape[<span class="number">0</span>]) / y.shape[<span class="number">0</span>]) * temp_ent</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ent</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_ent_grap</span><span class="params">(x,y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        calculate ent grap</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    base_ent = calc_ent(y)</span><br><span class="line">    condition_ent = calc_condition_ent(x, y)</span><br><span class="line">    ent_grap = base_ent - condition_ent</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ent_grap</span><br></pre></td></tr></table></figure><h4 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h4><p>&emsp;&emsp;以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比（information gain ratio）可以对这一问题进行校正。这是特征选择的另一准则。 </p><p>&emsp;&emsp;特征<script type="math/tex">A</script> 对训练数据集<script type="math/tex">D</script> 的信息增益比<script type="math/tex">g_R(D,A)</script> 定义为其信息增益<script type="math/tex">g(D,A)</script> 与训练数据集<script type="math/tex">D</script> 关于特征<script type="math/tex">A</script> 的值的熵<script type="math/tex">H_A(D)</script> 之比，即 </p><script type="math/tex; mode=display">g_R(D,A)=\frac{g(D,A)} {H_A(D)}</script><p>&emsp;&emsp;其中，<script type="math/tex">H_A(D)=-\sum \limits_{i=1}^n \frac{\left | D_i \right |}{\left | D \right |}\log_2 \frac{\left | D_i \right |}{\left | D \right |}</script> </p><h2 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a>决策树的生成</h2><h3 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h3><p>&emsp;&emsp;ID3算法（interative dichotomiser 3）的核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。具体方法是：从根结点（root node）开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点；再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止。最后得到一个决策树。ID3相当于用极大似然法进行概率模型的选择。<br>&emsp;&emsp;ID3算法只有树的生成，所以该算法生成的树容易产生过拟合。</p><h4 id="算法-ID3算法"><a href="#算法-ID3算法" class="headerlink" title="算法  (ID3算法)"></a>算法  (ID3算法)</h4><p>输入：训练数据集<script type="math/tex">D</script> ，特征集<script type="math/tex">A</script> ，阈值<script type="math/tex">\varepsilon</script> ；</p><p>输出：决策树<script type="math/tex">T</script>  。</p><p>(1) 如果<script type="math/tex">D</script> 中的所有实例属于同一类<script type="math/tex">C_{k}</script>   ，则置<script type="math/tex">T</script> 为单结点树，并将<script type="math/tex">C_{k}</script> 作为该结点的类，返回<script type="math/tex">T</script> ；</p><p>(2) 如果<script type="math/tex">A=\phi</script> ，则置<script type="math/tex">T</script> 为单节点树，并将<script type="math/tex">D</script> 中实例数最大的类<script type="math/tex">C_{k}</script> 作为该结点的类，返回<script type="math/tex">T</script> ；</p><p>(3) 否则，按<script type="math/tex">g(D,A)=H(D)-H(D|A)</script> 计算<script type="math/tex">A</script> 中个特征对<script type="math/tex">D</script> 的信息增益比，选择信息增益比最大的特征<script type="math/tex">A_{g}</script>  ；</p><p>(4) 如果<script type="math/tex">A_{g}</script> 的信息增益比小于阈值<script type="math/tex">\varepsilon</script> ，则置<script type="math/tex">T</script> 为单结点树，并将<script type="math/tex">D</script> 中实例数最大的类<script type="math/tex">C_{k}</script> 作为该结点的类，返回<script type="math/tex">T</script> ；</p><p>(5) 否则，对<script type="math/tex">A_{g}</script> 的每一可能值<script type="math/tex">a_{i}</script> ，依<script type="math/tex">A_{g} = a_{i}</script> 将<script type="math/tex">D</script> 分割为子集若干非空<script type="math/tex">D_{i}</script> ，将<script type="math/tex">D_{i}</script> 中实例树最大的类作为标记，构建子结点，由结点及其子结点构成树<script type="math/tex">T</script> 返回<script type="math/tex">T</script> ；</p><p>(6) 对结点<script type="math/tex">i</script> ，以<script type="math/tex">D_{i}</script> 为训练集，以<script type="math/tex">A - \left \{A_{g}\right \}</script> 为特征集，递归地调用步(1)~步(5)，得到子树<script type="math/tex">T_{i}</script> ，返回<script type="math/tex">T_{i}</script>  .</p><p><strong>例子</strong> ：对下表训练数据，利用ID3算法建立决策树。</p><p>&emsp;&emsp;下面给出python代码(绘图代码不在建立决策树之内)：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">majorityCnt</span><span class="params">(classList)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">返回出现次数最多的分类名称</span></span><br><span class="line"><span class="string">    :param classList: 类列表</span></span><br><span class="line"><span class="string">    :return: 出现次数最多的类名称</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    classCount = &#123;&#125;  <span class="comment"># 这是一个字典</span></span><br><span class="line">    <span class="keyword">for</span> vote <span class="keyword">in</span> classList:</span><br><span class="line">        <span class="keyword">if</span> vote <span class="keyword">not</span> <span class="keyword">in</span> classCount.keys(): classCount[vote] = <span class="number">0</span></span><br><span class="line">        classCount[vote] += <span class="number">1</span></span><br><span class="line">    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"> </span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeatureToSplitByID3</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">选择最好的数据集划分方式</span></span><br><span class="line"><span class="string">    :param dataSet:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    numFeatures = len(dataSet[<span class="number">0</span>]) - <span class="number">1</span>  <span class="comment"># 最后一列是分类</span></span><br><span class="line">    baseEntropy = calcShannonEnt(dataSet)</span><br><span class="line">    bestInfoGain = <span class="number">0.0</span></span><br><span class="line">    bestFeature = - <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):  <span class="comment"># 遍历所有维度特征</span></span><br><span class="line">        infoGain = calcInformationGain(dataSet, baseEntropy, i)</span><br><span class="line">        <span class="keyword">if</span> (infoGain &gt; bestInfoGain):  <span class="comment"># 选择最大的信息增益</span></span><br><span class="line">            bestInfoGain = infoGain</span><br><span class="line">            bestFeature = i</span><br><span class="line">    <span class="keyword">return</span> bestFeature  <span class="comment"># 返回最佳特征对应的维度</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(dataSet, labels, chooseBestFeatureToSplitFunc=chooseBestFeatureToSplitByID3)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">创建决策树</span></span><br><span class="line"><span class="string">    :param dataSet:数据集</span></span><br><span class="line"><span class="string">    :param labels:数据集每一维的名称</span></span><br><span class="line"><span class="string">    :return:决策树</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    classList = [example[<span class="number">-1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]  <span class="comment"># 类别列表</span></span><br><span class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == len(classList):</span><br><span class="line">        <span class="keyword">return</span> classList[<span class="number">0</span>]  <span class="comment"># 当类别完全相同则停止继续划分</span></span><br><span class="line">    <span class="keyword">if</span> len(dataSet[<span class="number">0</span>]) == <span class="number">1</span>:  <span class="comment"># 当只有一个特征的时候，遍历完所有实例返回出现次数最多的类别</span></span><br><span class="line">        <span class="keyword">return</span> majorityCnt(classList)</span><br><span class="line">    bestFeat = chooseBestFeatureToSplitFunc(dataSet)</span><br><span class="line">    bestFeatLabel = labels[bestFeat]</span><br><span class="line">    myTree = &#123;bestFeatLabel: &#123;&#125;&#125;</span><br><span class="line">    <span class="keyword">del</span> (labels[bestFeat])</span><br><span class="line">    featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">    uniqueVals = set(featValues)</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">        subLabels = labels[:]  <span class="comment"># 复制操作</span></span><br><span class="line">        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels)</span><br><span class="line">    <span class="keyword">return</span> myTree</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">创建数据集</span></span><br><span class="line"><span class="string"> </span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dataSet = [[<span class="string">u'青年'</span>, <span class="string">u'否'</span>, <span class="string">u'否'</span>, <span class="string">u'一般'</span>, <span class="string">u'拒绝'</span>],</span><br><span class="line">               [<span class="string">u'青年'</span>, <span class="string">u'否'</span>, <span class="string">u'否'</span>, <span class="string">u'好'</span>, <span class="string">u'拒绝'</span>],</span><br><span class="line">               [<span class="string">u'青年'</span>, <span class="string">u'是'</span>, <span class="string">u'否'</span>, <span class="string">u'好'</span>, <span class="string">u'同意'</span>],</span><br><span class="line">               [<span class="string">u'青年'</span>, <span class="string">u'是'</span>, <span class="string">u'是'</span>, <span class="string">u'一般'</span>, <span class="string">u'同意'</span>],</span><br><span class="line">               [<span class="string">u'青年'</span>, <span class="string">u'否'</span>, <span class="string">u'否'</span>, <span class="string">u'一般'</span>, <span class="string">u'拒绝'</span>],</span><br><span class="line">               [<span class="string">u'中年'</span>, <span class="string">u'否'</span>, <span class="string">u'否'</span>, <span class="string">u'一般'</span>, <span class="string">u'拒绝'</span>],</span><br><span class="line">               [<span class="string">u'中年'</span>, <span class="string">u'否'</span>, <span class="string">u'否'</span>, <span class="string">u'好'</span>, <span class="string">u'拒绝'</span>],</span><br><span class="line">               [<span class="string">u'中年'</span>, <span class="string">u'是'</span>, <span class="string">u'是'</span>, <span class="string">u'好'</span>, <span class="string">u'同意'</span>],</span><br><span class="line">               [<span class="string">u'中年'</span>, <span class="string">u'否'</span>, <span class="string">u'是'</span>, <span class="string">u'非常好'</span>, <span class="string">u'同意'</span>],</span><br><span class="line">               [<span class="string">u'中年'</span>, <span class="string">u'否'</span>, <span class="string">u'是'</span>, <span class="string">u'非常好'</span>, <span class="string">u'同意'</span>],</span><br><span class="line">               [<span class="string">u'老年'</span>, <span class="string">u'否'</span>, <span class="string">u'是'</span>, <span class="string">u'非常好'</span>, <span class="string">u'同意'</span>],</span><br><span class="line">               [<span class="string">u'老年'</span>, <span class="string">u'否'</span>, <span class="string">u'是'</span>, <span class="string">u'好'</span>, <span class="string">u'同意'</span>],</span><br><span class="line">               [<span class="string">u'老年'</span>, <span class="string">u'是'</span>, <span class="string">u'否'</span>, <span class="string">u'好'</span>, <span class="string">u'同意'</span>],</span><br><span class="line">               [<span class="string">u'老年'</span>, <span class="string">u'是'</span>, <span class="string">u'否'</span>, <span class="string">u'非常好'</span>, <span class="string">u'同意'</span>],</span><br><span class="line">               [<span class="string">u'老年'</span>, <span class="string">u'否'</span>, <span class="string">u'否'</span>, <span class="string">u'一般'</span>, <span class="string">u'拒绝'</span>],</span><br><span class="line">               ]</span><br><span class="line">    labels = [<span class="string">u'年龄'</span>, <span class="string">u'有工作'</span>, <span class="string">u'有房子'</span>, <span class="string">u'信贷情况'</span>]</span><br><span class="line">    <span class="comment"># 返回数据集和每个维度的名称</span></span><br><span class="line">    <span class="keyword">return</span> dataSet, labels</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet</span><span class="params">(dataSet, axis, value)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">按照给定特征划分数据集</span></span><br><span class="line"><span class="string">    :param dataSet: 待划分的数据集</span></span><br><span class="line"><span class="string">    :param axis: 划分数据集的特征的维度</span></span><br><span class="line"><span class="string">    :param value: 特征的值</span></span><br><span class="line"><span class="string">    :return: 符合该特征的所有实例（并且自动移除掉这维特征）</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    retDataSet = []</span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="keyword">if</span> featVec[axis] == value:</span><br><span class="line">            reducedFeatVec = featVec[:axis]  <span class="comment"># 删掉这一维特征</span></span><br><span class="line">            reducedFeatVec.extend(featVec[axis + <span class="number">1</span>:])</span><br><span class="line">            retDataSet.append(reducedFeatVec)</span><br><span class="line">    <span class="keyword">return</span> retDataSet</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcShannonEnt</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">计算训练数据集中的Y随机变量的香农熵</span></span><br><span class="line"><span class="string">    :param dataSet:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    numEntries = len(dataSet)  <span class="comment"># 实例的个数</span></span><br><span class="line">    labelCounts = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:  <span class="comment"># 遍历每个实例，统计标签的频次</span></span><br><span class="line">        currentLabel = featVec[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys(): labelCounts[currentLabel] = <span class="number">0</span></span><br><span class="line">        labelCounts[currentLabel] += <span class="number">1</span></span><br><span class="line">    shannonEnt = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:</span><br><span class="line">        prob = float(labelCounts[key]) / numEntries</span><br><span class="line">        shannonEnt -= prob * math.log(prob, <span class="number">2</span>)  <span class="comment"># log base 2</span></span><br><span class="line">    <span class="keyword">return</span> shannonEnt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcConditionalEntropy</span><span class="params">(dataSet, i, featList, uniqueVals)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    计算X_i给定的条件下，Y的条件熵</span></span><br><span class="line"><span class="string">    :param dataSet:数据集</span></span><br><span class="line"><span class="string">    :param i:维度i</span></span><br><span class="line"><span class="string">    :param featList: 数据集特征列表</span></span><br><span class="line"><span class="string">    :param uniqueVals: 数据集特征集合</span></span><br><span class="line"><span class="string">    :return:条件熵</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    ce = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">        subDataSet = splitDataSet(dataSet, i, value)</span><br><span class="line">        prob = len(subDataSet) / float(len(dataSet))  <span class="comment"># 极大似然估计概率</span></span><br><span class="line">        ce += prob * calcShannonEnt(subDataSet)  <span class="comment"># ∑pH(Y|X=xi) 条件熵的计算</span></span><br><span class="line">    <span class="keyword">return</span> ce</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcInformationGain</span><span class="params">(dataSet, baseEntropy, i)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算信息增益</span></span><br><span class="line"><span class="string">    :param dataSet:数据集</span></span><br><span class="line"><span class="string">    :param baseEntropy:数据集中Y的信息熵</span></span><br><span class="line"><span class="string">    :param i: 特征维度i</span></span><br><span class="line"><span class="string">    :return: 特征i对数据集的信息增益g(dataSet|X_i)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]  <span class="comment"># 第i维特征列表</span></span><br><span class="line">    uniqueVals = set(featList)  <span class="comment"># 转换成集合</span></span><br><span class="line">    newEntropy = calcConditionalEntropy(dataSet, i, featList, uniqueVals)</span><br><span class="line">    infoGain = baseEntropy - newEntropy  <span class="comment"># 信息增益，就是熵的减少，也就是不确定性的减少</span></span><br><span class="line">    <span class="keyword">return</span> infoGain</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcInformationGainRate</span><span class="params">(dataSet, baseEntropy, i)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算信息增益比</span></span><br><span class="line"><span class="string">    :param dataSet:数据集</span></span><br><span class="line"><span class="string">    :param baseEntropy:数据集中Y的信息熵</span></span><br><span class="line"><span class="string">    :param i: 特征维度i</span></span><br><span class="line"><span class="string">    :return: 特征i对数据集的信息增益g(dataSet|X_i)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> calcInformationGain(dataSet, baseEntropy, i) / baseEntropy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 决策树的构建</span></span><br><span class="line">myDat, labels = createDataSet()</span><br><span class="line">myTree = createTree(myDat, labels)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘制决策树</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义文本框和箭头格式</span></span><br><span class="line">decisionNode = dict(boxstyle=<span class="string">"round4"</span>, color=<span class="string">'#3366FF'</span>)  <span class="comment">#定义判断结点形态</span></span><br><span class="line">leafNode = dict(boxstyle=<span class="string">"circle"</span>, color=<span class="string">'#FF6633'</span>)  <span class="comment">#定义叶结点形态</span></span><br><span class="line">arrow_args = dict(arrowstyle=<span class="string">"&lt;-"</span>, color=<span class="string">'g'</span>)  <span class="comment">#定义箭头</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#绘制带箭头的注释</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotNode</span><span class="params">(nodeTxt, centerPt, parentPt, nodeType)</span>:</span></span><br><span class="line">    createPlot.ax1.annotate(nodeTxt, xy=parentPt, xycoords=<span class="string">'axes fraction'</span>,</span><br><span class="line">                            xytext=centerPt, textcoords=<span class="string">'axes fraction'</span>,</span><br><span class="line">                            va=<span class="string">"center"</span>, ha=<span class="string">"center"</span>, bbox=nodeType, arrowprops=arrow_args)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment">#计算叶结点数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getNumLeafs</span><span class="params">(myTree)</span>:</span></span><br><span class="line">    numLeafs = <span class="number">0</span></span><br><span class="line">    firstStr = list(myTree.keys())[<span class="number">0</span>]</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">        <span class="keyword">if</span> type(secondDict[key]).__name__ == <span class="string">'dict'</span>:</span><br><span class="line">            numLeafs += getNumLeafs(secondDict[key])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            numLeafs += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> numLeafs</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment">#计算树的层数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getTreeDepth</span><span class="params">(myTree)</span>:</span></span><br><span class="line">    maxDepth = <span class="number">0</span></span><br><span class="line">    firstStr = list(myTree.keys())[<span class="number">0</span>]</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">        <span class="keyword">if</span> type(secondDict[key]).__name__ == <span class="string">'dict'</span>:</span><br><span class="line">            thisDepth = <span class="number">1</span> + getTreeDepth(secondDict[key])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            thisDepth = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> thisDepth &gt; maxDepth:</span><br><span class="line">            maxDepth = thisDepth</span><br><span class="line">    <span class="keyword">return</span> maxDepth</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="comment">#在父子结点间填充文本信息</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotMidText</span><span class="params">(cntrPt, parentPt, txtString)</span>:</span></span><br><span class="line">    xMid = (parentPt[<span class="number">0</span>] - cntrPt[<span class="number">0</span>]) / <span class="number">2.0</span> + cntrPt[<span class="number">0</span>]</span><br><span class="line">    yMid = (parentPt[<span class="number">1</span>] - cntrPt[<span class="number">1</span>]) / <span class="number">2.0</span> + cntrPt[<span class="number">1</span>]</span><br><span class="line">    createPlot.ax1.text(xMid, yMid, txtString, va=<span class="string">"center"</span>, ha=<span class="string">"center"</span>, rotation=<span class="number">30</span>)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotTree</span><span class="params">(myTree, parentPt, nodeTxt)</span>:</span></span><br><span class="line">    numLeafs = getNumLeafs(myTree)</span><br><span class="line">    depth = getTreeDepth(myTree)</span><br><span class="line">    firstStr = list(myTree.keys())[<span class="number">0</span>]</span><br><span class="line">    cntrPt = (plotTree.xOff + (<span class="number">1.0</span> + float(numLeafs)) / <span class="number">2.0</span> / plotTree.totalW, plotTree.yOff)</span><br><span class="line">    plotMidText(cntrPt, parentPt, nodeTxt)  <span class="comment">#在父子结点间填充文本信息</span></span><br><span class="line">    plotNode(firstStr, cntrPt, parentPt, decisionNode)  <span class="comment">#绘制带箭头的注释</span></span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    plotTree.yOff = plotTree.yOff - <span class="number">1.0</span> / plotTree.totalD</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">        <span class="keyword">if</span> type(secondDict[key]).__name__ == <span class="string">'dict'</span>:</span><br><span class="line">            plotTree(secondDict[key], cntrPt, str(key))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            plotTree.xOff = plotTree.xOff + <span class="number">1.0</span> / plotTree.totalW</span><br><span class="line">            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)</span><br><span class="line">            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))</span><br><span class="line">    plotTree.yOff = plotTree.yOff + <span class="number">1.0</span> / plotTree.totalD</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createPlot</span><span class="params">(inTree)</span>:</span></span><br><span class="line">    fig = plt.figure(<span class="number">1</span>, facecolor=<span class="string">'white'</span>)</span><br><span class="line">    fig.clf()</span><br><span class="line">    axprops = dict(xticks=[], yticks=[])</span><br><span class="line">    createPlot.ax1 = plt.subplot(<span class="number">111</span>, frameon=<span class="keyword">False</span>, **axprops)</span><br><span class="line">    plotTree.totalW = float(getNumLeafs(inTree))</span><br><span class="line">    plotTree.totalD = float(getTreeDepth(inTree))</span><br><span class="line">    plotTree.xOff = - <span class="number">0.5</span> / plotTree.totalW;</span><br><span class="line">    plotTree.yOff = <span class="number">1.0</span>;</span><br><span class="line">    plotTree(inTree, (<span class="number">0.5</span>, <span class="number">1.0</span>), <span class="string">''</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">createPlot(myTree)</span><br></pre></td></tr></table></figure><p>&emsp;&emsp;我们应该可以得到如下差不多的图：</p><p><img src="http://img.blog.csdn.net/20180303165520146?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMzA2MTE2MDE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><h3 id="C4-5的生成算法"><a href="#C4-5的生成算法" class="headerlink" title="C4.5的生成算法"></a>C4.5的生成算法</h3><p>&emsp;&emsp;C4.5算法与ID3算法相似，C4.5算法对ID3算法进行了改进。C4.5在生成的过程中，用信息增益比来选择特征。</p><h4 id="算法-C4-5的生成算法-："><a href="#算法-C4-5的生成算法-：" class="headerlink" title="算法  (C4.5的生成算法)："></a>算法  (C4.5的生成算法)：</h4><p>输入：训练数据集<script type="math/tex">D</script> ，特征集<script type="math/tex">A</script> ，阈值<script type="math/tex">\varepsilon</script> </p><p>输出：决策树<script type="math/tex">T</script> </p><p>(1) 如果<script type="math/tex">D</script> 中的所有实例属于同一类<script type="math/tex">C_{k}</script>   ，则置<script type="math/tex">T</script> 为单结点树，并将<script type="math/tex">C_{k}</script> 作为该结点的类，返回<script type="math/tex">T</script> ；</p><p>(2) 如果<script type="math/tex">A=\phi</script> ，则置<script type="math/tex">T</script> 为单节点树，并将<script type="math/tex">D</script> 中实例数最大的类<script type="math/tex">C_{k}</script> 作为该结点的类，返回<script type="math/tex">T</script> ；</p><p>(3) 否则，按<script type="math/tex">g_{R}(D,A) = \frac{g(D/A)}{H_{A}(D)}</script> 计算<script type="math/tex">A</script> 中个特征对<script type="math/tex">D</script> 的信息增益比，选择信息增益比最大的特征<script type="math/tex">A_{g}</script>  ；</p><p>(4) 如果<script type="math/tex">A_{g}</script> 的信息增益比小于阈值<script type="math/tex">\varepsilon</script> ，则置<script type="math/tex">T</script> 为单结点树，并将<script type="math/tex">D</script> 中实例数最大的类<script type="math/tex">C_{k}</script> 作为该结点的类，返回<script type="math/tex">T</script> ；</p><p>(5) 否则，对<script type="math/tex">A_{g}</script> 的每一可能值<script type="math/tex">a_{i}</script> ，依<script type="math/tex">A_{g} = a_{i}</script> 将<script type="math/tex">D</script> 分割为子集若干非空<script type="math/tex">D_{i}</script> ，将<script type="math/tex">D_{i}</script> 中实例树最大的类作为标记，构建子结点，由结点及其子结点构成树<script type="math/tex">T</script> 返回<script type="math/tex">T</script> ；</p><p>(6) 对结点<script type="math/tex">i</script> ，以<script type="math/tex">D_{i}</script> 为训练集，以<script type="math/tex">A - \left \{A_{g}\right \}</script> 为特征集，递归地调用步(1)~步(5)，得到子树<script type="math/tex">T_{i}</script> ，返回<script type="math/tex">T_{i}</script>  .</p><p>&emsp;&emsp;实现的python代码(省略与之前重复的代码)：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeatureToSplitByC45</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">选择最好的数据集划分方式</span></span><br><span class="line"><span class="string">    :param dataSet:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    numFeatures = len(dataSet[<span class="number">0</span>]) – <span class="number">1</span>  <span class="comment"># 最后一列是分类</span></span><br><span class="line">    baseEntropy = calcShannonEnt(dataSet)</span><br><span class="line">    bestInfoGainRate = <span class="number">0.0</span></span><br><span class="line">    bestFeature = –<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):  <span class="comment"># 遍历所有维度特征</span></span><br><span class="line">        infoGainRate = calcInformationGainRate(dataSet, baseEntropy, i)</span><br><span class="line">        <span class="keyword">if</span> (infoGainRate &gt; bestInfoGainRate):  <span class="comment"># 选择最大的信息增益</span></span><br><span class="line">            bestInfoGainRate = infoGainRate</span><br><span class="line">            bestFeature = i</span><br><span class="line">    <span class="keyword">return</span> bestFeature  <span class="comment"># 返回最佳特征对应的维度</span></span><br></pre></td></tr></table></figure><h2 id="决策树的剪枝"><a href="#决策树的剪枝" class="headerlink" title="决策树的剪枝"></a>决策树的剪枝</h2><p>&emsp;&emsp;决策树很容易发生过拟合，过拟合的原因在于学习的时候过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。解决这个问题的办法就是简化已生成的决策树，也就是剪枝。</p><p>&emsp;&emsp;决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。</p><p>决策树生成：考虑更好的拟合训练集数据  —— 学习局部的模型<br>剪枝：通过优化损失函数还考虑了减小模型复杂度 —— 学习整体的模型</p><p>&emsp;&emsp;设树T的叶节点个数为<script type="math/tex">\left |T\right|</script> ，每个叶节点即为沿从root的某条路径条件下的一类。<script type="math/tex">t</script> 是树<script type="math/tex">T</script> 的叶节点，该节点有<script type="math/tex">N_t</script> 个样本点，其中属于各个分类的点为<script type="math/tex">N_{tk}</script> 个。</p><p>&emsp;&emsp;该叶节点的经验熵为： </p><script type="math/tex; mode=display">H_t(T)=-\sum \limits_{k=1}^K \frac{\left | N_{tk} \right |}{\left | N_t \right |}\log_2 \frac{\left | N_{tk} \right |}{\left | N_t \right |}</script><p>&emsp;&emsp;则决策树学习的损失函数可以定义为：</p><script type="math/tex; mode=display">C_{\alpha}(T)=\sum \limits_{t=1}^{\left |T\right |} N_tH_t(T) + \alpha \left |T\right|</script><p>&emsp;&emsp;记右端第一项为</p><script type="math/tex; mode=display">C(T)=\sum \limits_{t=1}^{\left |T\right |} N_tH_t(T)=-\sum \limits_{t=1}^{\left |T\right |} \sum \limits_{k=1}^{K} N_{tk} \log \frac{N_{tk}}{N_t}</script><p>&emsp;&emsp;有<script type="math/tex">C_{\alpha}(T)=C(T) + \alpha \left |T\right|</script> </p><p>&emsp;&emsp;第一项反映对训练集的拟合程度，第二项反映模型复杂度。等价于正则化的极大似然估计。</p><p>&emsp;&emsp;L1范数是指向量中各个元素绝对值之和，也叫“稀疏规则算子”（Lasso regularization）。</p><p>&emsp;&emsp;L2范数是指向量各元素的平方和然后求平方根。</p><h4 id="算法-树的剪枝算法"><a href="#算法-树的剪枝算法" class="headerlink" title="算法 (树的剪枝算法)"></a>算法 (树的剪枝算法)</h4><p>输入：生成的树<script type="math/tex">T</script> ，参数<script type="math/tex">α</script><br>输出：子树<script type="math/tex">T_α</script> </p><p>(1) 计算每个节点的经验熵。</p><p>(2) 递归地从叶节点向上收缩。</p><p>设有一组叶节点回缩前后的整体树分别为<script type="math/tex">T_B</script> ,<script type="math/tex">T_A</script> ，对应的损失函数如果是： </p><script type="math/tex; mode=display">C_{\alpha}(T_A)\leq C_{\alpha}(T_B)</script><p>剪枝后损失函数更小，则说明应该剪枝，将父结点变为新的叶节点。<br>(3) 返回(2)直至不能继续为止。得到损失函数最小的子树<script type="math/tex">T_{\alpha}</script> 。</p><p>&emsp;&emsp;注意(1)我们计算的是所有节点的经验熵，虽然我们考虑叶节点比较，但因为不断剪枝时，每个节点都可能变为叶节点。所以计算全部存起来。</p><p>&emsp;&emsp;注意：每次考虑两个树的损失函数的查，计算可以在局部进行，所以剪枝算法可以由一种动态规划的算法实现。</p><h2 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h2><p>&emsp;&emsp;分类与回归树(classification and regression tree, CART)，即可用于分类，也可用于回归。由三步组成：特征选择，生成树，剪枝。</p><p>&emsp;&emsp;CART树假设决策树为二叉树，每个结点为2分支，“是”与“否”。</p><h3 id="CART生成"><a href="#CART生成" class="headerlink" title="CART生成"></a>CART生成</h3><p>对回归树：平方误差最小化<br>对分类树：基尼指数-Gini index</p><h4 id="回归树的生成"><a href="#回归树的生成" class="headerlink" title="回归树的生成"></a>回归树的生成</h4><p>&emsp;&emsp;一个回归树对应特征空间的一个划分，每个划分单元上输出一个值。假设已将输入空间划分成<script type="math/tex">M</script> 个单元<script type="math/tex">R_1,R_2...,R_M</script> ，对应输出值为<script type="math/tex">c_1,c_2...,c_M</script> ，那么，回归树模型表示为：</p><script type="math/tex; mode=display">f(x)=\sum \limits_{m=1}^{M} c_mI(x\in R_m)</script><p>&emsp;&emsp;当输入空间的划分确定时，可以用平方误差<script type="math/tex">\sum \limits_{x_i\in R_m} (y_i-f(x_i))^2</script> 表示回归树对训练集<script type="math/tex">D</script> 的预测误差</p><p>&emsp;&emsp;每个单元上的均值为<script type="math/tex">\hat{c}_m= ave(y_i|x_i \in R_m)</script> </p><p>&emsp;&emsp;用启发式的方法对空间进行划分，选择切分变量和切分值：特征的第<script type="math/tex">j</script> 个变量，以及其取值<script type="math/tex">s</script> ，将空间划分为两个子空间：</p><script type="math/tex; mode=display">R_1(j,s)=\left \{x|x^{(j)} \leq s\right \}$$ 和 $$R_2(j,s)=\left \{x|x^(j)>s\right \}</script><p>&emsp;&emsp;然后我们求解这个划分下的最小值：</p><script type="math/tex; mode=display">\min \limits_{j,s}\left [ \min \limits_{c_1} \sum \limits_{x_i\in R_1(j,s)} (y_i-c_1)^2+\min \limits_{c_2} (y_i-c_2)^2 \right ]</script><p>&emsp;&emsp;这样，我们可以遍历<script type="math/tex">j</script> 和<script type="math/tex">s</script> ，对每一组情况下，算出[ ]中的最小值。在所有<script type="math/tex">j</script> ，<script type="math/tex">s</script> 组合情况下找出一个最小值。此时的<script type="math/tex">j</script> 和<script type="math/tex">s</script> 就是我们需要的。第一次划分后，对两个子区域迭代这样做，就可以将空间不断细化。</p><h4 id="算法-最小二乘回归树生成算法"><a href="#算法-最小二乘回归树生成算法" class="headerlink" title="算法  (最小二乘回归树生成算法)"></a>算法  (最小二乘回归树生成算法)</h4><p>输入：训练数据集<script type="math/tex">D</script> </p><p>输出：回归树<script type="math/tex">f(x)</script><br>(1) 选择最优切分变量<script type="math/tex">j</script> 和切分点<script type="math/tex">s</script> 。求解：</p><script type="math/tex; mode=display">\min \limits_{j,s}\left [ \min \limits_{c_1} \sum \limits_{x_i\in R_1(j,s)} (y_i-c_1)^2+\min \limits_{c_2} (y_i-c_2)^2 \right ]</script><p>遍历<script type="math/tex">j</script> 和<script type="math/tex">s</script> ，对某个<script type="math/tex">j</script> 扫描<script type="math/tex">s</script> ，使得上式最小</p><p>(2) 用选好的<script type="math/tex">(j, s)</script> 划分区域，并决定各分区的输出值：</p><script type="math/tex; mode=display">R_1(j,s)=\left \{x|x^{(j)} \leq s\right \}$$ 和 $$R_2(j,s)=\left \{x|x^(j)>s\right \}</script><script type="math/tex; mode=display">\hat{c}_m=\frac{1}{N_m} \sum \limits_{x_i\in R_1(j,s)} y_i$$ ，$$x\in R_m,m=1,2</script><p>(3) 继续对两个子区域调用步骤(1)，(2)，直到满足停止的条件</p><p>(4) 将输入空间划分为<script type="math/tex">M</script> 个区域<script type="math/tex">R_1,R_2,...,R_M</script> ，生成决策树：</p><script type="math/tex; mode=display">f(x)=\sum \limits_{m=1}^{M} \hat{c_m} I(x\in R_m)</script><h4 id="分类树的生成"><a href="#分类树的生成" class="headerlink" title="分类树的生成"></a>分类树的生成</h4><p>&emsp;&emsp;对分类问题，随机变量在每个类上都有概率。以数据集中各类个数比上总数，极大似然估计，得到离散的概率分布。</p><p>基尼系数： </p><script type="math/tex; mode=display">Gini(x)=\sum \limits_{k=1}^{K} p_k(1-p_k)=1-\sum \limits_{k=1}^{K}p_k^2</script><p>&emsp;&emsp;对于给定的样本集合<script type="math/tex">D</script> ，基尼指数为 </p><script type="math/tex; mode=display">Gini(x)=1-\sum \limits_{k=1}^{K}\left ( \frac{\left | C_k \right |}{\left | D \right |} \right )^2</script><p>&emsp;&emsp;<script type="math/tex">C_k</script> 是<script type="math/tex">D</script> 中第<script type="math/tex">k</script> 类的个数，<script type="math/tex">K</script> 是类别总个数。</p><p>&emsp;&emsp;若样本集合<script type="math/tex">D</script> 根据特征<script type="math/tex">A</script> 是否取某一可能值<script type="math/tex">a</script> 被划分为<script type="math/tex">D_1</script> ,<script type="math/tex">D_2</script> ，则在特征<script type="math/tex">A</script> 的条件下，集合<script type="math/tex">D</script> 的基尼指数为 </p><script type="math/tex; mode=display">Gini(D,A)=\frac{\left |D_1\right |}{\left |D\right |}Gini(D_1)+\frac{\left |D_2\right |}{\left |D\right |}Gini(D_2)</script><h4 id="算法-CART生成算法"><a href="#算法-CART生成算法" class="headerlink" title="算法  (CART生成算法)"></a>算法  (CART生成算法)</h4><p>输入：训练数据集<script type="math/tex">D</script> ，停止计算的条件</p><p>输出：CART决策树</p><p>根据训练数据集，从根结点开始，递归地对每个结点进行一下操作，构建二叉决策树：<br>(1)设结点的训练数据集为<script type="math/tex">D</script> ,计算现有特征对该数据集的基尼指数。此时，对每一个特征<script type="math/tex">A</script> ，对其可能取的每个值<script type="math/tex">a</script> ，根据样本点对<script type="math/tex">A=a</script> 的测试是“是”或“否”将<script type="math/tex">D</script> 分割成<script type="math/tex">D_1</script> 和<script type="math/tex">D_2</script> 两部分<br>(2)在所有可能的特征<script type="math/tex">A</script> 以及它们所有可能的切分点<script type="math/tex">a</script> 中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点，依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中<br>(3)对两个子结点递归地调用(1),(2),直至满足停止条件<br>(4)生成CART决策树<br>&emsp;&emsp;算法停止的条件是结点中的样本个数小于预定阈值，或样本集的基尼指数小于预定阈值（样本基本属于同一类），或者没有更多特征。</p><h3 id="CART剪枝"><a href="#CART剪枝" class="headerlink" title="CART剪枝"></a>CART剪枝</h3><p>&emsp;&emsp;CART剪枝算法从“完全生长”的决策树的底端剪去一些子树，使决策树变小（模型变简单），从而能够对未知数据有更准确的预测。CART剪枝算法由两步组成：首先，从生成算法产生的决策树<script type="math/tex">T_0</script> 底端开始不断剪枝，直到<script type="math/tex">T_0</script> 的根结点，形成一个子树序列<script type="math/tex">\left \{T_0,T_1,⋯,T_n\right \}</script> ；然后，通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树。</p><h4 id="剪枝，形成一个子树序列"><a href="#剪枝，形成一个子树序列" class="headerlink" title="剪枝，形成一个子树序列"></a>剪枝，形成一个子树序列</h4><p>&emsp;&emsp;在剪枝过程中，计算子树的损失函数:</p><script type="math/tex; mode=display">C_{\alpha}(T)=C(T) + \alpha \left |T\right|</script><p>&emsp;&emsp;可以用递归的方法对树进行剪枝，将<script type="math/tex">a</script> 从小增大，<script type="math/tex">\alpha_0<\alpha_1<...<\alpha_n<+\infty</script> ，产生一系列的区间<script type="math/tex">[\alpha_i，\alpha_{i+1})，i =0,1,...,n</script> ；剪枝得到的子树序列对应着区间<script type="math/tex">\alpha \in [\alpha_i，\alpha_{i+1})，i =0,1,...,n</script> 的最优子树序列<script type="math/tex">\left \{T_0, T_1, ... , T_n\right \}</script> ，序列中的子树是嵌套的。</p><p>对<script type="math/tex">T_0</script> 中每一内部结点<script type="math/tex">t</script> ，计算</p><script type="math/tex; mode=display">g(t)=\frac{C(t)-C(T_t)} {\left |T_t\right |-1}</script><p>&emsp;&emsp;表示剪枝后整体损失函数减少的程度，在<script type="math/tex">T_0</script> 中剪去<script type="math/tex">g(t)</script> 最小的<script type="math/tex">T_t</script> ，将得到的子树作为<script type="math/tex">T_1</script> ，同时将最小的<script type="math/tex">g(t)</script> 设为<script type="math/tex">\alpha_1</script>，<script type="math/tex">T_1</script> 为区间<script type="math/tex">[\alpha_1，\alpha_2)</script> 的最优子树。如此剪枝下去，直至得到根结点。在这一过程中，不断地增加<script type="math/tex">\alpha</script>的值，产生新的区间。</p><h4 id="在剪枝得到的子树序列-T-0-T-1-…-T-n-中通过交叉验证选取最优子树-T-a"><a href="#在剪枝得到的子树序列-T-0-T-1-…-T-n-中通过交叉验证选取最优子树-T-a" class="headerlink" title="在剪枝得到的子树序列$T_0,T_1, … , T_n$ 中通过交叉验证选取最优子树$T_a$"></a>在剪枝得到的子树序列$T_0,T_1, … , T_n$ 中通过交叉验证选取最优子树$T_a$</h4><p>&emsp;&emsp;具体地，利用独立的验证数据集，测试子树序列<script type="math/tex">T_0, T_1, ... , T_n</script> 中各棵子树的平方误差或基尼指数。平方误差或基尼指数最小的决策树被认为是最优的决策树。在子树序列中，每棵子树<script type="math/tex">T_0, T_1, ... , T_n</script> 都对应于一个参数<script type="math/tex">a_0, a_1, ... , a_n</script> 。所以，当最优子树<script type="math/tex">T_k</script> 确定时，对应的<script type="math/tex">a_k</script> 也确定了，即得到最优决策树<script type="math/tex">T_a</script> 。</p><h4 id="算法-CART剪枝算法"><a href="#算法-CART剪枝算法" class="headerlink" title="算法  (CART剪枝算法)"></a>算法  (CART剪枝算法)</h4><p>输入：CART算法生成的决策树<script type="math/tex">T_0</script> </p><p>输出：最优决策树<script type="math/tex">T_a</script> </p><p>(1) 设<script type="math/tex">k=0</script> ，<script type="math/tex">T=T_0</script> </p><p>(2) 设<script type="math/tex">\alpha=+\infty</script> </p><p>(3) 自下而上地对各内部的点<script type="math/tex">t</script> 计算<script type="math/tex">C(T_t)</script> ，<script type="math/tex">\left |T_t\right |</script> 以及</p><script type="math/tex; mode=display">g(t)=\frac{C(t)-C(T_t)} {\left |T_t\right |-1}</script><script type="math/tex; mode=display">\alpha=\min(\alpha,g(t))</script><p>这里，<script type="math/tex">T_t</script> 表示以<script type="math/tex">t</script> 为根结点的子树，<script type="math/tex">C(T_t)</script> 是对训练数据的预测误差，<script type="math/tex">\left |T_t\right |</script> 是<script type="math/tex">T_t</script> 的叶结点个数</p><p>(4) 对<script type="math/tex">g(t)=\alpha</script> 的内部结点<script type="math/tex">t</script> 进行剪枝，并对叶结点<script type="math/tex">t</script> 以多数表决法决定其类，得到树<script type="math/tex">T</script> </p><p>(5) 设<script type="math/tex">k=k+1,\alpha_k=\alpha,T_k=T</script> </p><p>(6) 如果<script type="math/tex">T_k</script> 不是由根结点及两个叶结点构成的树，则回到步骤(3)；否则令<script type="math/tex">T_k=T_n</script> </p><p>(7) 采取交叉验证法在子树序列<script type="math/tex">T_0,T_1,...,T_n</script> 中选取最优子树<script type="math/tex">T_a</script> .</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>（1）分类决策树模型，是表示基于特征对实例进行分类的树的结构。决策树可换成一个if-then规则的集合。也可看做是定义在特征空间划分上的类的条件概率分布。</p><p>（2）决策树学习的目的是构建一个与训练集拟合很好，并且复杂度小的决策树。从可能的决策树中直接选取最优决策树是NP完全问题，实际中采用启发式方法学习次优的决策树。</p><p>（3）学习算法有ID3，C4.5，CART。学习过程包括：特征选择，生成树，剪枝。特征选择目的在于选择对训练集能够分类的特征。特征选取的准则三种算法分别是信息增益最大，信息增益比最大，基尼系数最小。从根节点开始，递归产生决策树，分别对子树调用过程，不断选取局部最优特征。</p><p>（4）由于生成的决策树存在过拟合问题，需要剪枝。从生成的树上剪掉一些叶节点或者叶节点以上的子树，将其父节点或者根节点作为新的叶节点，简化生成的决策树。\</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="http://blog.csdn.net/niaolianjiulin/article/details/76263789" target="_blank" rel="noopener">《统计学习方法》笔记05：决策树模型</a></li></ol><ol><li><a href="http://blog.csdn.net/demon7639/article/details/51011416" target="_blank" rel="noopener">统计学习方法 李航—-第5章 决策树</a></li><li><a href="http://www.ppvke.com/Blog/archives/25042" target="_blank" rel="noopener">决策树</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;p&gt;&amp;emsp;&amp;emsp;决策树是一种基本的分类和回
      
    
    </summary>
    
      <category term="note" scheme="http://quanfita.cn/categories/note/"/>
    
    
      <category term="Machine Learning" scheme="http://quanfita.cn/tags/Machine-Learning/"/>
    
      <category term="决策树" scheme="http://quanfita.cn/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
  </entry>
  
</feed>
